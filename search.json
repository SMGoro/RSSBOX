[{"title":"I Got My Wish and Reincarnated as the Villainess (Last Boss)! 001-024 as V01-03 (Digital-Compilation) (Oan) [Oak]","path":"/RSSBOX/rss/9478a4fa.html","content":"#1890616 | I Got My Wish and Reincarnated as the Villainess (Last Boss)! 001-024 as v01-03 (Digital-Compilation) (oan) [Oak] | 376.3 MiB | Literature - English-translated | D4D88613795129202548FB7FBF4ADE95FEC4E749 I Got My Wish and Reincarnated as the Villainess (Last Boss)! 001-024 as v01-03 (Digital-Compilation) (oan) [Oak]https://nyaa.si/download/1890616.torrent","tags":["ACG","nyaa"],"categories":["ACG","nyaa"]},{"title":"Keiken Zumi Na Kimi To, Keiken Zero Na Ore Ga, Otsukiai Suru Hanashi / You Were Experienced, I Was Not: Our Dating Story V01-05 [J-Novel Club] [Antithetical]","path":"/RSSBOX/rss/81997030.html","content":"#1890614 | Keiken Zumi na Kimi to, Keiken Zero na Ore ga, Otsukiai Suru Hanashi / You Were Experienced, I Was Not: Our Dating Story v01-05 [J-Novel Club] [Antithetical] | 145.2 MiB | Literature - English-translated | C835777660B8405A25FDDA32D7F44A96B0D84237 Keiken Zumi na Kimi to, Keiken Zero na Ore ga, Otsukiai Suru Hanashi / You Were Experienced, I Was Not: Our Dating Story v01-05 [J-Novel Club] [Antithetical]https://nyaa.si/download/1890614.torrent","tags":["ACG","nyaa"],"categories":["ACG","nyaa"]},{"title":"Trillion Game - 05 「乙女の眼差し」 (TBS 1920x1080 X264 AAC).mkv","path":"/RSSBOX/rss/2e17acd3.html","content":"#1890612 | Trillion Game - 05 「乙女の眼差し」 (TBS 1920x1080 x264 AAC).mkv | 908.5 MiB | Anime - Raw | A9BB5655BCDFC4FE560E52EA767BCCB6938A3818 Trillion Game - 05 「乙女の眼差し」 (TBS 1920x1080 x264 AAC).mkvhttps://nyaa.si/download/1890612.torrent","tags":["ACG","nyaa"],"categories":["ACG","nyaa"]},{"title":"Sayonara Ryuusei, Konnichiwa Jinsei - 03 「女剣士」 (TBS 1920x1080 X264 AAC).mkv","path":"/RSSBOX/rss/7bd92795.html","content":"#1890611 | Sayonara Ryuusei, Konnichiwa Jinsei - 03 「女剣士」 (TBS 1920x1080 x264 AAC).mkv | 650.5 MiB | Anime - Raw | C924FA728BBD7ADFF2DFAE2324F061B04E3774E4 Sayonara Ryuusei, Konnichiwa Jinsei - 03 「女剣士」 (TBS 1920x1080 x264 AAC).mkvhttps://nyaa.si/download/1890611.torrent","tags":["ACG","nyaa"],"categories":["ACG","nyaa"]},{"title":"Aldnoah.Zero (2024) - 16 「熱砂の進撃 -Soldiers' Pay-」 (BS11 1920x1080 X264 AAC).mp4","path":"/RSSBOX/rss/da1442de.html","content":"#1890610 | Aldnoah.Zero (2024) - 16 「熱砂の進撃 -Soldiers' Pay-」 (BS11 1920x1080 x264 AAC).mp4 | 912.3 MiB | Anime - Raw | B778E9F8151D37956B2072F88930456CB789155B Aldnoah.Zero (2024) - 16 「熱砂の進撃 -Soldiers' Pay-」 (BS11 1920x1080 x264 AAC).mp4https://nyaa.si/download/1890610.torrent","tags":["ACG","nyaa"],"categories":["ACG","nyaa"]},{"title":"Kekkon Suru Tte, Hontou Desu Ka - 04 「人生を、分かち合えますか？」 (BS11 1920x1080 X264 AAC).mp4","path":"/RSSBOX/rss/129f1479.html","content":"#1890609 | Kekkon suru tte, Hontou desu ka - 04 「人生を、分かち合えますか？」 (BS11 1920x1080 x264 AAC).mp4 | 982.7 MiB | Anime - Raw | 3A2BE1E9AD22ED77B0D6EEBC241BB451E8EDE69E Kekkon suru tte, Hontou desu ka - 04 「人生を、分かち合えますか？」 (BS11 1920x1080 x264 AAC).mp4https://nyaa.si/download/1890609.torrent","tags":["ACG","nyaa"],"categories":["ACG","nyaa"]},{"title":"NEW FLOWER ALERT!","path":"/RSSBOX/rss/2bdf2ded.html","content":"#minecraftshorts #minecraft #shortsvideo #eyeblossom #palegarden NEW FLOWER ALERT!https://www.youtube.com/watch?v=patfiJmDh5s","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"不朽之爱与地底烂仗：女王心怀扭曲之爱，纳迦什面对无尽鼠潮 | 中古战锤国家地理 Vol.7","path":"/RSSBOX/rss/4d1424b2.html","content":"本期时间轴制作： 9S夜风老师又带来了新的一期中古战锤节目。夜风老师又带来了新的一期中古战锤节目。本期故事是双线叙事：在莱弥亚的王宫里，鲜血女王正在呵护她扭曲的爱恋之心，而本来剑指尼赫喀拉的纳迦什，发现自家的后院闹起了耗子…是的是的，耗子！ 不朽之爱与地底烂仗：女王心怀扭曲之爱，纳迦什面对无尽鼠潮 | 中古战锤国家地理 Vol.7https://www.gcores.com/radios/189747","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"《英雄联盟》S15全球总决赛将在中国举办","path":"/RSSBOX/rss/7571145b.html","content":"拳头游戏宣布，2025年《英雄联盟》S15全球总决赛将在中国举办。这将是继2017年、2020年以来，《英雄联盟》全球总决赛第三次落地中国。同时， OPPO 仍将担任全球总决赛和季中冠军赛最有价值选手（MVP）奖项的全球赞助伙伴。 &lt;内嵌内容，请前往机核查看&gt; 《英雄联盟》S15全球总决赛将在中国举办https://www.gcores.com/articles/189984","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"Billions and Billions (Of Logs): Scaling AI Gateway With the Cloudflare Developer Platform","path":"/RSSBOX/rss/cdf1362d.html","content":"With the rapid advancements occurring in the AI space, developers face significant challenges in keeping up with the ever-changing landscape. New models and providers are continuously emerging, and understandably, developers want to experiment and test these options to find the best fit for their use cases. This creates the need for a streamlined approach to managing multiple models and providers, as well as a centralized platform to efficiently monitor usage, implement controls, and gather data for optimization.AI Gateway is specifically designed to address these pain points. Since its launch in September 2023, AI Gateway has empowered developers and organizations by successfully proxying over 2 billion requests in just one year, as we highlighted during September’s Birthday Week. With AI Gateway, developers can easily store, analyze, and optimize their AI inference requests and responses in real time.With our initial architecture, AI Gateway faced a significant challenge: the logs, those critical trails of data interactions between applications and AI models, could only be retained for 30 minutes. This limitation was not just a minor inconvenience; it posed a substantial barrier for developers and businesses needing to analyze long-term patterns, ensure compliance, or simply debug over more extended periods.In this post, we'll explore the technical challenges and strategic decisions behind extending our log storage capabilities from 30 minutes to being able to store billions of logs indefinitely. We'll discuss the challenges of scale, the intricacies of data management, and how we've engineered a system that not only meets the demands of today, but is also scalable for the future of AI development. Background AI Gateway is built on Cloudflare Workers, a serverless platform that runs on the Cloudflare network, allowing developers to write small JavaScript functions that can execute at the point of need, near the user, on Cloudflare's vast network of data centers, without worrying about platform scalability. Our customers use multiple providers and models and are always looking to optimize the way they do inference. And, of course, in order to evaluate their prompts, performance, cost, and to troubleshoot what’s going on, AI Gateway’s customers need to store requests and responses. New requests show up within 15 seconds and customers can check a request’s cost, duration, number of tokens, and provide their feedback (thumbs up or down). This scales in a way where an account can have multiple gateways and each gateway has its own settings. In our first implementation, a backend worker was responsible for storing Real Time Logs and other background tasks. However, in the rapidly evolving domain of artificial intelligence, where real-time data is as precious as the insights it provides, managing log data efficiently becomes paramount. We recognized that to truly empower our users, we needed to offer a solution where logs weren't just transient records but could be stored permanently. Permanent log storage means developers can now track the performance, security, and operational insights of their AI applications over time, enabling not only immediate troubleshooting but also longitudinal studies of AI behavior, usage trends, and system health. The diagram above describes our old architecture, which could only store 30 minutes of data.Tracing the path of a request through the AI Gateway, as depicted in the sequence above:A developer sends a new inference request, which is first received by our Gateway Worker.The Gateway Worker then performs several checks: it looks for cached results, enforces rate limits, and verifies any other configurations set by the user for their gateway. Provided all conditions are met, it forwards the request to the selected inference provider (in this diagram, OpenAI).The inference provider processes the request and sends back the response.Simultaneously, as the response is relayed back to the developer, the request and response details are also dispatched to our Backend Worker. This worker's role is to manage and store the log of this transaction. The challenge: Store two billion logs First step: real-time logs Initially, the AI Gateway project stored both request metadata and the actual request bodies in a D1 database. This approach facilitated rapid development in the project's infancy. However, as customer engagement grew, the D1 database began to fill at an accelerating rate, eventually retaining logs for only 30 minutes at a time.To mitigate this, we first optimized the database schema, which extended the log retention to one hour. However, we soon encountered diminishing returns due to the sheer volume of byte data from the request bodies. Post-launch, it became clear that a more scalable solution was necessary. We decided to migrate the request bodies to R2 storage, significantly alleviating the data load on D1. This adjustment allowed us to incrementally extend log retention to 24 hours.Consequently, D1 functioned primarily as a log index, enabling users to search and filter logs efficiently. When users needed to view details or download a log, these actions were seamlessly proxied through to R2.This dual-system approach provided us with the breathing room to contemplate and develop more sophisticated storage solutions for the future. Second step: persistent logs and Durable Object transactional storage As our traffic surged, we encountered a growing number of requests from customers wanting to access and compare older logs.Upon learning that the Durable Objects team was seeking beta testers for their new Durable Objects with SQLite, we eagerly signed up.Originally, we considered Durable Objects as the ideal solution for expanding our log storage capacity, which required us to shard the logs by a unique string. Initially, this string was the account ID, but during a mid-development load test, we hit a cap at 10 million logs per Durable Object. This limitation meant that each account could only support up to this number of logs.Given our commitment to the DO migration, we saw an opportunity rather than a constraint. To overcome the 10 million log limit per account, we refined our approach to shard by both account ID and gateway name. This adjustment effectively raised the storage ceiling from 10 million logs per account to 10 million per gateway. With the default setting allowing each account up to 10 gateways, the potential storage for each account skyrocketed to 100 million logs.This strategic pivot not only enabled us to store a significantly larger number of logs. But also enhanced our flexibility in gateway management. Now, when a gateway is deleted, we can simply remove the corresponding Durable Object.Additionally, this sharding method isolates high-volume request scenarios. If one customer's heavy usage slows down log insertion, it only impacts their specific Durable Object, thereby preserving performance for other customers. Taking a glance at the revised architecture diagram, we replaced the Backend Worker with our newly integrated Durable Object. The rest of the request flow remains unchanged, including the concurrent response to the user and the interaction with the Durable Object, which occurs in the fourth step.Leveraging Cloudflare’s network, our Gateway Worker operates near the user's location, which in turn positions the user's Durable Object close by. This proximity significantly enhances the speed of log insertion and query operations. Third step: managing thousands of Durable Objects As the number of users and requests on AI Gateway grows, managing each unique Durable Object (DO) becomes increasingly complex. New customers join continuously, and we needed an efficient method to track each DO, ensure users stay within their 10 gateway limit, and manage the storage capacity for free users.To address these challenges, we introduced another layer of control with a new Durable Object we've named the Account Manager. The primary function of the Account Manager is straightforward yet crucial: it keeps user activities in check.Here's how it works: before any Gateway commits a new log to permanent storage, it consults the Account Manager. This check determines whether the gateway is allowed to insert the log based on the user's current usage and entitlements. The Account Manager uses its own SQLite database to verify the total number of rows a user has and their service level. If all checks pass, it signals the Gateway that the log can be inserted. It was paramount to guarantee that this entire validation process occurred in the background, ensuring that the user experience remains seamless and uninterrupted.The Account Manager stays updated by periodically receiving data from each Gateway’s Durable Object. Specifically, after every 1000 inference requests, the Gateway sends an update on its total rows to the Account Manager, which then updates its local records. This system ensures that the Account Manager has the most current data when making its decisions.Additionally, the Account Manager is responsible for monitoring customer entitlements. It tracks whether an account is on a free or paid plan, how many gateways a user is permitted to create, and the log storage capacity allocated to each gateway. Through these mechanisms, the Account Manager not only helps in maintaining system integrity but also ensures fair usage across all users of AI Gateway. AI evaluations and Durable Objects sharding As we continue to develop evaluations to fully automatic and, in the future, use Large Language Models (LLMs), we are now taking the first step towards this goal and launching the open beta phase of comprehensive AI evaluations, centered on Human-in-the-Loop feedback.This feature empowers users to create bespoke datasets from their application logs, thereby enabling them to score and evaluate the performance, speed, and cost-effectiveness of their models, with a primary focus on LLMs and automated scoring, analyzing the performance of LLMs, providing developers with objective, data-driven insights to refine their models.To do this, developers require a reliable logging mechanism that persists logs from multiple gateways, storing up to 100 million logs in total (10 million logs per gateway, across 10 gateways). This represents a significant volume of data, as each request made through the AI Gateway generates a log entry, with some log entries potentially exceeding 50 MB in size.This necessity leads us to work on the expansion of log storage capabilities. Since log storage is limited to 10 million logs per gateway, in future iterations, we aim to scale this capacity by implementing sharded Durable Objects (DO), allowing multiple Durable Objects per gateway to handle and store logs. This scaling strategy will enable us to store significantly larger volumes of logs, providing richer data for evaluations (using LLMs as a judge or from user input), all through AI Gateway. Coming Soon We are working on improving our existing Universal Endpoint, the next step on an enhanced solution that builds on existing fallback mechanisms to offer greater resilience, flexibility, and intelligence in request management.Currently, when a provider encounters an error or is unavailable, our system falls back to an alternative provider to ensure continuity. The improved Universal Endpoint takes this a step further by introducing automatic retry capabilities, allowing failed requests to be reattempted before fallback is triggered. This significantly improves reliability by handling transient errors and increasing the likelihood of successful request fulfillment. It will look something like this: curl --location 'https://aig.example.com/' \\ --header 'CF-AIG-TOKEN: Bearer XXXX' \\ --header 'Content-Type: application/json' \\ --data-raw '[ &#123; \"id\": \"0001\", \"provider\": \"openai\", \"endpoint\": \"chat/completions\", \"headers\": &#123; \"Authorization\": \"Bearer XXXX\", \"Content-Type\": \"application/json\" &#125;, \"query\": &#123; \"model\": \"gpt-3.5-turbo\", \"messages\": [ &#123; \"role\": \"user\", \"content\": \"generate a prompt to create cloudflare random images\" &#125; ] &#125;, \"option\": &#123; \"retry\": 2, \"delay\": 200, \"onComplete\": &#123; \"provider\": \"workers-ai\", \"endpoint\": \"@cf/stabilityai/stable-diffusion-xl-base-1.0\", \"headers\": &#123; \"Authorization\": \"Bearer A5UFQkHewHF1-sA3hTVQFaPxRuu5wmS0eJcCS_MC\", \"Content-Type\": \"application/json\" &#125;, \"query\": &#123; \"messages\": [ &#123; \"role\": \"user\", \"content\": \"&lt;prompt-response id='\\''0001'\\'' /&gt;\" &#125; ] &#125; &#125; &#125; &#125;, &#123; \"provider\": \"workers-ai\", \"endpoint\": \"@cf/stabilityai/stable-diffusion-xl-base-1.0\", \"headers\": &#123; \"Authorization\": \"Bearer XXXXXX\", \"Content-Type\": \"application/json\" &#125;, \"query\": &#123; \"messages\": [ &#123; \"role\": \"user\", \"content\": \"create a image of a missing cat\" &#125; ] &#125; &#125; ]' The request to the improved Universal Endpoint system demonstrates how it handles multiple providers with integrated retry mechanisms and fallback logic. In this example, the first request is sent to a provider like OpenAI, asking it to generate a text-to-image prompt. The “retry” option ensures that transient issues don’t result in immediate failure.The system’s ability to seamlessly switch between providers while applying retry strategies ensures higher reliability and robustness in managing requests. By leveraging fallback logic, the Improved Universal Endpoint can dynamically adapt to provider failures, ensuring that tasks are completed successfully even in complex, multi-step workflows.In addition to retry logic, we will have the ability to inspect requests and responses and make dynamic decisions based on the content of the result. This enables developers to create conditional workflows where the system can adapt its behavior depending on the nature of the response, creating a highly flexible and intelligent decision-making process.If you haven’t yet used AI Gateway, check out our developer documentation on how to get started. If you have any questions, reach out on our Discord channel. Billions and billions (of logs): scaling AI Gateway with the Cloudflare Developer Platformhttps://blog.cloudflare.com/billions-and-billions-of-logs-scaling-ai-gateway-with-the-cloudflare","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"Build Durable Applications on Cloudflare Workers: You Write the Workflows, We Take Care of the Rest","path":"/RSSBOX/rss/d8304cc8.html","content":"Workflows, Cloudflare’s durable execution engine that allows you to build reliable, repeatable multi-step applications that scale for you, is now in open beta. Any developer with a free or paid Workers plan can build and deploy a Workflow right now: no waitlist, no sign-up form, no fake line around-the-block.If you learn by doing, you can create your first Workflow via a single command (or visit the docs for the full guide): npm create cloudflare@latest workflows-starter -- \\ --template \"cloudflare/workflows-starter\" Open the src/index.ts file, poke around, start extending it, and deploy it with a quick wrangler deploy.If you want to learn more about how Workflows works, how you can use it to build applications, and how we built it, read on. Workflows? Durable Execution? Workflows—which we announced back during Developer Week earlier this year—is our take on the concept of “Durable Execution”: the ability to build and execute applications that are durable in the face of errors, network issues, upstream API outages, rate limits, and (most importantly) infrastructure failure.As over 2.4 million developers continue to build applications on top of Cloudflare Workers, R2, and Workers AI, we’ve noticed more developers building multi-step applications and workflows that process user data, transform unstructured data into structured, export metrics, persist state as they progress, and automatically retry &amp; restart. But writing any non-trivial application and making it durable in the face of failure is hard: this is where Workflows comes in. Workflows manages the retries, emitting the metrics, and durably storing the state (without you having to stand up your own database) as the Workflow progresses.What makes Workflows different from other takes on “Durable Execution” is that we manage the underlying compute and storage infrastructure for you. You’re not left managing a compute cluster and hoping it scales both up (on a Monday morning) and down (during quieter periods) to manage costs, or ensuring that you have compute running in the right locations. Workflows is built on Cloudflare Workers — our job is to run your code and operate the infrastructure for you.As an example of how Workflows can help you build durable applications, assume you want to post-process file uploads from your users that were uploaded to an R2 bucket directly via a pre-signed URL. That post-processing could involve multiple actions: text extraction via a Workers AI model, calls to a third-party API to validate data, updating or querying rows in a database once the file has been processed… the list goes on.But what each of these actions has in common is that it could fail. Maybe that upstream API is unavailable, maybe you get rate-limited, maybe your database is down. Having to write extensive retry logic around each action, manage backoffs, and (importantly) ensure your application doesn’t have to start from scratch when a later step fails is more boilerplate to write and more code to test and debug.What’s a step, you ask? The core building block of every Workflow is the step: an individually retriable component of your application that can optionally emit state. That state is then persisted, even if subsequent steps were to fail. This means that your application doesn’t have to restart, allowing it to not only recover more quickly from failure scenarios, but it can also avoid doing redundant work. You don’t want your application hammering an expensive third-party API (or getting you rate limited) because it’s naively retrying an API call that you don’t have to. export class MyWorkflow extends WorkflowEntrypoint&lt;Env, Params&gt; &#123; async run(event: WorkflowEvent&lt;Params&gt;, step: WorkflowStep) &#123; const files = await step.do('my first step', async () =&gt; &#123; return &#123; inputParams: event, files: [ 'doc_7392_rev3.pdf', 'report_x29_final.pdf', 'memo_2024_05_12.pdf', 'file_089_update.pdf', 'proj_alpha_v2.pdf', 'data_analysis_q2.pdf', 'notes_meeting_52.pdf', 'summary_fy24_draft.pdf', ], &#125;; &#125;); // Other steps... &amp;#125; &amp;#125; Notably, a Workflow can have hundreds of steps: one of the Rules of Workflows is to encapsulate every API call or stateful action within your application into its own step. Each step can also define its own retry strategy, automatically backing off, adding a delay and/or (eventually) giving up after a set number of attempts. await step.do( 'make a call to write that could maybe, just might, fail', // Define a retry strategy &#123; retries: &#123; limit: 5, delay: '5 seconds', backoff: 'exponential', &#125;, timeout: '15 minutes', &#125;, async () =&gt; &#123; // Do stuff here, with access to the state from our previous steps if (Math.random() &gt; 0.5) &#123; throw new Error('API call to $STORAGE_SYSTEM failed'); &#125; &#125;, ); To illustrate this further, imagine you have an application that reads text files from an R2 storage bucket, pre-processes the text into chunks, generates text embeddings using Workers AI, and then inserts those into a vector database (like Vectorize) for semantic search. In the Workflows programming model, each of those is a discrete step, and each can emit state. For example, each of the four actions below can be a discrete step.do call in a Workflow:Reading the files from storage and emitting the list of filenamesChunking the text and emitting the resultsGenerating text embeddingsUpserting them into Vectorize and capturing the result of a test queryYou can also start to imagine that some steps, such as chunking text or generating text embeddings, can be broken down into even more steps — a step per file that we chunk, or a step per API call to our text embedding model, so that our application is even more resilient to failure.Steps can be created programmatically or conditionally based on input, allowing you to dynamically create steps based on the number of inputs your application needs to process. You do not need to define all steps ahead of time, and each instance of a Workflow may choose to conditionally create steps on the fly. Building Cloudflare on Cloudflare As the Cloudflare Developer platform continues to grow, almost all of our own products are built on top of it. Workflows is yet another example of how we built a new product from scratch using nothing but Workers and its vast catalog of features and APIs. This section of the blog has two goals: to explain how we built it, and to demonstrate that anyone can create a complex application or platform with demanding requirements and multiple architectural layers on our stack, too.If you’re wondering how Workflows manages to make durable execution easy, how it persists state, and how it automatically scales: it’s because we built it on Cloudflare Workers, including the brand-new zero-latency SQLite storage we recently introduced to Durable Objects. To understand how Workflows uses Workers &amp; Durable Objects, here’s the high-level overview of our architecture: There are three main blocks in this diagram:The user-facing APIs are where the user interacts with the platform, creating and deploying new workflows or instances, controlling them, and accessing their state and activity logs. These operations can be executed through our public API gateway using REST calls, a Worker script using bindings, Wrangler (Cloudflare's developer platform command line tool), or via the Dashboard user interface.The managed platform holds the internal configuration APIs running on a Worker implementing a catalog of REST endpoints, the binding shim, which is supported by another dedicated Worker, every account controller, and their correspondent workflow engines, all powered by SQLite-backed Durable Objects. This is where all the magic happens and what we are sharing more details about in this technical blog.Finally, there are the workflow instances, essentially independent clones of the workflow application. Instances are user account-owned and have a one-to-one relationship with a managed engine that powers them. You can run as many instances and engines as you want concurrently.Let's get into more detail… Configuration API and Binding Shim The Configuration API and the Binding Shim are two stateless Workers; one receives REST API calls from clients calling our API Gateway directly, using Wrangler, or navigating the Dashboard UI, and the other is the endpoint for the Workflows binding, an efficient and authenticated interface to interact with the Cloudflare Developer Platform resources from a Workers script.The configuration API worker uses HonoJS and Zod to implement the REST endpoints, which are declared in an OpenAPI schema and exported to our API Gateway, thus adding our methods to the Cloudflare API catalog. import &#123; swaggerUI &#125; from '@hono/swagger-ui'; import &#123; createRoute, OpenAPIHono, z &#125; from '@hono/zod-openapi'; import &#123; Hono &#125; from 'hono'; … ​​api.openapi( createRoute(&amp;#123; method: ‘get’, path: ‘&#x2F;‘, request: &amp;#123; query: PaginationParams, &amp;#125;, responses: &amp;#123; 200: &amp;#123; content: &amp;#123; ‘application&#x2F;json’: &amp;#123; schema: APISchemaSuccess(z.array(WorkflowWithInstancesCountSchema)), &amp;#125;, &amp;#125;, description: ‘List of all Workflows belonging to a account.’, &amp;#125;, &amp;#125;, &amp;#125;), async (ctx) &#x3D;&gt; &amp;#123; … &amp;#125;,); … api.route(‘&#x2F;:workflow_name’, routes.workflows);api.route(‘&#x2F;:workflow_name&#x2F;instances’, routes.instances);api.route(‘&#x2F;:workflow_name&#x2F;versions’, routes.versions); These Workers perform two different functions, but they share a large portion of their code and implement similar logic; once the request is authenticated and ready to travel to the next stage, they use the account ID to delegate the operation to a Durable Object called Account Controller. // env.ACCOUNTS is the Account Controllers Durable Objects namespace const accountStubId = c.env.ACCOUNTS.idFromName(accountId.toString()); const accountStub = c.env.ACCOUNTS.get(accountStubId); As you can see, every account has its own Account Controller Durable Object. Account Controllers The Account Controller is a dedicated persisted database that stores the list of all the account’s workflows, versions, and instances. We scale to millions of account controllers, one per every Cloudflare account using Workflows, by leveraging the power of Durable Objects with SQLite backend.Durable Objects (DOs) are single-threaded singletons that run in our data centers and are bound to a stateful storage API, in this case, SQLite. They are also Workers, just a special kind, and have access to all of our other APIs. This makes it easy to build consistent, highly available distributed applications with them.Here’s what we get for free by using one Durable Object per Workflows account:Sharding based on account boundaries aligns perfectly with the way we manage resources at Cloudflare internally. Also, due to the nature of DOs, there are other things that this model gets us for free: Not that we expect them, but eventual bugs or state inconsistencies during beta are confined to the affected account, and don’t impact everyone.DO instances run close to the end user; Alice is in London and will call the config API through our LHR data center, while Bob is in Lisbon and will connect to LIS.Because every account is a Worker, we can gradually upgrade them to new versions, starting with the internal users, thus derisking real customers.Before SQLite, our only option was to use the Durable Object's key-value storage API, but having a relational database at our fingertips and being able to create tables and do complex queries is a significant enabler. For example, take a look at how we implement the internal method getWorkflow(): async function getWorkflow(accountId: number, workflowName: string) &#123; try &#123; const res = this.ctx.storage.transactionSync(() =&gt; &#123; const cursor = Array.from( this.ctx.storage.sql.exec( ` SELECT *, (SELECT class_name FROM versions WHERE workflow_id = w.id ORDER BY created_on DESC LIMIT 1) AS class_name FROM workflows w WHERE w.name = ? `, workflowName ) )[0] as Workflow; return cursor; &amp;#125;); this.sendAnalytics(accountId, begin, &quot;getWorkflow&quot;); return res as Workflow | undefined; &amp;#125; catch (err) &amp;#123; this.sendErrorAnalytics(accountId, begin, “getWorkflow”); throw err; &amp;#125;&amp;#125; The other thing we take advantage of in Workflows is using the recently announced JavaScript-native RPC feature when communicating between components.Before RPC, we had to fetch() between components, make HTTP requests, and serialize and deserialize the parameters and the payload. Now, we can async call the remote object's method as if it was local. Not only does this feel more natural and simplify our logic, but it's also more efficient, and we can take advantage of TypeScript type-checking when writing code.This is how the Configuration API would call the Account Controller’s countWorkflows() method before: const resp = await accountStub.fetch( \"https://controller/count-workflows\", &#123; method: \"POST\", headers: &#123; \"Content-Type\": \"application/json; charset=utf-8\", &#125;, body: JSON.stringify(&#123; accountId &#125;), &#125;, ); if (!resp.ok) &amp;#123; return new Response(“Internal Server Error”, &amp;#123; status: 500 &amp;#125;);&amp;#125; const result &#x3D; await resp.json();const total_count &#x3D; result.total_count; This is how we do it using RPC: const total_count = await accountStub.countWorkflows(accountId); The other powerful feature of our RPC system is that it supports passing not only Structured Cloneable objects back and forth but also entire classes. More on this later.Let’s move on to Engine. Engine and instance Every instance of a workflow runs alongside an Engine instance. The Engine is responsible for starting up the user’s workflow entry point, executing the steps on behalf of the user, handling their results, and tracking the workflow state until completion. When we started thinking about the Engine, we thought about modeling it after a state machine, and that was what our initial prototypes looked like. However, state machines require an ahead-of-time understanding of the userland code, which implies having a build step before running them. This is costly at scale and introduces additional complexity.A few iterations later, we had another idea. What if we could model the engine as a game loop?Unlike other computer programs, games operate regardless of a user's input. The game loop is essentially a sequence of tasks that implement the game's logic and update the display, typically one loop per video frame. Here’s an example of a game loop in pseudo-code: while (game in running) check for user input move graphics play sounds end while Well, an oversimplified version of our Workflow engine would look like this: while (last step not completed) iterate every step use memoized cache as response if the step has run already continue running step or timer if it hasn't finished yet end while A workflow is indeed a loop that keeps on going, performing the same sequence of logical tasks until the last step completes.The Engine and the instance run hand-in-hand in a one-to-one relationship. The first is managed, and part of the platform. It uses SQLite and other platform APIs internally, and we can constantly add new features, fix bugs, and deploy new versions, while keeping everything transparent to the end user. The second is the actual account-owned Worker script that declares the Workflow steps.For example, when someone passes a callback into step.do(): export class MyWorkflow extends WorkflowEntrypoint&lt;Env, Params&gt; &#123; async run(event: WorkflowEvent&lt;Params&gt;, step: WorkflowStep) &#123; step.do('step1', () =&gt; &#123; ... &#125;); &#125; &#125; We switch execution over to the Engine. Again, this is possible because of the power of JS RPC. Besides passing Structured Cloneable objects back and forth, JS RPC allows us to create and pass entire application-defined classes that extend the built-in RpcTarget. So this is what happens behind the scenes when your Instance calls step.do() (simplified): export class Context extends RpcTarget &#123; async do&lt;T&gt;(name: string, callback: () &#x3D;&gt; Promise&lt;T&gt;): Promise&lt;T&gt; &amp;#123; // First we check we have a cache of this step.do() already const maybeResult = await this.#state.storage.get(name); // We return the cache if it exists if (maybeValue) &amp;#123; return maybeValue; &amp;#125; // Else we run the user callback return doWrapper(callback); &amp;#125; &amp;#125; Here’s a more complete diagram of the Engine’s step.do() lifecycle: Again, this diagram only partially represents everything we do in the Engine; things like logging for observability or handling exceptions are missing, and we don't get into the details of how queuing is implemented. However, it gives you a good idea of how the Engine abstracts and handles all the complexities of completing a step under the hood, allowing us to expose a simple-to-use API to end users.Also, it's worth reiterating that every workflow instance is an Engine behind the scenes, and every Engine is an SQLite-backed Durable Object. This ensures that every instance runtime and state are isolated and independent of each other and that we can effortlessly scale to run billions of workflow instances, a solved problem for Durable Objects. Durability Durable Execution is all the rage now when we talk about workflow engines, and ours is no exception. Workflows are typically long-lived processes that run multiple functions in sequence where anything can happen. Those functions can time out or fail because of a remote server error or a network issue and need to be retried. A workflow engine ensures that your application runs smoothly and completes regardless of the problems it encounters.Durability means that if and when a workflow fails, the Engine can re-run it, resume from the last recorded step, and deterministically re-calculate the state from all the successful steps' cached responses. This is possible because steps are stateful and idempotent; they produce the same result no matter how many times we run them, thus not causing unintended duplicate effects like sending the same invoice to a customer multiple times. We ensure durability and handle failures and retries by sharing the same technique we use for a step.sleep() that requires sleeping for days or months: a combination of using scheduler.wait(), a method of the upcoming WICG Scheduling API that we already support, and Durable Objects alarms, which allow you to schedule the Durable Object to be woken up at a time in the future.These two APIs allow us to overcome the lack of guarantees that a Durable Object runs forever, giving us complete control of its lifecycle. Since every state transition through userland code persists in the Engine’s strongly consistent SQLite, we track timestamps when a step begins execution, its attempts (if it needs retries), and its completion. This means that steps pending if a Durable Object is evicted — perhaps due to a two-month-long timer — get rerun on the next lifetime of the Engine (with its cache from the previous lifetime hydrated) that is triggered by an alarm set with the timestamp of the next expected state transition. Real-life workflow, step by step Let's walk through an example of a real-life application. You run an e-commerce website and would like to send email reminders to your customers for forgotten carts that haven't been checked out in a few days.What would typically have to be a combination of a queue, a cron job, and querying a database table periodically can now simply be a Workflow that we start on every new cart: import &#123; WorkflowEntrypoint, WorkflowEvent, WorkflowStep, &#125; from \"cloudflare:workers\"; import &#123; sendEmail &#125; from \"./legacy-email-provider\"; type Params &#x3D; &amp;#123; cartId: string;&amp;#125;; type Env &#x3D; &amp;#123; DB: D1Database;&amp;#125;; export class Purchase extends WorkflowEntrypoint&lt;Env, Params&gt; &amp;#123; async run( event: WorkflowEvent&lt;Params&gt;, step: WorkflowStep ): Promise&lt;unknown&gt; &amp;#123; await step.sleep(“wait for three days”, “3 days”); // Retrieve cart from D1 const cart = await step.do(&quot;retrieve cart from database&quot;, async () =&amp;gt; &amp;#123; const &amp;#123; results &amp;#125; = await this.env.DB.prepare(`SELECT * FROM cart WHERE id = ?`) .bind(event.payload.cartId) .all(); return results[0]; &amp;#125;); if (!cart.checkedOut) &amp;#123; await step.do(&quot;send an email&quot;, async () =&amp;gt; &amp;#123; await sendEmail(&quot;reminder&quot;, cart); &amp;#125;); &amp;#125; &amp;#125;&amp;#125; This works great. However, sometimes the sendEmail function fails due to an upstream provider erroring out. While step.do automatically retries with a reasonable default configuration, we can define our settings: if (cart.isComplete) &#123; await step.do( \"send an email\", &#123; retries: &#123; limit: 5, delay: \"1 min\", backoff: \"exponential\", &#125;, &#125;, async () =&gt; &#123; await sendEmail(\"reminder\", cart); &#125; ); &#125; Managing Workflows Workflows allows us to create and manage workflows using four different interfaces:Using our REST HTTP API available on Cloudflare’s API catalogUsing Wrangler, Cloudflare's developer platform command-line toolProgrammatically inside a Worker using bindingsUsing our Web UI in the dashboardThe HTTP API makes it easy to trigger new instances of workflows from any system, even if it isn’t on Cloudflare, or from the command line. For example: curl --request POST \\ --url https://api.cloudflare.com/client/v4/accounts/$ACCOUNT_ID/workflows/purchase-workflow/instances/$CART_INSTANCE_ID \\ --header 'Authorization: Bearer $ACCOUNT_TOKEN \\ --header 'Content-Type: application/json' \\ --data '&#123; \"id\": \"$CART_INSTANCE_ID\", \"params\": &#123; \"cartId\": \"f3bcc11b-2833-41fb-847f-1b19469139d1\" &#125; &#125;' Wrangler goes one step further and gives us a friendlier set of commands to interact with workflows with fancy formatted outputs without needing to authenticate with tokens. Type npx wrangler workflows for help, or: npx wrangler workflows trigger purchase-workflow '&#123; \"cartId\": \"f3bcc11b-2833-41fb-847f-1b19469139d1\" &#125;' Furthermore, Workflows has first-party support in wrangler, and you can test your instances locally. A Workflow is similar to a regular WorkerEntrypoint in your Worker, which means that wrangler dev just naturally works. ❯ npx wrangler dev ⛅️ wrangler 3.82.0Your worker has access to the following bindings: Workflows: CART_WORKFLOW: EcommerceCartWorkflow⎔ Starting local server…[wrangler:inf] Ready on http://localhost:8787╭───────────────────────────────────────────────╮│ [b] open a browser, [d] open devtools │╰───────────────────────────────────────────────╯Workflow APIs are also available as a Worker binding. You can interact with the platform programmatically from another Worker script in the same account without worrying about permissions or authentication. You can even have workflows that call and interact with other workflows. import &#123; WorkerEntrypoint &#125; from \"cloudflare:workers\"; type Env &#x3D; &amp;#123; DEMO_WORKFLOW: Workflow &amp;#125;;export default class extends WorkerEntrypoint&lt;Env&gt; &amp;#123; async fetch() &amp;#123; &#x2F;&#x2F; Pass in a user defined name for this instance &#x2F;&#x2F; In this case, we use the same as the cartId const instance &#x3D; await this.env.DEMO_WORKFLOW.create(&amp;#123; id: “f3bcc11b-2833-41fb-847f-1b19469139d1”, params: &amp;#123; cartId: “f3bcc11b-2833-41fb-847f-1b19469139d1”, &amp;#125; &amp;#125;); &amp;#125; async scheduled() &amp;#123; &#x2F;&#x2F; Restart errored out instances in a cron const instance &#x3D; await this.env.DEMO_WORKFLOW.get( “f3bcc11b-2833-41fb-847f-1b19469139d1” ); const status &#x3D; await instance.status(); if (status.error) &amp;#123; await instance.restart(); &amp;#125; &amp;#125;&amp;#125; Observability Having good observability and data on often long-lived asynchronous tasks is crucial to understanding how we're doing under normal operation and, more importantly, when things go south, and we need to troubleshoot problems or when we are iterating on code changes.We designed Workflows around the philosophy that there is no such thing as too much logging. You can get all the SQLite data for your workflow and its instances by calling the REST APIs. Here is the output of an instance: &#123; \"success\": true, \"errors\": [], \"messages\": [], \"result\": &#123; \"status\": \"running\", \"params\": &#123;&#125;, \"trigger\": &#123; \"source\": \"api\" &#125;, \"versionId\": \"ae042999-39ff-4d27-bbcd-22e03c7c4d02\", \"queued\": \"2024-10-21 17:15:09.350\", \"start\": \"2024-10-21 17:15:09.350\", \"end\": null, \"success\": null, \"steps\": [ &#123; \"name\": \"send email\", \"start\": \"2024-10-21 17:15:09.411\", \"end\": \"2024-10-21 17:15:09.678\", \"attempts\": [ &#123; \"start\": \"2024-10-21 17:15:09.411\", \"end\": \"2024-10-21 17:15:09.678\", \"success\": true, \"error\": null &#125; ], \"config\": &#123; \"retries\": &#123; \"limit\": 5, \"delay\": 1000, \"backoff\": \"constant\" &#125;, \"timeout\": \"15 minutes\" &#125;, \"output\": \"celso@example.com\", \"success\": true, \"type\": \"step\" &#125;, &#123; \"name\": \"sleep-1\", \"start\": \"2024-10-21 17:15:09.763\", \"end\": \"2024-10-21 17:17:09.763\", \"finished\": false, \"type\": \"sleep\", \"error\": null &#125; ], \"error\": null, \"output\": null &#125; &#125; As you can see, this is essentially a dump of the instance engine SQLite in JSON. You have the errors, messages, current status, and what happened with every step, all time stamped to the millisecond.It's one thing to get data about a specific workflow instance, but it's another to zoom out and look at aggregated statistics of all your workflows and instances over time. Workflows data is available through our GraphQL Analytics API, so you can query it in aggregate and generate valuable insights and reports. In this example we ask for aggregated analytics about the wall time of all the instances of the “e-commerce-carts” workflow: &#123; viewer &#123; accounts(filter: &#123; accountTag: \"febf0b1a15b0ec222a614a1f9ac0f0123\" &#125;) &#123; wallTime: workflowsAdaptiveGroups( limit: 10000 filter: &#123; datetimeHour_geq: \"2024-10-20T12:00:00.000Z\" datetimeHour_leq: \"2024-10-21T12:00:00.000Z\" workflowName: \"e-commerce-carts\" &#125; orderBy: [count_DESC] ) &#123; count sum &#123; wallTime &#125; dimensions &#123; date: datetimeHour &#125; &#125; &#125; &#125; &#125; For convenience, you can evidently also use Wrangler to describe a workflow or an instance and get an instant and beautifully formatted response: sid ~ npx wrangler workflows instances describe purchase-workflow latest ⛅️ wrangler 3.80.4 Workflow Name: purchase-workflowInstance Id: d4280218-7756-41d2-bccd-8d647b82d7ceVersion Id: 0c07dbc4-aaf3-44a9-9fd0-29437ed11ff6Status: ✅ CompletedTrigger: 🌎 APIQueued: 14&#x2F;10&#x2F;2024, 16:25:17Success: ✅ YesStart: 14&#x2F;10&#x2F;2024, 16:25:17End: 14&#x2F;10&#x2F;2024, 16:26:17Duration: 1 minuteLast Successful Step: wait for three daysOutput: falseSteps: Name: wait for three days Type: 💤 Sleeping Start: 14&#x2F;10&#x2F;2024, 16:25:17 End: 17&#x2F;10&#x2F;2024, 16:25:17 Duration: 3 day And finally, we worked really hard to get you the best dashboard UI experience when navigating Workflows data. So, how much does it cost? It’d be painful if we introduced a powerful new way to build Workers applications but made it cost prohibitive.Workflows is priced just like Cloudflare Workers, where we introduced CPU-based pricing: only on active CPU time and requests, not duration (aka: wall time). Workers Standard pricing modelThis is especially advantageous when building the long-running, multi-step applications that Workflows enables: if you had to pay while your Workflow was sleeping, waiting on an event, or making a network call to an API, writing the “right” code would be at odds with writing affordable code.There’s also no need to keep a Kubernetes cluster or a group of virtual machines running (and burning a hole in your wallet): we manage the infrastructure, and you only pay for the compute your Workflows consume. What’s next? Today, after months of developing the platform, we are announcing the open beta program, and we couldn't be more excited to see how you will be using Workflows. Looking forward, we want to do things like triggering instances from queue messages and have other ideas, but at the same time, we are certain that your feedback will help us shape the roadmap ahead.We hope that this blog post gets you thinking about how to use Workflows for your next application, but also that it inspires you on what you can build on top of Workers. Workflows as a platform is entirely built on top of Workers, its resources, and APIs. Anyone can do it, too.To chat with the team and other developers building on Workflows, join the #workflows-beta channel on the Cloudflare Developer Discord, and keep an eye on the Workflows changelog during the beta. Otherwise, visit the Workflows tutorial to get started.If you're an engineer, look for opportunities to work with us and help us improve Workflows or build other products. Build durable applications on Cloudflare Workers: you write the Workflows, we take care of the resthttps://blog.cloudflare.com/building-workflows-durable-execution-on-workers","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"Durable Objects Aren't Just Durable, They're Fast: A 10x Speedup for Cloudflare Queues","path":"/RSSBOX/rss/15e77533.html","content":"Cloudflare Queues let a developer decouple their Workers into event-driven services. Producer Workers write events to a Queue, and consumer Workers are invoked to take actions on the events. For example, you can use a Queue to decouple an e-commerce website from a service which sends purchase confirmation emails to users. During 2024’s Birthday Week, we announced that Cloudflare Queues is now Generally Available, with significant performance improvements that enable larger workloads. To accomplish this, we switched to a new architecture for Queues that enabled the following improvements:Median latency for sending messages has dropped from ~200ms to ~60msMaximum throughput for each Queue has increased over 10x, from 400 to 5000 messages per secondMaximum Consumer concurrency for each Queue has increased from 20 to 250 concurrent invocations Median latency drops from ~200ms to ~60ms as Queues are migrated to the new architectureIn this blog post, we'll share details about how we built Queues using Durable Objects and the Cloudflare Developer Platform, and how we migrated from an initial Beta architecture to a geographically-distributed, horizontally-scalable architecture for General Availability. v1 Beta architecture When initially designing Cloudflare Queues, we decided to build something simple that we could get into users' hands quickly. First, we considered leveraging an off-the-shelf messaging system such as Kafka or Pulsar. However, we decided that it would be too challenging to operate these systems at scale with the large number of isolated tenants that we wanted to support.Instead of investing in new infrastructure, we decided to build on top of one of Cloudflare's existing developer platform building blocks: Durable Objects. Durable Objects are a simple, yet powerful building block for coordination and storage in a distributed system. In our initial v1 architecture, each Queue was implemented using a single Durable Object. As shown below, clients would send messages to a Worker running in their region, which would be forwarded to the single Durable Object hosted in the WNAM (Western North America) region. We used a single Durable Object for simplicity, and hosted it in WNAM for proximity to our centralized configuration API service. One of a Queue's main responsibilities is to accept and store incoming messages. Sending a message to a v1 Queue used the following flow:A client sends a POST request containing the message body to the Queues API at /accounts/:accountID/queues/:queueID/messagesThe request is handled by an instance of the Queue Broker Worker in a Cloudflare data center running near the client.The Worker performs authentication, and then uses Durable Objects idFromName API to route the request to the Queue Durable Object for the given queueIDThe Queue Durable Object persists the message to storage before returning a success back to the client.Durable Objects handled most of the heavy-lifting here: we did not need to set up any new servers, storage, or service discovery infrastructure. To route requests, we simply provided a queueID and the platform handled the rest. To store messages, we used the Durable Object storage API to put each message, and the platform handled reliably storing the data redundantly. Consuming messages The other main responsibility of a Queue is to deliver messages to a Consumer. Delivering messages in a v1 Queue used the following process:Each Queue Durable Object maintained an alarm that was always set when there were undelivered messages in storage. The alarm guaranteed that the Durable Object would reliably wake up to deliver any messages in storage, even in the presence of failures. The alarm time was configured to fire after the user's selected max wait time, if only a partial batch of messages was available. Whenever one or more full batches were available in storage, the alarm was scheduled to fire immediately.The alarm would wake the Durable Object, which continually looked for batches of messages in storage to deliver.Each batch of messages was sent to a \"Dispatcher Worker\" that used Workers for Platforms dynamic dispatch to pass the messages to the queue() function defined in a user's Consumer Worker This v1 architecture let us flesh out the initial version of the Queues Beta product and onboard users quickly. Using Durable Objects allowed us to focus on building application logic, instead of complex low-level systems challenges such as global routing and guaranteed durability for storage. Using a separate Durable Object for each Queue allowed us to host an essentially unlimited number of Queues, and provided isolation between them.However, using only one Durable Object per queue had some significant limitations:Latency: we created all of our v1 Queue Durable Objects in Western North America. Messages sent from distant regions incurred significant latency when traversing the globe.Throughput: A single Durable Object is not scalable: it is single-threaded and has a fixed capacity for how many requests per second it can process. This is where the previous 400 messages per second limit came from.Consumer Concurrency: Due to concurrent subrequest limits, a single Durable Object was limited in how many concurrent subrequests it could make to our Dispatcher Worker. This limited the number of queue() handler invocations that it could run simultaneously.To solve these issues, we created a new v2 architecture that horizontally scales across multiple Durable Objects to implement each single high-performance Queue. v2 Architecture In the new v2 architecture for Queues, each Queue is implemented using multiple Durable Objects, instead of just one. Instead of a single region, we place Storage Shard Durable Objects in all available regions to enable lower latency. Within each region, we create multiple Storage Shards and load balance incoming requests amongst them. Just like that, we’ve multiplied message throughput. Sending a message to a v2 Queue uses the following flow:A client sends a POST request containing the message body to the Queues API at /accounts/:accountID/queues/:queueID/messagesThe request is handled by an instance of the Queue Broker Worker running in a Cloudflare data center near the client.The Worker:Performs authenticationReads from Workers KV to obtain a Shard Map that lists available storage shards for the given region and queueIDPicks one of the region's Storage Shards at random, and uses Durable Objects idFromName API to route the request to the chosen shardThe Storage Shard persists the message to storage before returning a success back to the client.In this v2 architecture, messages are stored in the closest available Durable Object storage cluster near the user, greatly reducing latency since messages don't need to be shipped all the way to WNAM. Using multiple shards within each region removes the bottleneck of a single Durable Object, and allows us to scale each Queue horizontally to accept even more messages per second. Workers KV acts as a fast metadata store: our Worker can quickly look up the shard map to perform load balancing across shards.To improve the Consumer side of v2 Queues, we used a similar \"scale out\" approach. A single Durable Object can only perform a limited number of concurrent subrequests. In v1 Queues, this limited the number of concurrent subrequests we could make to our Dispatcher Worker. To work around this, we created a new Consumer Shard Durable Object class that we can scale horizontally, enabling us to execute many more concurrent instances of our users' queue() handlers. Consumer Durable Objects in v2 Queues use the following approach:Each Consumer maintains an alarm that guarantees it will wake up to process any pending messages. v2 Consumers are notified by the Queue's Coordinator (introduced below) when there are messages ready for consumption. Upon notification, the Consumer sets an alarm to go off immediately.The Consumer looks at the shard map, which contains information about the storage shards that exist for the Queue, including the number of available messages on each shard.The Consumer picks a random storage shard with available messages, and asks for a batch.The Consumer sends the batch to the Dispatcher Worker, just like for v1 Queues.After processing the messages, the Consumer sends another request to the Storage Shard to either \"acknowledge\" or \"retry\" the messages.This scale-out approach enabled us to work around the subrequest limits of a single Durable Object, and increase the maximum supported concurrency level of a Queue from 20 to 250. The Coordinator and “Control Plane” So far, we have primarily discussed the \"Data Plane\" of a v2 Queue: how messages are load balanced amongst Storage Shards, and how Consumer Shards read and deliver messages. The other main piece of a v2 Queue is the \"Control Plane\", which handles creating and managing all the individual Durable Objects in the system. In our v2 architecture, each Queue has a single Coordinator Durable Object that acts as the brain of the Queue. Requests to create a Queue, or change its settings, are sent to the Queue's Coordinator. The Coordinator maintains a Shard Map for the Queue, which includes metadata about all the Durable Objects in the Queue (including their region, number of available messages, current estimated load, etc.). The Coordinator periodically writes a fresh copy of the Shard Map into Workers KV, as pictured in step 1 of the diagram. Placing the shard map into Workers KV ensures that it is globally cached and available for our Worker to read quickly, so that it can pick a shard to accept the message.Every shard in the system periodically sends a heartbeat to the Coordinator as shown in steps 2 and 3 of the diagram. Both Storage Shards and Consumer Shards send heartbeats, including information like the number of messages stored locally, and the current load (requests per second) that the shard is handling. The Coordinator uses this information to perform autoscaling. When it detects that the shards in a particular region are overloaded, it creates additional shards in the region, and adds them to the shard map in Workers KV. Our Worker sees the updated shard map and naturally load balances messages across the freshly added shards. Similarly, the Coordinator looks at the backlog of available messages in the Queue, and decides to add more Consumer shards to increase Consumer throughput when the backlog is growing. Consumer Shards pull messages from Storage Shards for processing as shown in step 4 of the diagram.Switching to a new scalable architecture allowed us to meet our performance goals and take Queues to GA. As a recap, this new architecture delivered these significant improvements:P50 latency for writing to a Queue has dropped from ~200ms to ~60ms.Maximum throughput for a Queue has increased from 400 to 5000 messages per second.Maximum consumer concurrency has increased from 20 to 250 invocations. What's next for Queues We plan on leveraging the performance improvements in the new beta version of Durable Objects which use SQLite to continue to improve throughput/latency in Queues.We will soon be adding message management features to Queues so that you can take actions to purge messages in a queue, pause consumption of messages, or “redrive”/move messages from one queue to another (for example messages that have been sent to a Dead Letter Queue could be “redriven” or moved back to the original queue).Work to make Queues the \"event hub\" for the Cloudflare Developer Platform:Create a low-friction way for events emitted from other Cloudflare services with event schemas to be sent to Queues.Build multi-Consumer support for Queues so that Queues are no longer limited to one Consumer per queue.To start using Queues, head over to our Getting Started guide. Do distributed systems like Cloudflare Queues and Durable Objects interest you? Would you like to help build them at Cloudflare? We're Hiring! Durable Objects aren't just durable, they're fast: a 10x speedup for Cloudflare Queueshttps://blog.cloudflare.com/how-we-built-cloudflare-queues","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"《潜水员戴夫》三大联动更新上线，限时史低折扣同步开启","path":"/RSSBOX/rss/e3e35866.html","content":"今日，海洋冒险游戏《潜水员戴夫》（DAVE THE DIVER）正式迎来全新的联动更新！先前在8月21日举办的2024科隆国际游戏展上，韩国MINTROCKET工作室就已经预告过此次更新的三大特别合作，联动对象分别是：人气肉鸽卡牌游戏《Balatro》、炼金模拟游戏《Potion Craft》 和美国创作型歌手 mxmtoon。此次更新将免费放送。此外，10月24日至11月1日期间，《潜水员戴夫》游戏本体会在Steam、PlayStation以及Nintendo平台上开启限时史低优惠7折！联动的两款游戏还会和《潜水员戴夫》共享一个捆绑包优惠，限时9折。 具体联动更新内容：1. 《Potion Craft》联动 在本次更新之后，玩家可以在班乔寿司店遇见游戏《Potion Craft》中的经典角色们。通过游走的炼金术士NPC，可以获取全新的增益药水和鱼饵制作工具。同时，还可以从蘑菇商人处购买新的食材和独特配方。 2. 《Balatro》 联动 人气卡牌构筑游戏《Balatro》已经风靡了鲛人村。《Balatro》的主角Jimbo会将游戏带到村里，邀请玩家和村民们一起挑战，过关即可赢得特殊护符，解锁更多独特的联动奖励。 3. mxmtoon 联动 美国创作型歌手兼YouTuber Mxmtoon 也光临了班乔寿司店，只为寻找特别的料理灵感。戴夫和班乔将为她精心烹饪一道由“马粪海胆”组成的美味料理。当你完成料理后，还能欣赏到一场由Mxmtoon亲自演唱的迷你音乐会，感受音乐与美食的完美融合！ 《潜水员戴夫》三大联动更新上线，限时史低折扣同步开启https://www.gcores.com/articles/189978","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"Attacking Browser Extensions","path":"/RSSBOX/rss/7a4438ab.html","content":"Browser extensions first became mainstream in the early 2000s with their adoption by Firefox and Chromium and their popularity has been growing ever since. Nowadays, it is common for even the average user to have at least one extension installed, often an adblocker. Research into the security of browser extensions is mostly scattered around between individual bug reports and coverage on malicious chrome extensions. In this blog, I will introduce the structure of a browser extension and the vulnerabilities that are present in the ecosystem. I will then discuss the progression of security in the extension space, highlighting the attack surface and its relationship with mitigations that have been implemented. Lastly, I will recommend some CodeQL queries and best practices that users, developers and researchers can use to ensure the security of their extension. The extension structure Mozilla and Google, and their respective browsers, Firefox and Chromium, set the standard for most browser extensions (note, we will not cover Apple’s Safari here). Throughout this blog, I will talk about extension core concepts, and highlight the differences between Firefox and Chromium. The differences between Firefox and Chromium are manifested in the differences in policy on what is allowed on the corresponding extension stores and how extensions interact with the browser, which ultimately decide the security and safety of extension for the end user. A browser extension is a group of HTML, CSS, and JavaScript files that work together to enhance the browsing experience. Usually, the code runs in its own domain, the domain labeled by the extensions ID. For example, the Chromium extension uBlock origin https://chromewebstore.google.com/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm will run in the domain cjpalhdlnbpafiamejdnhcphjbkeiagm, which the Chromium web store makes obvious via the URL. Extension URLs vary depending on the browser, but generally follow the pattern: browser_specific_extension_scheme://extension_id/actual_resource_name. If you want to access the popup present on uBlock origin Chromium extension, you can use the URL: chrome-extension://cjpalhdlnbpafiamejdnhcphjbkeiagm/popup-fenix.html. Besides the HTML, CSS and JavaScript files, an extension also has one important settings file, named manifest.json. This is a required file that lists the identification of the extension, permissions required of the extension, and the accessibility of the extension. As the ecosystem of browsers have progressed, the version of the manifest.json has also progressed, with new versions often enforcing more secure settings and making changes to the nomenclature. In the next section, we will discuss the contexts an extension’s files can run in and how this can be directed via the manifest file. In the manifest.json file, we can specify the context that a file will run. The three major contexts are the webpage/content script, the popup and the background. You can let the browser know which context you want the file to run in by specifying it in the manifest.json file, aptly labeled content_scripts, background_script, and browser_action in the manifest version 2 (v2). On manifest version 3 (v3), a manifest.json may look like the following: Here, you can see how the v2 permissions have changed slightly to their v3 counterparts: content_scripts, background and action, showing the minor changes between versions. Background script and permissions Let’s start by talking about the background context. The background context is the most powerful of the three contexts with the ability to access most of the browser extension APIs/WebExtensions API. From now on, I will refer to both browser extension APIs and WebExtensions API as the Extension APIs for the sake of brevity. The Extension APIs give an extension a lot of control of the user’s browsing experience, with the ability to arbitrarily control tabs, read from the websites, or modify and read cookies, to name a few. Luckily, these abilities are each locked behind permissions, requested in the manifest.json file under the “permissions” key. When installing an extension, a popup will show up describing in a user friendly way which permissions an extension will be granted. There are some important permissions to look out for during a security review of an extension. Firstly, look for the permissions key in manifest.json, whose values are a combination of hosts and actual permissions in manifest version 2. In this example, the extension can access all URLs, which can be specified through regex or through the keyword all_urls. When an extension has permissions for a domain, it will allow the extension to send requests to the domain with all cookies, ignoring some security considerations. For example, if the website puts some SameSite strict cookies in the browser, the extension can still make requests on your behalf with those cookies despite being part of a separate domain. In v3, domain permissions are moved to host_permissions and additional optional permissions are introduced, which are requested during runtime based on user consent. A v3 manifest may look like: Some important permissions to look for are those that include user sensitive information, such as history, bookmarks, cookies or permissions or ones that give the extension more control over the browser, such as downloads, management, or tab. One permission that has a lot of power is the activeTab permission, allowing the extension to inject JavaScript code into any domain that the user is currently interacting with. In order to inject into the current tab, it must have user interaction. This activeTab is interesting for exploitation and malicious extensions alike due to its immense power. If malicious input can be injected into the executed JavaScript, the attacker may get the ability to get Universal XSS (UXSS). A malicious extension, on the other hand, can create shortcuts that overlap with common user actions, such as copy or paste, and interact with tabs it is not supposed to have access to. The permissions of an extension are a great way to start assessing an extension to see if it even has enough privileges to perform actions that may pose a risk. The background script should be audited to ensure the safety of calls to the browser APIs, and analyzed to see how messages are sent back to the content scripts. Content scripts The frontend of an extension is just as important as the backend when extensions want to interact with the DOM of the pages visited by the user, a responsibility that a background script cannot achieve due to its lack of access to the DOM. This is where content scripts come into play. The content script runs in the context of the website but lives in an isolated world, where “JavaScript variables in an extension’s content scripts are not visible to the host page or other extensions’ content scripts.” For example, if an extension wanted to add the summary of a page to the top to help readability, the code may look something like this. The content script may also listen for user interaction on the current page, allowing an action to be taken based on the user interaction. For example, some extensions translate text present on the current page, based on the user’s highlight or focus on a certain word or phrase. The content script and background script work in harmony in order to create the extension’s intended experience. Popup context Lastly, the popup context is present for the HTML and JavaScript that makes up the menu that “pops up” when you click on the icon of the extension. Often, the popup will let the user directly interact with the functionality of the extension, usually allowing them to change settings and make requests to backend servers that are tied to the extension. For example, in the uBlock origin popup, we can click on different icons to access the options page, disable fonts and JavaScript, or disable the extension on the current website. The popup page, along with the other HTML pages included in the extension, can be a critical source of interest. For example, MetaMask is a crypto wallet extension. If a website can cover the extension, a malicious website can trick the user into signing transactions and thus result in the loss of funds. Like the background script, the JavaScript running in the popup page can use all the Extensions APIs that the extension has permissions for and any JavaScript runs in the domain of the extension. Attack surface Because browser extensions are made from HTML, CSS, and JavaScript, they are vulnerable to many of the classic JavaScript vulnerabilities. I will first introduce the attack surface of v2 extensions because it is a superset of v3, then I will conclude by talking about the mitigations brought about in v3 and how it restricts the attacker. I will also write about browser specific implementations and how they affect security. All attacks must start from an attacker-controlled source, and the extension interacts with two main attacker-controlled sources: A website loaded by the user Other installed extensions The most common attack surface in the content script occurs when data is parsed from the current website and the script injects the data into the document as HTML. Some extensions extract the DOM text and attempt to make changes, often to beautify the text or to use the data in some way, then return the DOM text back to the webpage. If the extension allows the text to be inserted back into the page as HTML, we can get an XSS vulnerability in the website. However, this has the prerequisite that the extracted DOM text is controlled by an attacker. Common cases may include when a user comments on a website, or on websites that allow user uploaded content like many social media sites. Secondly, an extension can interact with another extension by calling the sendMessage API to send a message and onConnectExternal/onMessageExternal APIs to receive a message. If the extension does not check the sender, a malicious extension may be able to access any functionality that the onMessageExternal/onConnectExternal function facilitates. Depending on the configuration of the manifest, new vulnerabilities can be introduced. Let’s see some of those. The external_connectable property allows an extension to be connected to by a given website or extension ID. Here we can see how onMessageExternal/onConnectExternal can be extremely dangerous if there is a misconfiguration, as the functionality that was only meant for other extensions is now available to websites. Another interesting configuration property is the web_accessible_resources: This property opens up an extension up to a greater attack surface by introducing two new possible attack vectors. If an HTML file is web accessible, then a website can load the HTML file in an iframe. If the page takes URL parameters and uses them in any privileged way, a malicious website may be able to make privileged actions. Secondly, if the HTML page allows for sensitive actions and is web accessible a clickjacking vulnerability is possible, where the website will cover the iframe and get the user to input or click on privileged operations. More information on clickjacking can be found on this great blog post showing an attack on Privacy Badger. Thus, our three attack surfaces boil down to: The extension takes attacker-supplied input from the website and uses it in some unsafe way. Another extension or a website sends a message to the extension and the extension uses that input in a dangerous way. The extension takes in URL parameters when it is loaded, and those parameters are used to do a privileged operation. This requires a vulnerable configuration. A quick assessment of all these vulnerabilities shows us why browser extensions are generally pretty secure, because it often requires multiple points of failure in order to introduce an exploitable vulnerability. Often, a misconfiguration is needed alongside a vulnerability in order to make the vulnerability truly exploitable. Next, we will take a look at the possible vulnerabilities that occur in a browser extension, and the mitigations that browser developers have developed to mitigate these issues. Vulnerabilities Cross-site scripting Cross-site scripting vulnerabilities are present across many web applications, and are present in browser extensions as well. XSS can occur in two contexts, in the context of the content script and in the context of the background script. An XSS gives the attacker the same privileges as the running JavaScript, therefore an XSS in the context of the content script allows the attacker to compromise the user on that specific website. In contrast, an XSS in the context of the background script allows the attacker to call any Extension API the extension has permissions for, and thus gives the attacker much more control over the entire browser (for example, UXSS). In order to talk about XSS, we must talk about the Content Security Policy (CSP). On manifest v2 and v3 of extensions, the unsafe-inline directive is not allowed in the extension. Therefore, any HTML pages that are part of the extension such as the popup, the options page or any other, are immune from XSS. They are, however, still vulnerable to HTML injection attacks. However, the unsafe-eval attribute is still allowed on manifest v2 but has since been deprecated in manifest v3. When looking for XSS vulnerabilities in extensions, check if the manifest contains the unsafe-eval directive. Then, look for functions that execute code such as eval(), Function(), setTimeout(), setInterval(), etc. Another function to look out for is the Extension API function executeScript. On manifest v2, the API is tabs.executeScript() and allows taking in a string as code, so it is just like the eval() function. Manifest v3 has removed this API, introducing a new API called scripting.executeScript() which only allows local files to be executed. Generally speaking, outdated Firefox extensions are much more likely to be vulnerable to XSS, as Firefox AddOn Store is still accepting new manifest v2 extensions and thus has access to unsafe-eval directive and the tabs.executeScript() API. Many actively developed extensions even have a Firefox v2 and Chromium v3 extension, due to Google’s push for the new version. I want to shout out this article by Wladimir Palant, which goes into detail about the possible vulnerabilities that occur when using old versions of jQuery and unsafe-eval, and shows some code demonstrating the attacks. SSRF Server-Side request forgery is a common vulnerability found in web applications, and we can also find them in browser extensions. Browser extensions are similar to web applications, but run with the cookies of a client-side browser. If an attacker is able to influence the URL of a network request such as XMLHTTPRequest or fetch, it is possible that an SSRF can result. The effect of the SSRF will depend on the specified method, whether the extension developer makes the request with credentials/cookies, and whether the manifest of the extension has that website allowed in the permissions/host_permissions entry. In its move from v2 to v3, changes to an extension’s ability to make requests have been implemented. Specifically on manifest v2 and on Firefox, an extension with no permissions is able to send requests to arbitrary domains with cookies, but is not able to send SameSite cookies, which is reserved for those with the correct permissions. In Chromium, an extension with no permissions cannot send requests with cookies. In contrast, v3 extensions require the host_permissions in order to send any cookies with the request. This means that SSRF with v2 in Firefox is much more powerful than with v3, depending on the security of the website you are targeting. This, along with Firefox’s lack of enforcement for new extensions to be v3 in the addons store, makes Firefox extensions more vulnerable to SSRF attacks than Chromium. Extension API injection Injection into the Extension APIs is a vulnerability unique to browser extensions. If attacker-controlled data is able to be injected into an API call, an attacker may gain the abilities of the extension. Generally, the APIs fall into two categories, those that change data and those that leak data. Some of the APIs that allow you to change data include downloads.download(), the bookmark create or remove function, and even the cookies set function. Unsurprisingly, these APIs are made with safe defaults present. The download method can only download to the user-defined Downloads folder, and is sanitized from path traversal and the like. Another example would be tabs.update(). If you go into your browser’s bar and type in javascript:alert(document.domain)while accessing a website, you should get an alert showing the website’s domain. A XSS used to be possible using tabs.update() by doing this exact action programmatically, which has since been patched. Most API Injection attacks lead to DOS attacks or information loss, where an attacker can download an infinite number of files until the disk is full, or create/remove bookmarks to annoy the user, but are not as powerful as the traditional web application primitives. Mitigations I have discussed many mitigations that have come from the transition from v2 to v3 in the respective vulnerability categories above, but I would like to highlight one browser-specific mitigation below. UUID randomization Now that we understand the attack surface and some common vulnerabilities, we can see what mitigations browser developers have put in place to prevent vulnerabilities. Firstly, many browsers have randomized the ID of the extension, so that attacking exposed HTML files is no longer a threat, unless a leak of the internal ID, called the UUID, can be found. Here, we can see that all files, such as the manifest, are relative to the Internal UUID. Content scripts can still access the pages by calling browser.runtime.getURL, but this is not possible from within the page itself due to this change. From my testing, both Firefox and Safari randomize this UUID by default, while Chromium does not (at the time of writing this blog). Firefox randomizes based on the container and Safari on application restart. Despite claims both in Chromium and MDN docs that the Chromium browser will randomize the UUID if given the key use_dynamic_url in the manifest being in the docs for many years, this feature was only implemented and enabled by default in August 2024. Modeling with CodeQL Just like many web application vulnerabilities, vulnerabilities in browser extensions can be modeled in CodeQL. CodeQL already has support for many of the vulnerabilities, such as XSS and SSRF, which we can use as the base for our queries. CodeQL’s code injection query gets a RemoteFlowSource (any Javascript APIs that potentially take in data from an external system or user) and looks for flow into Javascript code injection sinks such as eval. These sinks are relevant for browser extensions, but more sinks applicable only to browsers exist. We can create a module of sinks that would apply to browser extensions and extend the Code Injection query’s Sink class to tell CodeQL to consider these sinks. /** * Sink for chrome.tabs.executeScript() which may allow an allow arbitrary * javascript execution. **/ class ExecuteScript extends DataFlow::Node &#123; ExecuteScript() &#123; exists( DataFlow::CallNode c | c = tabsRef().getAMethodCall(\"executeScript\") | (this = c.getArgument(0) and c.getNumArgument() = 1) or (this = c.getArgument(1) and c.getNumArgument() = 2 ) )&#125; &#125; Here, we get a dataflow node that corresponds to browser.tabs and look for the executeScript method. If the method is called with one argument, then it looks for the only argument, but if it has two arguments then the method looks for the second argument, because the first argument is the tabID. When CodeQL is trying to find vulnerabilities in source code, it needs to know how data flows if an external API is called whose implementation is not given. The Chrome APIs sources are not included in a browser extension, therefore we need to model how the foreground script communicates with the background script. class BrowserStep extends DataFlow::SharedFlowStep &#123; override predicate step(DataFlow::Node pred, DataFlow::Node succ) &#123; (exists (DataFlow::ParameterNode p | pred instanceof BrowserAPI::SendMessage and succ = p and p.getParameter() instanceof BrowserAPI::AddListener )) &#125; &#125; class ReturnStep extends DataFlow::SharedFlowStep &amp;#123; override predicate step(DataFlow::Node pred, DataFlow::Node succ) &amp;#123; (exists (DataFlow::ParameterNode p | succ instanceof BrowserAPI::SendMessageReturnValue and pred &#x3D; p.getAnInvocation().getArgument(0) and p.getParameter() instanceof BrowserAPI::AddListenerReturn )) &amp;#125; &amp;#125; In Javascript CodeQL, we can extend the SharedFlowStep class in order to tell CodeQL that data flows between two data flow nodes. In this first class, I tell CodeQL that data travels between parameter one of sendMessage in the foreground script to the third parameter of the AddListener method in the background script, all in one step. Likewise, the second class models the background script sending a message to the foreground script. With these models in place, we are able to use the Code Injection and CSRF queries to help find XSS and SSRF in browser extensions. CodeQL packs with support for browser extensions are available at our Community Pack repository for developers and researchers to use. Real-world attack In this example, I will show how these CodeQL models found a Universal XSS (UXSS) vulnerability in smartup, an extension that has over 100,000 downloads. Smartup is an extension that allows users to do an action in the browser after a gesture has been taken by the user. The extensions takes untrusted input via onMessageExternal, does a variety of parsing on the message, and eventually processes the message. In one case, apps_test, the extensions uses the chrome.tabs.executeScript v2 API and appends the message property apptype to the code, resulting in XSS. Due to smartup’s broad permission policy (access to all urls or activeTab) and its permissive message receiving policy (arbitrary browser extensions can send it messages), an extension downloaded by the user with no permissions can get UXSS on any website. chrome.runtime.onMessageExternal.addListener(function(message,sender,sendResponse)&#123; sub.funOnMessage(message,sender,sendResponse); &#125;) ... case\"apps_test\": let _fun=function()&#123; if(message.appjs)&#123; chrome.tabs.executeScript(&#123;code:\"sue.apps['\"+message.apptype+\"'].initUI();\",runAt:\"document_start\"&#125;); &lt;----- message is passed into executeScript return; &#125; This vulnerability may seem serious, but it required three points of failure by the developer (broad permissions, open messaging policy and code injection vulnerability) in order to be fully exploitable. Efforts to inform developers about the importance of security and the risks they take when changing secure defaults should help reduce similar security issues. Conclusion Now that you know the security model of browser extensions, what can you do as a user to ensure that your extensions are secure? First, check the author of your extension and understand that this user has access to all the permissions listed when you install the extensions. Extensions that have not been updated in a while by the author are more likely to be insecure due to usage of old, insecure APIs. Secondly, don’t trust the prompt that pops up when installing an extension, instead open the manifest file and read the permissions to make sure you really understand what is happening. For example, did you know the popup for an extension with the activeTab permission will not show the presence of the activeTab permission? Understand that, generally speaking, Firefox is less secure due to the lack of requirement for new extensions to have manifest v3, and thus many Firefox extensions are still stuck on v2. If you would like to go further, you can also check the CodeQL queries published at our CodeQL community packs. in order to check for vulnerabilities. The queries will cover all the vulnerabilities mentioned in this article such as XSS, SSRF, API Injection and include additional best practices alerts. Want to learn more about how GitHub can take the stress out of shipping secure code? At GitHub Universe 2024, we’ll explore cutting-edge research and best practices in developer-first security—so you can keep your code secure with tools you already know and love. The post Attacking browser extensions appeared first on The GitHub Blog. Attacking browser extensionshttps://github.blog/security/vulnerability-research/attacking-browser-extensions/","tags":["Blogs","Github"],"categories":["Blogs","Github"]},{"title":"《艾尔登法环》官方美术设定集第3弹12月25日发售","path":"/RSSBOX/rss/5ae7e223.html","content":"《艾尔登法环》官方设定集第3弹《ELDEN RING OFFICIAL ART BOOK Volume III》将于12月25日发售，现已开启预订。 第3弹设定集规格为A4，共320页，定价3960日元。书籍将收录DLC“黄金树幽影”相关的美术作品，除各类宣传图外，也将收录场景、敌人、NPC、防具、武器、图标等内容。 另外，在ebten购买带有限定特典的“ebten DX bag”版本，定价12760日元：同捆特典为高精度印刷的A4复制原画“金针骑士 蕾妲”，并附赠收纳画集的专用盒子。 《艾尔登法环》官方美术设定集第3弹12月25日发售https://www.gcores.com/articles/189976","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"手游《最终幻想7 永恒危机》下载量破千万，官方发放奖励","path":"/RSSBOX/rss/e400ce7c.html","content":"手游《最终幻想7 永恒危机》（ FINAL FANTASY VII EVER CRISIS）官方在10月24日宣布下载量破1000万，将为玩家发放3000蓝水晶的奖励，领取时间截止至11月11日9时59分（北京时间）。 《最终幻想7 永恒危机》于2023年9月上线iOS/Android平台，12月上线Steam（锁国区），采用基本游玩免费、部分道具氪金的模式。该作计划将能让玩家体验《最终幻想7》及衍生作的故事，上线时收录《最终幻想7》《核心危机》《第一战士》的部分章节。 目前《第一战士》故事第2部分的首个篇章已在10月17日上线，在该章节实装了新的可用角色安吉尔。 手游《最终幻想7 永恒危机》下载量破千万，官方发放奖励https://www.gcores.com/articles/189975","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"老白出差历险记，录音笔 VOL.313","path":"/RSSBOX/rss/6a629abd.html","content":"【录音笔】是GPASS推出的一档全新会员专享节目，内容来自机核办公室日程生活和工作中的一些短小记录，每期时长十几分钟左右（大概吧）。内容也许是一些不成体统的碎碎念，也许是一些突然发疯的暴言，还有可能是哪个缺德的把办公室里真实的日常对话（dui ma）偷偷录了下来，总之就连我们自己现在也不知道每期会有哪些人参与，会录些什么。 老白出差历险记，录音笔 VOL.313https://www.gcores.com/radios/189974","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"《元素战争》原创故事 - 01- 裁军周（上）·新世界","path":"/RSSBOX/rss/d5302e16.html","content":"《裁军周（上）：新世界》“在一些‘知情人’口中，从1945年到现在，核武器被用于实战，一共有六次。这纯属胡扯，我告诉你，只有四次。有人认为1991年，在哈萨克斯坦，被莫名引爆的那枚核弹，不能算是用于‘实战’。我同意这个观点。不过，我必须说，从某种意义上来看，那枚核弹对历史的影响， 也许比投在广岛的那一枚还要大。只是时至今日，所有人，还没有意识到这点。”——摘自 《爱德华·温斯顿回忆录 - 沃福兰疗养院卷》手稿 1 .新城市2036年10月24日，查尔斯·温斯顿像往常一样，在早上7：30左右，端着一只茶杯站在窗前，一边眺望远处，一边喝茶。这个习惯，他已经保持了50年。无论是在自己家、在办公室，还是在任何一个城市的任何一家酒店，都是如此。这天早上，温斯顿身处的城市，是巴黎。在“二次前夜”后新建的戴高乐大道，与旧香榭丽舍大街交叉路口的西北角，一家不对外营业的酒店的4楼，视野正对街心的一处套房中，温斯顿用茶匙轻轻搅动着杯里的茶。房间中一切，温斯顿都觉得说得过去，但面前的窗子，他不大喜欢。那不是窗户，而是一面弧形的玻璃墙，虽然玻璃有弧度，但是透过它往外望，看到的东西却几乎没有变形。这玻璃墙是单向的，像警局审讯室用的玻璃一样，别人从外面看，只能看到镜像，而且，这玻璃墙的防弹性极好，据说可以挡住口径在20mm以下的所有子弹。站在这样的玻璃前喝茶，当然不用担心安全，但温斯顿很不喜欢这感觉，他认为，如此“务实”的建筑，与巴黎的气质很不匹配。向南望去，视野中最显眼的建筑，当然是埃菲尔铁塔——确切的说，是尚未完工的“新埃菲尔铁塔”。“二次前夜”之前，温斯顿来过巴黎无数次，却从来没有登上过埃菲尔铁塔。还记得在2024年6月，巴黎奥运会前夕，某次商务晚宴上，温斯顿在闲谈中和一个法国朋友聊到这事，对方一脸严肃地说：“要不是因为害怕你的贴身保镖，我真想用桌上的餐刀挟持着你这傲慢的英国佬，现在立刻就爬上那铁塔，去体验一下从那上面俯瞰整个巴黎的感觉。抓紧时间去吧，我的朋友，不要总想着‘机会多得是，下次再说’，世事无常，巴黎圣母院就是个例子。天晓得那座铁塔还能戳在那里多久，瞧瞧现在这些在野党里的疯子，未来他们一旦得势，真说不准会不会把那铁塔拆掉。”温斯顿当时笑道：以我对这些人的了解，若是他们掌权，他们只会把那铁塔加高、变丑，然后刷成银色。温斯顿没想到，12年后，他的那句玩笑，竟阴差阳错地变成了现实。重新修缮的埃菲尔铁塔，塔尖将采用全新的设计方案，等到2037年完全竣工之后，整个塔身高度将达到332米，比2025年损毁之前，高出了8米。另外，为了降低养护成本，塔身还刷上了一种新型的保护涂层，虽然官方表示那种涂层是完全无色的，绝对不会改变铁塔的样貌，但还是有人认为，在光照强烈的时候，走近铁塔，会看到上面泛着一层浅灰色。有人将那种灰色称为“科技灰”。那是NEU主持重建的巴黎南部城区——也就是被称作“巴黎新城”的那一带，那些高耸入云的行政大楼，以及一些科技公司总部奇形怪状的建筑物的颜色。2031年，《查理周报》的一份采访调查显示；46%的巴黎人认为，在NEU的规划下涌现出的大批“科技灰”的新建筑，对巴黎这样一座城市，是一种污染。当时的温斯顿想不明白，为什么仅有46%的巴黎人和自己的想法一样。2.新气象这天的巴黎，天气好到让人浑身发酥。一周之前，天气预报还说这两天巴黎市区会有中到大雨，但现在，晴空万里。显然，NEU的气象部为了保障裁军周阅兵式顺利进行，下了不少功夫。按照活动时间表，阅兵式将在早上10点正式开始。8点钟不到，巴黎中心市区的街道上就已经涌满了人——这对于不习惯早起的巴黎人来说，实在不易。温斯顿放眼望去，感觉那阵仗不像在巴黎，倒很像东京或纽约。过了9点钟，重建后的香榭丽舍大街两侧，人群渐渐在警察和组委会工作人员的引导下站定了位置，官方这一次划定的观礼街区，足足有4公里长，是以往其它活动观礼区长度的3倍，各大媒体的直播车和装载着巨型LED屏幕的卡车，占满了路边的整条车道。高空中，巨型飞艇拖着巨幅的NEU旗帜缓缓游弋着，高声播放着进行曲式的音乐。彩旗招展的战神广场的上空，如蜂群一样的无人机喷着五颜六色的彩带烟雾、不时变换着队形，组成各种各样的文字和图形。当无人机群组成的“Made For Safety”字样和NEU标志出现在空中时，人群中爆发出了一阵欢呼和掌声。在电视上看到这一幕的温斯顿，不禁扬起了一侧的嘴角。谁说巴黎人对政治冷感？如果谁对巴黎人民的政治热情有所质疑，可以去问问路易十六，相信他和他的皇后会捧着脑袋微笑着对人们做出最有力的诠释。12年前，巴黎奥运会的口号，是“Made For Sharing”——为分享而生。这句话当年不知招来了多少冷嘲热讽，巴黎奥申委甚至还因为用英语写了这口号，被法兰西学院的学究们骂得狗血淋头。如今的欧洲，在NEU治下，“和平”、“分享”、“博爱”这类词汇，悄然淡出了公共视野，新闻媒体的语言风格，已经变成了另一种画风。 想到这儿，温斯顿唤醒了房间里的智能屏，转到“新欧罗巴之星”频道。BBC这些主流媒体的视角及报道方式，他早就腻烦了，这种时候，他很想听一听意识形态激进的流媒体的声音。不出所料，“新欧罗巴之星”的直播间里，男女主持人和现场嘉宾，看上去都相当兴奋。“……2036年的NEU阅兵仪式，重新点燃了巴黎这座伟大的城市，让这里重新找回了她该有的样子。巴黎上一次出现如此欢腾、热烈的场面，还是12年前的2024年的奥运会，这座城市已经沉寂了太久了，今天，我们有理由相信，欢聚在巴黎的人们，无论是来自柏林、华沙、罗马，或是里斯本、阿姆斯特丹、雅典，在这里，都将感受到一种从未有过的归属感与凝聚力…………是的，今天的巴黎，已经成为名副其实的‘欧洲之都’，每一个渴望回归到传统欧洲文化血脉的人，在这里都感受到了无与伦比的自由与安全。如今在我们身边，和我们一起沉浸在欢乐与自豪中的，是真正的朋友与亲人，是彼此信任的兄弟姐妹，我们不再需要为避免冒犯不同文化信仰的客人们，而放弃自由表达、谨言慎行，不再需要为迁就无边界的所谓‘平等’，而无限让渡多数人的权利…… ……‘二次前夜’后的10年，欧陆经历了自第二次世界大战结束以来最剧烈的震荡，也正是这样的经历，使这个时代的欧洲人逐渐意识到——哪些才是我们应该去团结的，哪些是真正值得我们去捍卫的。和平是美好的，但也容易使人忽略了一些本质问题，因此，这一次的NEU阅兵仪式意义非凡。当然，有一些声音认为，NEU在联合国裁军周的首日举行这样的一场阅兵式，充满了挑衅意味，似乎是一种与联合国安理会及北约加速脱钩、另立门户的信号，但我认为，这完全是一种被迫害妄想，了解欧洲政治传统的人一定明白，一个团结、自主、奋进的欧洲，只会给世界格局带来更多的稳定。以NEU为组织架构的欧洲的军事力量，是当今世界上，具有强大的战斗力、同时又最没有侵略性的军事联盟，这是与美洲的NFA、及中俄全面战略协作伙伴关系，都是截然不同的……对话中频频出现的“我们”，令温斯顿不禁想起了那部与《1984》和《美丽新世界》齐名的反乌托邦政治惊悚小说《我们》，他下意识地撇了撇嘴。 画面切回现场，摄像机从路边的人群前掠过，温斯顿注意到人们手里挥舞的旗帜，只有大约1/3是法国的三色旗，另外1/3，是NEU的旗帜，剩下的1/3，拿的是NEU其余的23个成员国的国旗，这其中还有一部分人，手里拿的竟然是中世纪欧洲一些贵族王室的旗帜。掠过人群后，电视中出现无人机航拍的画面，等待检阅的队伍整齐地排在凯旋门北侧，从路边的画面中，温斯顿看到在受阅部队的前面，是一排排表演方阵，最前面是几架酷似古罗马战车的马车，后面还有像法兰西的胸甲骑兵一样的马队，不过，这些形象并不是考据还原真实的历史形象，而是杂糅了一些现代元素的“郎克古典主义”造型，有一种架空历史的独特韵味。这些战士的扮演者中有不少黑人和亚裔，也有许多身材健美的女性，而且，女性的盔甲和战袍设计得更加的显眼，显然，NEU打造的“传统欧洲血脉”，并不是一种与现实历史绑定的概念。演出方阵之后，无人机飞过一段空旷的街道，便拍到真正的受阅部队了。所有军人穿的制服，都是统一的NEU制服，右侧的长方形臂章，上面都是NEU的旗帜，下面才是士兵所属国的国旗，其中法国、德国、荷兰、西班牙的旗帜占比较多而颇为显眼，步兵队列后的坦克和装甲车辆上，没有国家的旗帜，只有NEU的徽章，车身上涂装的全部是NEU联合军特有的迷彩。尽管温斯顿经常和身边人吐槽NEU糟糕的审美，但对于NEU军队的制服和迷彩设计，他是暗暗服气的。温斯顿甚至很期待“歌利亚”在全面列装NEU后，会有哪些惊艳的机体涂装。没错——“歌利亚”的出场，才是温斯顿在这次阅兵式上，最关心的内容。花里胡哨的学校汇报表演中，每一个老父亲最关心的，当然是自己的家的孩子。“歌利亚”，是威格曼-莱茵军火集团成立以来，开发出的最重要的产品，没有之一 。至少，作为威格曼-莱茵最重要的董事会成员，温斯顿是这么认为的。他甚至相信，在未来3年内，“歌利亚”都将是欧洲最“像样的”机甲，是NEU联合军唯一的选择。对于怎样才算“像样的”机甲武器，威格曼-莱茵董事会最初对工程师们提出的要求，很简单：“充分利用AMA-02和AMA-11 元素派生的特殊装甲材料和新型电池，设计制造出一款能够适应复杂战斗环境下特种作战的装甲武器。它的机动性，要优于现有的一切坦克和轮式装甲车辆，要具备攀爬、跳跃的能力；它的防护性和火力，要优于各种遥控战斗机器人和外骨骼装备，能够在中等烈度的战斗中，作为移动火力点和火力支援单位。”“在满足这些要求的基础上，还要让它的外形尽量地令人着迷”——每次的会议中，温斯顿都要补充上这样一句——并不完全是玩笑。事实证明，造出这样的机甲武器，所需耗费的成本，不是一般的军工企业能够想象的。但是，威格曼-莱茵，不是一般的军工企业。从路边机位拍摄的画面来看，站在太脱拉T815重型卡车上的“歌利亚”，和前面车辆方阵队尾的几排AMX34坦克相比，显得极为巨大，特别是那厚重的腿部装甲、庞大的躯干部分，与造型锐利、形似科林斯头盔的头部所构成的视觉张力，使其自带一种压迫感。温斯顿对“歌利亚”的造型不是十分满意，但也给个八分。他说，如果《圣经》中腓力斯丁人的巨人战士真实存在，它站在以色列人面前，应该就是这个气势。董事会中一直有人诟病“歌利亚”那古希腊头盔式似的头部造型，说它既不实用、又与典故不符，但温斯顿还是坚持保留这个设计，并认为这头部恰恰是个神来之笔，是“欧洲产品”该有的样子，是与土气的美国佬和中俄的那套“远东几何学”区分的标志。就在电视画面从现场切回演播室的瞬间，温斯顿注意到在运载 “歌利亚”的卡车旁，有一个他熟悉的身影——卡尔·施密特，他那高大的身形、灰白色的头发，以及那套一丝不苟的黑色西服，总是十分显眼。施密特正在和几名“歌利亚”机甲驾驶员交谈，似乎在叮嘱着什么。驾驶员们穿着连体机师服，手里拿着头盔，都是些很年轻的面孔，施密特单手插着腰说话的样子，像是赛场边的球队教练。这个德国佬倔强得要命，而且经常一意孤行，做过不少冒险的事，可温斯顿一直相信，无论遇到什么事，只要有施密特在，“茶杯都掉不到地上”。3.新队伍“好了，没有什么好叮嘱的了，等一会儿进入驾驶舱，建议各位，再想一遍应急预案。”施密特边说着，边用目光将小队的12名驾驶员全都扫了一遍。12名队员中，有10人是来自NEU联合军的机甲驾驶员。NEU在他们的履历表上填满了各种出色的成绩和极高的评语，但施密特对此并不买账。在他看来，这几个没上过战场的年轻人，离真正的“机师”还差得远，只不过是一些乖乖的好学生。今天这趟“汇报表演”，光靠好学生撑场面，是不保险的。施密特从威格曼-莱茵本部调来了两个真正的“机师”——凯文和德雷克，让他俩来做领队，为的就是让这支接受检阅的机甲方阵，暂时成为一支真正具有战斗力的队伍——至少，不会在遇到麻烦时愣在原地当靶子。“应急预案很重要，一旦出现特殊情况，你们需要立即做出反应，没有时间等待‘指示’。”“放心吧，BOSS，凡是要靠‘指示’才会行动的家伙，根本通不过你订的考核，现在应该还都在蒙斯特的装甲实训基地背笔试题呢。”凯文接着施密特的话茬说道。大伙都笑了，一旁的德雷克用力拍了下凯文的肩说：“你这家伙，说话总这么欠揍吗？真不明白你是怎么活到现在的。”“当然靠脑子够快才活下来的，德雷克，看我这小体格儿，我要是再像你那么迟钝，准活不过青春期，早被学校里的混蛋体育生打成植物人了。”大家笑得更放松了，连总冷着脸的芬兰女驾驶员艾诺也笑了起来。只有施密特还板着张脸。“凯文，老实说，有时候我真希望找个话少一些的人做副队长。”凯文回手拍了拍德雷克说：“这点我完全同意！我也认为话少的人更适合做副队长，所以，请把我和德雷克的位置对调一下吧，让他做我的副手。”德雷克笑道：“想和我争队长的位子，等你的综合评级和我一样再说吧。”“别着急，等‘歌利亚’的改装模块开源、公司将每台机甲的改造权限交给专属机师的时候，你就等着看我的尾灯吧。”“你这乡巴佬，别总惦记着把‘歌利亚’当改装二手车行不行？”“说到二手车，我倒真有个亲身经历，说出来能活跃一下气氛，BOSS，可以借我30秒吗？”施密特挑了挑眉毛：“你认为现在是讲脱口秀的时候吗？”凯文没理会施密特的委婉拒绝，说道：“我第一次来欧洲是2030年，对法国一点兴趣都没有，德雷克，你别瞪我，对于我这种技术宅，德国才是梦想之地，那时我一穷二白，到了德国想租辆车，车行看看我手里那些零碎，租了辆1992年的高尔夫给我，92年！我都怀疑这车是不是东德货拼的！从拧钥匙点火开始就浑身响个不停！”大家被吊起了兴趣，凯文接着说，“那晚，我拉着一个泡吧认识的女导游，开着那92年的高尔夫上了不限速高速。路上一台车都没有，要知道，我一个成天混在城市里的澳洲宅佬，80公里以上我都没开过，突然到了不限速的地方，看着时速表上100多的数字，感觉自己简直是他妈的世界之王！” 大伙都笑了，施密特没打断他。他看得出来，这个节骨眼上10个NEU的新兵蛋子有点紧张，不妨插科打诨，让他们稍稍缓口气。“没过多会儿，一辆保时捷敞着蓬从我身边过去，开车的那位老哥还打着电话，我看看那保时捷，看看我的方向盘，一股子倔劲儿就涌上头了。我转头对旁边的女人说，我要超过他！”几个人大笑起来，凯文手舞足蹈地继续说：“我几乎把油门踩进了油箱！整个车抖得厉害，我俩也跟着车身一起抖，抖了快10分钟才和那辆保时捷齐头，身边的女人抓着扶手大叫‘你疯了！’，我却得意得很，侧头看着那台保时捷，心想：‘怎么样？保时捷！’”德雷克边笑边挤着眉头问道：“然后呢？”“然后？那老兄瞥了我一眼，挂了电话，一脚油门，我就看不见他的尾灯了……”大伙一阵爆笑。“我打赌那家伙肯定在跟电话里说：‘等会儿回给你，有个混球以为他能超过我！’”施密特也笑了起来，他当然知道这不是凯文的亲身经历，而是个几十年前的老段子，BBC著名的汽车节目TOPGEAR里讲过，只是年轻人没有听过。看着大伙笑得前仰后合，凯文满意地耸耸肩，说“我其实想说的是——感谢威格曼-莱茵，我们现在驾驶的‘歌利亚’，就是机甲界的‘保时捷’！所以，各位，打起精神来吧！今天，我们可是整个阅兵式上最闪耀的明星！”施密特说：“正因如此，我们才需要特别小心，这些年NEU的仇家不少，那些极端主义的疯子们，什么都干得出来，要是真有人想让这场阅兵式出丑，对 ‘歌利亚’下手，绝对是首选方案。”德雷克说：“不管那些家伙是什么人，不管他们准备的是一架自杀式无人机，还是一桶油漆，我们都用不着慌。”“别轻敌，那些家伙从来不按常理出牌，你们要瞪大眼睛，时刻注意周围的情况。”说着，施密特看了看手表。“时间差不多了，开工吧，孩子们，祝你们今天一切顺利，要是没那么顺利，就按应急预案行事，如果碰到了大麻烦，就跟紧德雷克和凯文。他俩处理过大麻烦。”“我不仅处理过大麻烦，还制造过大麻烦呢。”凯文边说边戴上了头盔。“我是靠制造麻烦，才被BOSS从巴什挖过来的。”“这段故事回来再讲吧。”德雷克调整好驾驶服，跨出队列，站到所有人前面，施密特拍了拍他的肩，示意接下来的事情，全权由他安排，转身走向停在路边黑色的SUV。“施密特先生，抱歉打扰，有件事情我需要亲自和您确认。”一个女人叫住了施密特。施密特转身，看到踩着高跟鞋朝他快步走来的，正是组委会的副主席，希尔薇。“可以叫您的驾驶员们先停下，等几分钟吗？”“54分钟后阅兵就要开始了，他们要做的事还很多，有什么需要我帮忙的，请直说。”“组委会想最后和您确认一下，阅兵式上，贵公司的12台‘歌利亚’机甲，确定要以步行方式通过观礼区域吗？”施密特努着嘴沉默了片刻，摊着右手说：“这不是早就确认过的事情么？”“不，并没有‘早就确认过’，两周前的最后一次彩排时，这些机甲还是全程固定在重型卡车上的。”“但是一周前，10月17日的会议上，我们提出修改方案，要让机甲以步行方式接受检阅，组委会是同意的，不然我们也不会在机甲足部加装用于保护路面的缓冲胶板”“是的，我知道，但是——” 希尔薇露出了一个苦笑的表情，继续说：“贵公司的这项调整太突然了，这样新颖的形式，没有经过彩排，所以——”“所以，组委会经过这一周的时间，现在认为这方案行不通了？”施密特的语气十分温和，但措辞中却带着不肯妥协的意味。“不，组委会非常重视威格曼-莱茵提出的诉求，只是认为有必要提醒您，机甲步行接受检阅，可能会带来一些问题。不过请放心，无论贵公司最终决定采取哪种方案，我们都会全力配合。”“我明白，那么，请告诉我组委会的顾虑。”“首先，由于机甲的步行节奏，与轮式车辆行驶节奏难以保持一致，即便速度一样，也可能对阅兵式的整体观感造成影响；第二，是安全问题，如果在阅兵过程中发生突发情况，比如——只是一种假设，比如阅兵队伍受到了恐怖袭击，我们的步兵、装甲车辆，演出人员，都将按照应急方案进行避险，但我们目前尚不清楚，步行状态下的机甲要如何应对突发情况，希望贵公司考虑过。”“感谢组委会替我们考虑得这样周到，先回答第一个问题：我们机甲不会紧跟着装甲车辆进入人们的视野的，我们会留出足够的空间和时间作为留白，让大家沉淀情绪、集中注意力，然后，伴随着交响曲，让‘歌利亚’正式登场。我们不希望人们将‘歌利亚’理解为装甲车辆的一种衍生变种，更不希望让它们像那些导弹一样，以一种‘装在卡车上的贵重货物’的样子示人。作为一个次世代革命性的产物，它们是最特殊的一种武器，配得上另起一章，以自己的节奏加入到阅兵式中。”施密特看了眼手表，继续说道：“第二个问题——安全，我们对NEU的安保工作非常有信心。同时，也请组委会相信我们的机甲及驾驶员。‘歌利亚’最初就是为应对城市环境中的特种作战而生的，即便真的有人袭击‘歌利亚’，在可以自由行动的步行状态下，我们有很多应对方法，既可以自保、也不会伤及观众和平民，老实说，我们的驾驶员平时训练的就是这个，相反，如果把它们固定在那些卡车上，反倒更危险，我们已经设计了周密的应急预案，请你放心”希尔薇有些惊讶。“抱歉，我不知道有这个安排，至少我的助理没有告诉我这些。”“该抱歉的是我们，我也是一个小时前才刚刚从董事会获悉这些细节的。”说完，施密特把目光转向了装载‘歌利亚’的那排太脱拉卡车。此时，驾驶员们已经进入座舱，开始最后一遍检查设备，卡车上的固定装置已经打开，左侧放下了坡板，路面上也留出了空间，几分钟后，12台‘歌利亚’将走下卡车，在路面上排好队形。“非常好，既然这样，一切照原计划进行。” 希尔薇将平板电脑递给了过来，施密特迅速浏览了上面显示的文件，拿起笔签了字，微笑着目送希尔薇离开。“为什么不对她讲实情呢？施密特先生。”助理兼保镖，阿勒西奥，在一旁问道。“我刚说的哪一点不是实情？”“您似乎并不是一个小时前才得知那些细节安排的。”“如果我5天前就把这些报备给他们，要走的流程可就多了。事实证明，提前50分钟知会他们就足够了，只需要签一个字。”“既然只剩50分钟了，施密特先生，我最好现在就动身去见温斯顿先生，从这里走路过去，路程不短。”“带好我给他的礼物，告诉他，这边一切就绪，没有任何问题。记住，见到他的时候，不要让他察觉到你是急匆匆赶过去的。”“除此之外，我还需要注意些什么？” 说着，阿勒西奥打开了SUV的后备箱，取出了一只精美的礼品盒拿在手上，借着车窗玻璃的反光调整着领带。“第一次单独去见温斯顿先生，没有您在场，确实有点紧张。”“不必紧张，阿勒西奥，我知道你对董事会里之前的那些傲慢刻薄的英国人印象都不大好，但温斯顿先生和他们不一样。”“是的，我从没见您对任何一个‘皇家军械’的人如此敬重。”“不止如此。”施密特笑道：“因为温斯顿先生，我甚至愿意从今以后尊重‘皇家军械’。”4.新格局“如果‘皇家军械’能找出一个连施密特都不反感的英国人进入董事会，那么威格曼-莱茵军火公司用不了多久，就能称霸欧洲了 。”2030年之前，这句话是威格曼-莱茵的董事会内部流传的一个梗。那个时候，刚刚成立不久的威格曼-莱茵，内部结构还不太稳固。在“二次前夜”的撞击中，欧洲大陆中西部的受灾程度比较严重，德国和法国本地的许多军工厂和军火公司均遭受不同程度的损失，灾后的几年间，随着欧洲政治、经济方面进一步的趋于一统，德国的一些军火公司逐步开始了并购重组，渐渐形成了威格曼-莱茵的雏形。作为新时代欧洲陆战装备的标志性军火集团，威格曼-莱茵主要由三家公司为主体合并而成，此外，还有部分容克资本集团和NEU联合军的注资。构成主体的那三家公司，全都资历不浅:首先，是克劳斯-玛菲-威格曼集团，这也是施密特引以为傲的“本家”，以生产豹2坦克闻名，拥有大量制造军用装甲载具的经验。“二次前夜”中由于多处工厂遭受轰击，公司资产缩水严重，但技术实力毋庸置疑。其次，是莱茵金属公司，是以生产各类车辆零部件、装甲轮式车辆、核生化防护系统和武器弹药为世人所知的，他们的155毫米火炮和120毫米坦克炮，更是天下闻名。同样，他们的实体资产也在“二次前夜”中遭受打击。最后，是黑克勒科赫公司，简称H&amp;K，这家公司的经历比较曲折，温斯顿能够成为董事会的重要成员，也与之有关。H&amp;K原本是德国著名枪械生产商，在1990年代因经营不善，被英国皇家军械公司收购。然而到了2024年，随着英国整体经济情况的下滑，英国皇家军械萌生了出售H&amp;K的念头，“二次前夜”后，这种意象变得更加明显，于是，德国国防军和部分容克资本集团陆续从英国皇家军械手中购回了H&amp;K的大部分股份，条件是董事会席位中有两个席位为皇家军械所有。就这样，H&amp;K加上了威格曼和莱茵金属，三家企业共同组建了威格曼-莱茵军火集团，H&amp;K也不再作为公司名称，而成为了威格曼-莱茵的轻武器品牌存在。在合并成立威格曼-莱茵以后，英国皇家军械集团因持有一部分H&amp;K的股份，自然也成为了威格曼莱茵的股东之一，虽然按照之前的收购协议，其股份占比不超过4%，但在董事会里还有一定的影响力。基于威格曼-莱茵的欧陆血统，基于英国与欧陆长期的若即若离以及被戏称为“美国第52个州”的亲美属性。英国皇家军械安排进董事会的人，一直被视作外戚，英国人的存在更是令NEU在和威格曼-莱茵合作时感到很不自在。不过，这种情况在查尔斯·温斯顿担任董事后开始有所改观。温斯顿在英国政商界就是出了名的“欧陆爱好者”，在进入威格曼-莱茵董事会后，他的一系列举动，为这家新兴的欧陆军火公司从英国方面实打实地争取到的不少好处，这一点，也是令施密特最服气的地方。帮助威格曼-莱茵，就是在帮助NEU——这谁都明白。实际上，英国皇家军械派出温斯顿担任董事，也间接表明了英国政府的某种态度——我们不一定非要跟着美国人走，只要条件合适，所有的“政治传统”都可以打破，NEU和NFA，对于英国来说，都是可选项。因此，施密特坚信，把温斯顿稳稳地留在董事会，夯实的可不只是威格曼-莱茵这一家公司的根基，而是英伦三岛和欧陆之间的关系框架。往大了说，一些NEU想做却做不到的事情，借由威格曼-莱茵的董事会内部的某些合作机制，甚至可以更加高效的实现。从某种角度来看，几百年来，欧洲内部的一些问题，从未像今天这样易于解决。作为容克贵族的后代，施密特认为自己有责任把这种机缘巧合下诞生的微妙格局，好好地保护起来，进一步培植起来，让它生根、发芽、开花结果。这一次，在巴黎，施密特希望自己送出的礼物以传达出一个明确的信号，从而帮自己——同时也帮助温斯顿，把这层关系栓得更牢一些。想到这，施密特对阿勒西奥又叮嘱了一句：“这份礼物，非常重要，路上一定小心。”“请放心，施密特先生。” 阿勒西奥点了点头。“您叮嘱两遍的事情，就是天大的事情。”5.英国佬“我对政治太感兴趣了，因此，我选择成为一个商人。”这是温斯顿在做自我介绍时经常讲的一句话。温斯顿一直认为，自己现在的商人身份，比当年自己的父亲、祖父，更便于促成一些国与国之间的“大生意”。父亲和祖父都是英国工党的实权派，分别在他们所处的时代，为了让“国际大局”保持稳定，做过不少努力。温斯顿曾半开玩笑地对妻子瑟琳达说过，温斯顿家族的使命，就是世世代代为大英帝国的利益，在世界各地拉偏架。1919年，在第一次世界大战后的巴黎和会上，温斯顿的祖父见到了两位重要人物。其一，是法国的国防部长克列孟梭，祖父当年想要极力劝说法国人，不要对战败的德国下手太狠，不要过分压榨德国，其二，是设法和费萨尔亲王见了一面，目的是想探探口风，看看能不能商量出一种让阿拉伯人和犹太人在巴勒斯坦地区相安无事的办法。后来的历史证明，祖父的眼光是具有预见性的，想法也是好的，但却并没有阻止最糟糕的事情发生。不过，这是旁人的见解，温斯顿的父亲并不这样认为。1968年，同样是在巴黎，温斯顿的父亲一边亲历着险些推翻戴高乐和第五共和国的大骚乱，一边参与了那场为尽快结束越战而展开的和平谈判。他与当时还是美国国家安全事务助理的基辛格，以及参议院的议员麦卡锡进行过几次长谈。谈话的具体内容，父亲从未对任何人透露过，只是到了晚年的时候，私下里颇为骄傲地对温斯顿暗示：那几次会面看似没有“立竿见影”的成效，但从长远来看，仍发挥了些作用。尽管美国人后来在越南仍然搞得一塌糊涂，可若是没有他的努力，一切只会更糟，除此之外，他还讳莫如深的暗示，自己为戴高乐平息了那场左翼运动。“每个人都觉得自己赶上的是最糟糕的牌局，但如果你瞥见过大家的底牌，就会明白，一切已经是最好的安排了。每当英国人想要看别人的底牌，到巴黎去，准会有收获。”这是父亲晚年常和疗养院的牌友们说的话——患上阿兹海默症之后也常说。温斯顿一直有种直觉，自己也将会有机会，赶上一些重要的牌局，看到些旁人看不到的底牌。因此，他每次到巴黎，都会变得格外敏感。 1991年夏天，温斯顿第一次为公事来到巴黎，那时的他还只是个初出茅庐的无名小卒，连上牌桌的资格都没有，但是当时的巴黎，正有一场大牌局。那一年2月，时任苏联最高领导人的萨哈切夫，在不到两周的时间里，做了一系列极为反常的举动，先是做了一个180度的转弯，改变了对“海湾危机”的态度，谴责美国及其同盟在伊拉克的军事行动，接着，就是对外公开表态，支持“正当反击”。3月，CIA的情报显示，有苏联军事人员出现在伊拉克，然而，这个消息还没来得及“得到进一步证实”，海军陆战队已经和疑似苏军小股部队的武装力量发生了正面冲突。 从后来解密的档案来看，当时的海湾危机，距离引发第三次世界大战的全面爆发，只有一步之遥——与这次相比，1962年的古巴危机差得还远着呢。在经历过几轮烈度不低的小规模冲突之后，美苏双方再次在某种默契之下，同时踩下了急刹车。否则，地球今天可能仍处在核冬天的阴霾之下。很快，美苏双方代表在巴黎签署了一份特殊协定，这才标志着危机解除。当时的温斯顿在巴黎除了邂逅了自己未来的妻子瑟琳达——什么大人物也没见到。几个月后，剧变开始了。萨哈切夫在处理海湾危机时表现的过激动作，不仅使苏联在国际上陷入了困局，也引发了政治局内部的大地震，接着，就是国内的一系列大震荡——巴库惨案、莫斯科事变，几个月后，一枚小型战术核弹在哈萨克斯坦的卡拉干达郊外爆炸，至今原因不明，几个小时后，事态急速失控，苏联全面进入紧急状态，整个欧洲也吓得半死，部队枕戈待旦，3周后，12月24日，平安夜，苏联宣布解体，萨哈切夫下落成谜，成为了20世纪末最具传奇色彩的一宗悬案。不可一世的庞大帝国以如此戏剧性的方式轰然倒塌，是谁也没想到的，温斯顿记得父亲后来常说，如果自己和同僚们暗中再使把劲儿，让那个名叫“戈尔巴乔夫”的家伙成为了苏联领导人，说不定苏联会走上另一条路，即便改革不能像中国那样成功，后来垮台的方式也会更柔缓、更和平一些。苏联若是以那样的方式退出历史舞台，对大英帝国、对整个世界，都会更有好处。和一般的阿兹海默症患者不同，温斯顿觉得父亲在患病之后，并没有变得迟钝木讷、浑浑噩噩，反而更加精神矍铄、思绪飞扬，经常说一些莫名其妙的大话、臆想着各种历史的假设。温斯顿有时候就在一旁静静听着，感觉父亲仿佛是一个超时空穿越者，窥到过另一个“平行宇宙”中的真实历史。那个圣诞节，温斯顿和瑟琳达刚刚结婚，正在阳光暖和煦的新西兰度蜜月。回到伦敦后，他身边的朋友对他说：“温斯顿，你度蜜月的这段时间，这个时代最大的政治危机结束了，我们这一代人看来注定要在无聊的和平中虚度余生了。”现在看来，朋友的这番话说得太早了。苏联消失后的这几十年，世界不但没有歌舞升平，反而更加纷乱、动荡。而2025年“二次前夜”的发生，使各种原有的动荡纷乱，产生了一系列超越一般规律的剧变。“二次前夜”之前，谁能想到欧盟成员国之间会发生兵戎相见？美军会兴师动众重返中东？一家科技公司居然能无视国际公约在月球上建立军事设施？欧洲各国居然在一种与苏联相似的政治模式下形成了一个新的共同体，和美国牵头的整个北美洲对抗？巴黎的工人居然愿意公休日接受超时工作、加班加点修缮凯旋门和埃菲尔铁塔？这每一件事，在“二次前夜”之前听起来，都像是科幻设定，可现在，都写入了历史教科书。不过这些，还都是世人皆知的事，温斯顿认为某些不为人知的秘史，更加耐人寻味。看着电视上的‘歌利亚’，温斯顿不禁想到了1996年，他第一次通过特殊渠道，从解密材料中看到巴什公司制造的实验机甲的情形。内部材料披露，早在1988年，后来在任职于巴什科技、被誉为“机甲之父”的利亚姆，就已经带领着一个名不见经传的小公司，完成了首台机甲——“GM50试验机”的设计。那个东西在今天看来，基本上就是巴什的“狼蛛”的始祖版本——比今天的“狼蛛”造价更高、性能更差，性价比低到令人瞠目结舌。可即便这样，在1991年的海湾危机中，利亚姆还是迫不及待的把两台这样的东西交给了美国军方，想要看看它的实战数据，结果——没能看到任何有价值的数据，他的两台宝贝疙瘩差一点儿被苏军特种部队炸残在沙漠中，险些被俘获。 据说，海军陆战队当时单独制定了一个名为“磁石”的行动任务，就是为了把这两坨废铁抢回来——利亚姆当时的要求是：一个零件也不能落下。整个事情当年一定搞得美军负责人和利亚姆都很没面子，于是，关于这件事的一切成了军方的机密。温斯顿还记得在那段画面模糊的录像中，一台高达5-6米的四足仿生机械，像个被卡车压过的螃蟹一样趴在车间里，被火烧黑的机体外表，布满了密密麻麻的弹孔和弹坑。自此，仿生机甲的研发工作石沉大海，在30多年的时间里，再无人问津了。然而，“二次前夜”后，天外陨石带来的新元素，所催生的新型高强度材料和超长续航电池，使整个科技领域进入了新一轮的技术革命，在这个前提下，机甲才重新获得了重视和发展。而国际局势也因此发生了剧变。巴什公司，也自然凭借着利亚姆带来的“老本儿”，成了第一个造出机甲的企业。“GM50试验机”的失败经验，让巴什在机甲赛道上的起跑线，往前挪了一大步。近10年，世界上一系列突然而剧烈的变化，总令温斯顿感到有些熟悉。这天早上，温斯顿看着远处的“新埃菲尔铁塔”，忽然意识到，第一次世界大战之前的世界，和眼下的一切，似乎有着某种特殊的同频。1910年代的欧洲，同样是突然激化的国际冲突、同样是带有狂想色彩的技术爆炸——大到量子物理学的跨越式发展、小到特斯拉的十万伏特超高压输电塔——这股疯狂的气质，和今天是何其相似。难道在那个时代，历史的走向、科技树的发展脉络，也是被某种突如其来的“外因”扭偏的么？如果是，那鲜为人知的“外因”，又会是什么呢？该不会是造成1908年通古斯大爆炸、却没有留下碎片的那颗神秘陨石吧？想到这，独自在房间里的温斯顿不禁笑了笑，他觉得这是一个相当有趣的话题，可以作为一会儿与访客见面的一个谈资。而且，他打算把这件事当做一件“秘史”，一本正经的讲给那个严肃的法国人，活跃一下氛围。顺便，看看对方的反应。在这种非正式的会谈中，第一个巧妙地开出玩笑的人，往往会占有一些额外的主动权。 6.访客“温斯顿先生，这里有一位未预约的先生要见您，他叫阿勒西奥，要让他上去吗？”安保人员的声音从对讲机里传来。温斯顿看了眼表，时间还比较充裕。“请他上来。”见到温斯顿，阿勒西奥毕恭毕敬地问了好，转达了施密特的问候，将那个精致的礼盒交到了温斯顿手里。温斯顿讳莫如深的笑了笑：“凭我对施密特先生的了解，我大概能猜到这里面是什么。请转告施密特先生，非常感谢他的这份礼物，有了它，一会儿和那位法国客人的会谈，会更加顺利。“这是再好不过了，温斯顿先生，如果没有其它事情，我先告辞了。”阿勒西奥转身准备离开，却被温斯顿叫住了。“等等，年轻人，喝杯饮料吧，我看得出你这一路来得很赶，休息一下，访客要过一会儿才到，我从施密特那里向你借10分钟时间。给我讲讲你所看到的巴黎。我被过度保护得太久了，许多有趣的事情我看不到，但很好奇，所以——柠檬汁，可以吗？我知道你这会儿不能喝酒。“谢谢，温斯顿先生。”接过温斯顿递来的杯子，阿勒西奥有些紧张。“说说吧，穿过这几条街区，看到了什么有趣的事？我是说，在那些小街区，没有媒体去拍摄的地方。”“其实，就在两个街区外，警察在和一些反对NEU的异见人士对峙。虽然还没有发生冲突的迹象，但气氛挺紧张，警察都是全副武装，空中一直盘旋着无人机，路也被拦住了——抱歉，我就是因为绕了路，才耽搁了时间。”“那些反对NEU和阅兵式的人，他们手里的横幅和标语，是什么？”“我的法语不太好，有些字面意思我理解的也许不准，总得来看，和以往一样——指责NEU在让欧洲苏联化，认为他们借着重建欧洲的由头，在搞科技集权、扼杀自由市场、自由经济，还将美洲、俄罗斯和中国都制造成了假想敌，搞军备竞赛——大概之类的吧。”“我猜，也一定有人把NEU的标志和纳粹的符号叠在了一起、或者——”“当然，那是肯定的——‘当你想丑化一个政体，无论它是什么样的，把它比作纳粹，准没错’——温斯顿先生，我记得您好像是这样说过。”“哈哈哈哈哈，没错，我是说过。”温斯顿笑了起来。“一半人把NEU比作新纳粹，另一半，把他们比作苏联。”“这很讽刺，是不是？”阿勒西奥略带腼腆的笑了笑，喝了口柠檬汁。“以你观察，反对NEU的那些人，主要是些什么样的人？年轻人多一些，还是——”“什么样的人都有，和支持NEU的那人群，看起来没有太明显的差异。”“你是说，和这些人对峙的人，除了警察，还有NEU的支持者？”“是的，不过支持NEU的人今天可没有功夫和他们费口舌，他们都赶着往香榭丽舍大道去看阅兵式呢，那是他们的狂欢。”阿勒西奥调整了一下坐姿，继续说：“NEU的支持者，就是那些……被称为‘左翼知识分子’的人，大学教授、学生、普通的工人，市民，一些规模化有需求的企业主，另外，还有一些是来自更底层的人——就是之前失业者。这些人很认可‘团结才是出路’这种口号，甚至认为为了团结，在欧洲消除国别也是可以考虑的。这两年，旗帜鲜明地支持NEU的人越来越多，基本上已经成为了主流，大概是因为很有安全感吧，所以这种时候，他们显得比较平和。刚才在路上，他们从反对NEU的人跟前走过时，并没有和那边对骂，只是有人喊了几句嘲讽的话，我没大听清，总之，双方都没有过激的行为，这和以前很不一样——几年前的巴黎，您知道的。”“哈哈，当然，我记得。”温斯顿苦笑着皱了皱眉。“2029年6月，我的眉骨和左侧的两根肋骨被打断过，就在巴黎——您知道，一般情况下，我是不会使用武器的，这是我的原则，也是公司的规定。”“我所认识的意大利人，各个是打架的好手，对了，阿勒西奥，听口音，你是意大利威尼托人么？”“是的，温斯顿先生，我家就在帕多瓦附近。”“那可真是个好地方。”“谢谢，是的，特别是这几年。”“听起来，你好像对现在更满意一些，对吗？”“我是个粗人，政治方面的事情，我不大了解，我只是说我看到的。”“现在的巴黎，中东面孔似乎少了很多，是这样吗？”“不仅在巴黎，在多特蒙德也是，在威尼托也是。阿拉伯裔，土耳其人，都比之前要少了。”“前些年的‘遣返计划’，造成了这么大的影响吗？”“这是主要原因，不过就我观察，自愿离开欧洲的人也不在少数。”“你是说，那些响应哈萨伊的‘重建巴比伦’的人？”“没错，‘二次前夜’之后，欧洲在重建，中东也在重建，我认识的一些阿拉伯人就说：‘既然现在哪里都是一团糟，哪里都需要重建了，为什么我们不回到自己的故乡，和我们的兄弟姐妹一起重建属于我们自己的地方呢？何必还要留在这儿呢？欧洲人不会真正接纳我们——这次重建之后，更不会了。’于是，前几年就出现了令人难以置信的‘逆难民潮’，很难想象，10多年前为躲避战祸背井离乡的难民，现在却主动返回了中东。那些人中大大部分，都非常崇拜哈萨伊，把他视为这个时代的先知。哈萨伊要他们回到巴勒斯坦、叙利亚和伊拉克，许多人便真的带着妻儿、带着在欧洲积攒下来的一切回去了，那番景象，只要见过一次，就不会忘记。”“你见到过那场面？”“是的，我见到过，2034年，在拉塔基亚港。”“这真是不可思议，前30年人类学家、社会学者们的很多预测，似乎已经无法解释这10年我们看到的许多现象，不是吗？”“我从来没读过那类书，实际上，在跟随施密特先生之前，我几乎没看过书——我看过所有书，都是施密特先生推荐的。”“我猜，不止是‘推荐’吧。”“施密特先生有时会用他的方式‘考’我有没有看那些书。”“啊哈，这像是施密特的做派。”温斯顿大笑道。“说到书，温斯顿先生，我想忽然起了一件事，在过来的路上，我看到了一个人，就是那名记者，他那样子很熟悉，但记不清他的名字了。”“哦，你是说，那个叫‘斯诺’的记者？”“对，斯诺，他好像是叫过这个名字。这家伙，站在人群中很不显眼，但我记得他。”“如果没记错，前一段时间，他披露了巴什公司的一些致命的黑料，爆料巴什公司在月球使用军用武器袭击中国科考队的报道，似乎就是他写的。”“对，他是个很厉害的家伙，前几年，他也做过一些关于我们的不利报道，董事会的一些人对他也是——”“我知道，董事会有人提出过想要‘解决’他，天呐，这些家伙把威格曼-莱茵当成什么了，我们可不是西西里黑手党，而是正经的生意人，老实说，得罪生意人，不会有什么危险，但巴什公司可就不一样了，他们不算是真正的生意人，更不是绅士。真希望这位斯诺先生小心一些。据我所知，在巴黎，替巴什公司做脏活的人大有人在。他身边真应该有一个像你一样出色的保镖，阿勒西奥。”阿勒西奥礼貌地看了下手表，放下了杯子，说道“：10分钟到了，很高兴能陪您聊天，但是您的访客恐怕很快就要来了，我应该——”“法国人通常会迟到，不用着急。”温斯顿笑了笑。“访客或许会迟到，但是阅兵式会准时开始。”“好吧，谢谢你，阿勒西奥，最后帮我个忙——帮我把施密特先生的礼物，放在一个显眼点儿的位置，我希望一会儿访客过来，能注意看到它。”“好的。”外面人声鼎沸，人群沸腾了起来。10:00 阅兵正式开始，温斯顿看了看表， 9:40，在电视直播的画面中， 12台 “歌利亚”已经在街心就位。温斯顿盘算了一下，如果一会儿谈话的节奏把握得当，那12台“歌利亚”在经过温斯顿面前的街区时，他和那位最重要的访客——“联盟阵线”新任领袖，塞巴斯蒂安·勒庞，应该正好谈到关键问题。在最关键的时候，那12台“歌利亚”，就是他最有力的谈判筹码。（未完待续） 《元素战争》原创故事 - 01- 裁军周（上）·新世界https://www.gcores.com/articles/189973","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"PlayStation合作伙伴大奖Japan Asia将于12月3日举行","path":"/RSSBOX/rss/a30b6c9b.html","content":"索尼互动娱乐将于12月3日举办“2024年PlayStation合作伙伴奖Japan Asia”（以下简称“PlayStation 合作伙伴奖”），以表彰日本和亚洲地区的PlayStation游戏。 “PlayStation合作伙伴奖”始于1994年，即PlayStation诞生后的第二年，旨在向创作者的作品们致敬，并庆祝各种热门游戏的推出，这将是该活动第30次举办。奖项如下：GRAND AWARD：颁发给2023年10月~2024年9月期间，日本和亚洲厂商们所开发的销量前三的作品PARTNER AWARD：颁给在2023年10月~2024年9月期间，日本和其他亚洲地区厂商所开发的游戏中，有过显著活动成果的，并在全球销量榜中有名的作品。 EXCELLENCE AWARD：连续3年获得GRAND AWARD的作品 SPECIAL AWARD：有其他显著的活动成功的作品。 USERS’CHOICE AWARD：玩家选择奖，活动现已开启，点此跳转&gt;&gt;&gt;&gt;&gt;&gt; PLAYSTATION GENERATIONS AWARDS： 为庆祝索尼PS 30周年的到来，会从PS每一世代（ PlayStation、PlayStation®2、PSP®“PlayStation Portable”、PlayStation®3、PlayStation®Vita 和 PlayStation®4 ）的合作伙伴奖中，选出最优秀的作品进行表彰。活动已开启，点此跳转&gt;&gt;&gt;&gt;&gt;&gt; PlayStation合作伙伴大奖Japan Asia将于12月3日举行https://www.gcores.com/articles/189970","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"特别好评武侠游戏：《天命奇御二》Xbox版现已发售","path":"/RSSBOX/rss/4b1b78fe.html","content":"由甲山林娱乐开发的原创武侠单机游戏《天命奇御二》现已登录 Xbox 。本作接续前作角色养成、思考探索等要素外，采无接缝地图，融入独特推理辩论及自由搭配武学玩法，创造出高自由度、写实的江湖世界。让玩家体验到不切换场景的实时战斗，感受想打就打的快意武侠。 &lt;内嵌内容，请前往机核查看&gt;游戏拥有扣人心弦的主线剧情和众多引人入胜的支线任务，很多任务需要你悉心搜集信息，通过缜密的思考推理出关键线索，并于最终时刻与各路人马互动推敲辩论，破解出一桩桩情节扑朔迷离的奇案，甚至还有机会与包青天、展昭一同办案！在江湖历险中，善用「出示」、「运劲」、「点燃」、「破坏」四种关键互动方式，可以挖掘出大量的传闻任务，不同的方式都可能衍生出的不同结果，这些结果将带给你脑洞大开的无限惊奇，更一步激发你隐匿的冒险精神与童趣之心。 招式任意搭 武学创巅峰高灵活高自由度的武学与养成模式，20余种心法，结合八卦加点养成独特天赋线和上百种招式技能自由搭配与切换，可以延伸出数千种武林绝学玩法。即时战斗机制融合自由配招与武学套路切换，可以让你拥有更多战术选择，酷炫的招式和良好的打击手感将为你带来酣畅淋漓，爽快十足的战斗体验！ 恣意江湖游 快意解恩仇游戏采用了无缝地图设计，玩家可以在开放的武侠世界自在遨游、恣意解恩仇。行经古色古香的亭臺楼阁，人声鼎沸的市井小巷，景色壮丽的山河湖海，结交江湖豪杰、红颜知己，勾勒出一个宏大自由的武侠世界。同时也可以在野外钓鱼，同棋手对弈围棋，或是在炉火旁烹饪菜肴，甚至还可以向商队购买建筑，过上安逸舒适的田园收租生活。 这款由甲山林制作的游戏目前在Steam的的评价为“特别好评”，拥有近90%的好评率，如今游戏中鲜活的武侠世界终于登陆Xbox游戏主机。 特别好评武侠游戏：《天命奇御二》Xbox版现已发售https://www.gcores.com/articles/189968","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"我跟外山圭一郎掏心窝子地聊了《野狗子》，没诞生的《死魂曲 3》以及给开发者的建议","path":"/RSSBOX/rss/39a33dc5.html","content":"作者：雪豆前不久，我应邀请同其他几家媒体前往东京，拜访了著名制作人外山圭一郎的工作室Bokeh Game Studio，并试玩了即将发售的《野狗子》的部分内容。之后，我们还对开发团队的几位主心骨：CEO 外山圭一郎 、COO&#x2F;制作人 佐藤一信 、CTO&#x2F;游戏导演 大仓纯也 、以及音乐音效总监 山冈晃 四位，进行了长达两个小时的采访。我自己本身就是《死魂曲》的忠实粉丝，除了同外山先生聊了《野狗子》在开发过程中的的方方面面以外，我也问了许多关于他们曾经创作过的作品的心路历程，以及对于工作室未来的展望。 同时，外山先生也为现在的年轻的开发者给予了一些建议，从我个人的观点来看，我觉得这些建议是非常宝贵的。整篇采访大约一万字左右，赶时间的各位可以阅读总结的精简版本。关于本作的试玩体验请参考《 在试玩了《野狗子》之后，我确信它仍然充满了外山圭一郎最擅长的“邪典”味儿》精简版 《野狗子》最初立项就是一个有点恐怖元素的动作游戏 本作的灵感其实源自于日本漫画家诸星大二郎对《聊斋志异》的漫画创作 外山先生很喜欢王家卫的《重庆森林》，游戏中也有许多来自港片的表现手法 《野狗子》中的许多系统灵感正是来自于《死魂曲 3》的早期策划 但玩法的核心灵感真的不是来自《寄生前夜 第三次生日》，是来自《死魂曲》的“视界截取” 希望大家在游戏过程中能多用“附身”来切换角色战斗，这也是游戏的主旨 本作的所有音乐音效全都是山冈晃先生一人完成的 只要SIE同意，外山先生很乐意开发《死魂曲 3》 制作人与导演之间的良好沟通是一款游戏能诞生出来的重要前提 希望现在的年轻开发者去敢于尝试自己喜欢的题材，一定会有人支持你的 但在规模和资金方面，也要量力而行，不要勉强自己，妥协得到的游戏不会是一款好游戏 关于前期开发： Q. 在中文语境下，“野狗”这个词会给人一种危险的感觉。虽然外山先生您曾经说过本作名字的来源是来自《聊斋志异》，但还是想问一下，《野狗子》这个名字是如何确定的呢？外山：主要还是想找到一个“新鲜感”吧。首先大家已经知道了本作名字是来自于中国的志怪小说《聊斋志异》，所以在定下这个名字之前，我们首先还是要在游戏中找到点灵感 —— 因为本作中有一个会吃人脑的怪物，从观感上来说就很新颖，所以我们也想为名字找到一种“不一样的感觉”，以此为启发确定了游戏的名字。 Q. 那请允许我再深入地问一下，既然《野狗子》这个名字来自《聊斋志异》，那么请问您是怎么会想到《聊斋》的呢？外山：我本人其实非常喜欢看漫画，从小开始就开始读了不少各种各类的漫画。90年代的时候我很喜欢漫画家诸星大二郎的作品，他经常用中国古典文学来创作的漫画。所以也是通过这些漫画，我了解到了《聊斋志异》这部作品。不过虽然我小时候就在看这些漫画，但真正对中国古典文学感兴趣还是变成大人之后的事儿了。诸星先生的作品是用日本人的视角去解构那些怪异奇谈，这一点也非常启发我。当然，诸星先生的作品在日本国内也是非常有名的，跟伊藤润二先生差不多有名吧。哦对了，其实《死魂曲》的很多早期的创意也都是来自诸星先生的作品。 Q. 那《野狗子》将地点选择在了架空的香港，是有什么理由吗？毕竟既然是都市怪谈主题的话，就算发生在新宿也没什么问题吧？外山：九龙作为香港的标志，在日本人心中是非常有地位的，既然九龙早已被大家所熟知，自然刻画起来也就相对轻松。既然游戏设定在了一个架空的世界观中，自然还是要在其中放一些经典的参照物。所以像九龙城寨这种早已在我们心中刻上烙印的场景自然也就是首选了……我还记得在有一张飞机飞过九龙城寨上方的照片非常有名。2000年左右的时候我自己亲自去了趟香港，其实在去之前自己的大脑中已经幻想过无数次香港的模样了。既然这次要决定将自己大脑中的东西变成现实，哪怕有点艰苦，也必须要尽可能地详细展现出那种我大脑中早就构思无数次的氛围。 Q. 您说2000年左右去了香港，但我记得九龙城寨早在93年就被拆除了，那游戏的主场景是如何创造出来的呢？外山：我自己确实没能亲眼看到九龙城寨，不得不说也是一种人生遗憾了。所以在创作的过程中更多的还是通过当时的照片和资料来进行还原，毕竟九龙城寨本身在海外有相当高的知名度，所以也留下来了大量的照片和视频资料，所以其实创作起来也没有遇到什么难处。不过还是得强调一下，《野狗子》是架空的世界观，所以在基于现实的基础上，我还是想融进去一些幻想的元素在里面。听说了我的想法之后。制作团队的大家其实都非常兴奋。因为九龙城寨的氛围其实是非常别致的……大概是一种霓虹灯下的破旧感吧。 虽然想要完全还原历史的九龙城寨是不可能的，这次也是架空世界的新东西，但我自己对九龙城寨是抱有敬意的，全体制作团队也下了非常多的功夫：比如游戏中的那些招牌看板，上面的中文是我们认真考究过的。希望各位中国玩家如果在游戏的过程中发现什么不对的地方，请立刻告诉我们，我们马上改！（全场笑） Q. 刚才在参观工作室的时候，我看到书架上有一套王家卫导演的《重庆森林》。那请问《野狗子》有受到王导的影响吗？游戏中会有致敬香港电影的地方吗？外山：我二十多岁的时候第一次看到王家卫导演的《重庆森林》。可以说是被彻底地震撼到了，而且现在每次重新看的都能回味出当时给我的冲击 —— 怎么说呢，之前我看的香港电影都是功夫片，比如成龙拍摄的一系列动作电影。但王导拍出来的香港竟然如此时髦，如此富有腔调，这点是我完全始料未及的。 所以在开发《野狗子》的时候，我还是挺想把年轻时候受到的这股冲击给表现出来，也算是某种程度上的致敬吧。当然，要说非常明显的致敬是没有的，最明显的大概是那种“午夜两人骑着摩托车穿过隧道”的那种程度吧。 Q. 《野狗子》在刚公开第一个预告片的时候，许多玩家都觉得它会是一个恐怖游戏，但从随后披露的试玩以及我们刚才的体验来看，本作是一个“动作冒险类”游戏。请问游戏为什么最终会选择这样一个类型呢？外山：最初《野狗子》在立项的时候就是动作冒险类游戏。也就是说，开发团队的大家想要去做的东西，跟现在所展现出来的其实是差不多的。《野狗子》刚公布的时候得到了很多人的帮助，比如说最初能够在TGA上公布第一个预告，就是Geoff的帮忙。我们当初公布的预告其实也是想传递出本作是一个“动作冒险类”游戏，在游戏的主题方面，是一直都没有变的。 Q. 总的来看，外山先生之前做的游戏有点偏向“生存恐怖”类型，战斗在游戏中的占比并不是很重。从刚才的试玩来看，本作的战斗系统其实还是非常重要的，而游戏也并没有那么恐怖，所以是出于什么原因，让《野狗子》选择这样的游戏系统呢？外山：一开始的时候，我们其实有考虑过要不要先凑一小波人，做一个小型的、成本可控的作品。但后来还是觉得大家好不容易独立出来了，机会难得，作为工作室的第一部作品，比起做一个小游戏，团队中的大家其实还是想做一个自己想做的东西。而又因为大家是以前一起共事过的同事，所以也想做一个有点过去的影子，但也包含了挑战性的作品。既然说到了挑战，那么选择这个以前没做过的游戏类型不但是一种挑战，也是一种验证“能不能做出工作室自己味道”的手段。另外我觉得把“动作”和“恐怖”两种要素融合到一起，也比较符合当下玩家的口味。 Q． 刚才提到，本作是一款动作元素偏多的游戏，那么开发团队成员是不是开发过动作游戏的经验？毕竟一个新团队上来就是开发一款动作游戏，想想确实还是挺有难度的。外山：确实是挺有难度的。刚才我也提到，团队最初组成时，很多都是我过去的老同事们 —— 他们之中有做过《最后的守护者》的、有做过《死魂曲》的，有做过《重力异想世界》的，其实并不是所有人都对动作游戏开发有丰富经验。在当下这个年代开发游戏，有些时候经验往往比灵感还重要。这对《野狗子》的开发而言确实是件蛮有挑战的事。那么，在开发过程中要如何确保成品会达到一种“令人满意”的预期呢？我觉得第一点是所有的团队成员在游戏的开发过程中都保持了一种积极的态度，大家都会认为除了有挑战以外，也可以获得宝贵的开发经验；第二个就是需要多听玩家的建议，不断让各位玩家参与到游戏的测试中，然后将用户的测试反馈导入到游戏的后续制作与调整之中，最后让游戏慢慢达到一个满意的品质。关于游戏内容： Q. 在刚才试玩的时候我发现，本作采用了多角色多角度进行多线叙事的手法，同时游戏中也出现了类似“从敌人视点的观看角色位置”的设计，这让我想起了《死魂曲》，请问这两个系统是受到过《死魂曲》的影响吗？外山：猜的没错，确实是这样。其实《野狗子》在企划最一开始的时候，就是以“假如把《死魂曲》以现代的玩法重现，会是什么样子的”为原点来进行设计的。您能感受到这一点我还是非常开心的。 Q. 本作的弹反系统设计的非常独特，从玩法上来看，玩家需要通过推动方向键，来格挡敌人来自四面八方的攻击，那么这一系统是如何设计的？佐藤：是这样，大家对本作印象最深的内容果然还是弹反吧。其实一开始设计的时候，就是设计成了现在比较常见的那种的普通式弹反，并不是现在这个样子。但是我们在开发过程中总是觉得要把游戏做出点“自己的味道”。要说到最能体现特色的地方，或许就是弹反这个系统了吧。所以我们就想着，要不要稍稍换个形式呢……另外说一下，这次负责本作开发的大仓先生是一位很厉害的高玩，或许他已经对这种普通的弹反已经受够了吧？大仓：说受够了这事儿也太失礼了。（全场笑）佐藤：所以总的来说是这样，一开始并没有想把这个系统想的这么清楚，但看到现在很多市面上高难度的动作游戏的弹反系统，我们就想做点不一样的东西，最后经过了不断尝试，就最终形成了这么一个很特殊的系统。 Q. 在刚才的试玩中，我注意到关卡中可以选择两名奇才进行游戏，那么是否会存在一些只有选择特定的奇才才会触发的剧情或是会影响到剧情发展的系统吗？大仓：在正式版中，一个关卡中切换不同的奇才，剧情当中肯定会有很多的对话会发生变化的，但选择奇才其实并不会影响游戏中大的剧情走向。不过奇才与奇才之间可能会有一些揭示各自关系对话，这些对话可以丰富游戏的世界观和故事性，同时也会有可能触发一些特定的事件。另外就是有一些关卡或者隐藏要素，如果不使用特殊奇才的话是无法解锁的。 Q. 刚才试玩中，奇才的性能会比较强，而普通人的实力就可能相对弱一些。这样的话似乎玩家会更倾向于操作这些强的角色，那么这一部分的平衡是如何处理的呢？大仓：本作采用的是“集合众人之力，大家一起打倒怪物野狗子”这样一个主题。所以实际上在大家进行游戏的时候，我们也希望玩家可以操作各种各样，能力不一的角色进行战斗，自然在游戏中，奇才与普通人其实是同等重要的，这也是《野狗子》的一个基本的核心设计理念。刚才大家试玩的时候也一定感受到了，虽然人类很弱小，但人数会很多；奇才很厉害，但往往势单力薄。那么在这个情况下，玩家如何能更好地去活用普通人这一角色，让大家一起将厉害的怪物打倒呢？实际上这是一个非常具有策略性的调整。所以我们也希望大家在玩游戏的时候能够多切换其他角色，感受更多的技能搭配，相信大家在掌握系统之后会研究出更有意思的新玩法。 当然，除了本身的策略性以外，我们也在奇才的设计上稍微做了一些调整。毕竟伴随着游戏剧情的不断深入，队伍中也会加入更多的奇才。如果奇才全是那种一味的很强的类型，那么游戏玩起来也会变得索然无味。所以加入普通人之后，游戏也会具备更多的可玩性，玩起来会更有趣一些，这是我的一个个人观点。 Q. 那么这些奇才是只要推动主线就可以全部获得的吗？有大约多少奇才可供玩家使用呢？大仓：这些奇才大概一半在主线中获得，一半隐藏在支线中，需要玩家自己去寻找解锁。 Q. 其实我一直想问，本作“附身”这个系统是如何构思的？这一个系统玩起来让我想起了《寄生前夜 第三次生日》那个游戏……它的玩法也是要控制主人公附身到别的角色上进行战斗的。外山：啊？很像吗？我有点震惊到了，因为我没玩过这部作品……其实这个系统是从《死魂曲》中的“视界截取”发掘过来的灵感。因为原本的“视界截取”是从尸人的视角去观察周围，而《死魂曲 2》中也有比如在“视界截取”状态下移动的设计，这次也是等于把这一系统更加深入地细化了一些。 Q. 我发现《野狗子》中并不存在地图，而能判断敌人方位的“气息”系统又似乎显得不够直观，所以为什么要这样处理呢？大仓：现在的市面上有地图的游戏很多，但我们觉得地图在《野狗子》中其实并不搭。因为有了地图的话玩家便会经常查看，这样就会分心，我们希望大家能够更进一步地沉浸在游戏中，所以最终决定把地图去掉。当然，游戏中有可能会发生迷路的问题。针对这一点，我们其实做了很多用户测试，通过反馈，把那些容易迷路的地方进行优化。另外，虽然本作中的场景看起来很复杂，但其实在设计方面并没有那么让人头晕，只是单纯地在美术上显得很多而已，请放心。 Q. 游戏中那种简洁的UI和干净的画面，也是出于这样的考虑吗？大仓：是这样的，地图很容易让玩家立刻感知到游戏世界的大小，自然就会减少沉浸感。UI也是做了精简，只保留了必要的部分。关于音乐： Q. 接下来想问一下山冈晃先生，不同于之前您所做的《寂静岭》，这次您想在《野狗子》中您想要传递给玩家怎样的感觉呢？山冈：对比起《寂静岭》，这次《野狗子》中除了音乐以外，连效果音、提示音等等全都是我一个人做的。说到想要传递给玩家的感觉呢……因为这次游戏的题材叫做“动作恐怖”（Horror Action）游戏，所以我觉得要平衡“恐怖”与“动作”这两个元素的平衡是非常重要的。比如像音乐的节奏、速度感等等。这算是这一次制作《野狗子》音乐最有挑战的一部分了。在创作《野狗子》的音乐的时候，我并没有采用传统的作曲方法，而是选择了一边看着游戏，一边思考“这里应该放什么声音”的这种模式。所以我希望通过这种方式，来为《野狗子》赋予声音上的独特性。然后这次毕竟是我一个人做嘛，我自己也比较穷。（笑）所以这次也算是拼尽全力。因为我跟外山先生早就从《寂静岭 1》就已经认识了，所以在这次合作中他也是允许我一边看着游戏一点一点完成，然后我再在中间一点一点地加入其他的内容。 Q. 那么既然这次游戏的舞台是架空的香港，山冈先生在作曲的时候是否有考虑过加入香港的音乐元素呢？山冈：在最初刚开始制作的时候还是挺困扰的。说实话，一开始我也是考虑是不是加入一些让大家一听“啊，是中国风”的音乐。虽然我之前也尝试着加入一些能表达中国的、香港的乐器或者音乐元素，但最后还是没有选择这样的做法。其实原因有两点，第一点是因为游戏的舞台发生在架空的香港，从游戏的视觉和美术上来讲，大家已经足够一目了然了。毕竟看到九龙那个地方，没有人会觉得那里是东京或者新宿吧。在这种情况下，如果在音乐上还要再去强调一些“中国风”的元素，我觉得是没有必要的。而且这种做法，会让音乐变得有些刻板，反而画蛇添足了。我觉得游戏中现有的视觉元素、设计元素已经足够了。 第二点是我认为游戏需要通过声音和音乐，来强调游戏的世界观的一种“温度感” —— 理解起来可能有些难，我觉得相较于视觉上给人那种“这个地方很热”“这个地方很冷”，应该是一种“角色内在的部分”的表现吧。这个角色是怎样的？这个世界会给人怎样的触感和印象？我想要通过声音的方式去强调、去传递这种感觉。所以比起要强调“中华风”，我觉得聚焦于氛围感本身可能更重要。 Q. 那我想问问您和外山先生是以怎样的方式合作的？是外山先生给您下一个详细需求您再制作呢？还是没什么干涉，让您随意发挥？山冈：啊，他可没下什么需求，比如说“一定要怎样怎样”这种。我们采取的都是“随时应对”的做法，毕竟我们也是熟识了，所以在游戏开发的过程中我们的联系非常紧密。我自己也经常会去问外山，“哎，这里的话这么做怎么样？”“这里设计成这样是什么意思？我这么做对不对”之类的问题。当然也会出现，“哦，这里不合适，得再调整一下”的情况。就这样一点一点地积累音乐，我也算是一直是陪着项目共同成长的吧。关于《死魂曲》，新工作室以及其他： Q. 接下来我想问外山先生一个关于《死魂曲》的问题。相信您也知道，每年在网上都会举办“死魂曲祭典”（異界入り祭り）这样的线上活动。每当一办这个活动，大家就都会觉得《死魂曲》是不是有可能要出新作了？当然，结果每次都是失望而归。 那我想问下，请问您考虑过要开发3代吗？还是说故事到2代就已经结束了？外山：以前在SIE的时候确实有想过这一些，一些东西已经在大脑中成型了。但毕竟现在IP在SIE那里，并且想要做还是要有一些条件，所以即便我是《死魂曲》的创作者，说想做的话也不一定管用啦……哦对，您也一定知道前几天《死魂曲》已经重新上架PS store并且加入会员阵容了吧？所以如果全球的各位玩家能够玩到这部作品，能够有更多的想要制作新作的呼声传递出来的话，或许时机成熟的话就会实现了。当然，前提是SIE也得同意让我们来做。如果要是明年夏天的“死魂曲祭典”能在全世界范围内搞起来的话，惊动了SIE的老板，那可能真的会有新作的希望吧！ Q. 如果排除所有困难的话，外山先生会想做《死魂曲 1》的重置版吗？外山：《死魂曲》的话确实有点难吧，主要是我自己不大想做。怎么说呢，最初的《死魂曲 1》不是启用了真人演员，把他们的表情放到游戏中从而营造出一些别样的恐怖与真实感。但放到现在来说，其实还是会有些风险的嘛。比如演员塌房了游戏会受影响，而且伴随着分辨率的提升，原来的那种方式可能也显得没那么恐怖了吧？至于开发方面到底好不好做，这一点其实我没怎么仔细考虑过。所以突然要说做的话，我只能说作为一个创作者，这确实是个很大的挑战了。 Q. 您说放现在做《死魂曲 1》的重置有些难，但您毕竟当年也还做了《死魂曲 1 新生》，虽然换了角色，但剧情仍然沿用一代。那为什么在那时候就做出来了呢？外山：其实之前有对外界说过，最早当时是好莱坞想要拍《死魂曲 1》的电影，然后我们同时推出一个与电影同时期推出的游戏版。本来说电影版是由好莱坞的著名导演山姆·雷米来担任，结果最后电影版黄了，游戏就先出来了……就是这么一回事儿。 Q. 《死魂曲》系列放到现在来看仍然是一部非常恐怖的游戏。但您刚才也说了，现在的画面分辨率变得越来越高。有些玩家会认为现在画面越来越清楚，那恐怖游戏也会因此变得不那么恐怖了。请问外山先生是如何看待这种恐怖的变化的？外山：哎呀，这个问题也太难了吧……因为山冈晃先生在旁边，刚做完的《寂静岭 2 重制版》挺恐怖的，销量上也挺成功的，这让我如何敢发表意见啊（笑）。我认为，能给人带来恐怖感的东西，是那种人看不清的、没见过不确定的东西。可能大家对以前玩过的游戏都会加一层美化滤镜。比如做《寂静岭 1》的时候，是我第一次用3D的环境去表现那种氛围，要放到现在的视角来看它还会不会可怕呢？这个我说不准。所以我觉得虽然现在游戏画面上越来越精致，但我觉得恐怖的本源是不会变的。说个题外话，抛开《寂静岭 2 重制版》不谈，我还是非常推荐现在的年轻开发者，去试着做些恐怖游戏。因为独立游戏可以用小规模、小预算来尝试更多的，表现恐怖的手段。可以做的放飞自我一些，开发者会获得属于自己的粉丝，也会做得更好吧。 Q. 其实我觉得《死魂曲》最恐怖地方的还是那个真实的脸。放在当年那个低多边形的建模上，那种脸就会给人一种非常冲击的感觉……外山：我们当时确实是故意这么设计的，看来目的达到了呢。 Q. 刚才说过，《野狗子》里的很多内容是最初为《死魂曲 3》而构思的，那为什么当年没能没实现呢？是资金原因，还是项目上的一些可控度问题？佐藤：像这种开发大型游戏，其实全世界的大公司们都会面临一个问题 —— 当你立项的游戏的规模开始变得越来越大的时候，资金的需求也会变得越来越多，自然，项目就更不允许失败。作为决策者，你就渐渐无法去尝试更多的新鲜的东西，比如从来没做过的新系统之类的。这是一种风险，我相信每一个大公司实际上都不敢冒这样的风险。 Q. 想问一下，在开发本作的时候，外山先生觉得最大的压力是来自哪里呢？是销量？还是说其他的东西？外山：对我来说做不出下一部作品才是最难受的吧。怎么说呢，像以前在大公司里去立项的时候，必须要背着那种“必须要卖个几百万套销量”的军令状这种事，这次是完全没有的，毕竟《野狗子》是一个彻头彻尾的新作。我觉得这一次是“要在有限的资源里把东西做完”给我带来的压力更大一些。这里的资源是也包括时间、预算、人力成本等方方面面。而在完成之后，接下来作品是不是能够获得市场认可，工作室能不能再继续走下去？这也是也是压力来源之一吧。 Q. 以前有很多有名的制作人向您一样独立出来开始做自己的作品，但有些作品是成功了，有些作品却没那么成功，外山先生是如何看待这个现象的呢？佐藤：评论别人肯定是不好的，所以就先讲讲我们自己吧。我们在独立出来之后，感觉运气还是挺好的。而说到运气，其中最重要的大概还是人才吧。在项目创立之初，团中的大家都是以前一起工作过、合作过、战斗过的同事们，大家都相互知根知底。通过这个契机，大家能聚集在一起，一起开发这款游戏，这应该是我们最幸运的事情吧。 Q. 说到大家都是以前的同事，那我想问一下外山先生，当您从SIE离开的时候，内心是怎样的？而到现在大家能够组建起团队创作新作品，其中的心路历程又是怎样的呢？外山：虽然没办法跟大家很详细地说明离开SIE的来龙去脉，但在SIE的时候，大家都非常擅长开发新IP……比如《死魂曲》啊《最后的守护者》之类的。所以其实像这种创新的东西，在SIE工作的时候想要推动起来其实还是有些难度的。离开SIE之后，最初就是我、佐藤和大仓三个人的小团队，然后伴随着新成员的加入，后来就形成了大约20个人的开发规模吧。其实最初的时候我们仨是想做一个像素风格的独立游戏，但后来我们发现好像也可以做一些稍微大一点的项目，于是就有了《野狗子》的企划。说实话，在离开SIE之后的这四年当中，一开始我也会觉得有些不安。毕竟这四年并不是说你只要“确立一个新项目，把这个新项目从无到有做完”这一件事而已，因为我们还要把公司运营起来。将这两件事同时推动，自然也是需要一些冒险和付出的。以前做项目的话，无论自己的能力如何或者是项目如何，其实最后都是公司来承受结果，而并不是我们自己。但这四年中，我发现无论做什么事，好也好，不好也好，包括马上要发售的《野狗子》，最终所有的结果都是要由自己来背负的。所以我自己也是有许多复杂的感情交织在了一起，不安、激动、期待、又或者说是恐惧，这些感情都会有一些。但总的来说，我觉得过程上还是蛮好的。 Q. 既然大家都是以前在SIE共事的同事们，那回溯过去，各位有没有什么会觉得遗憾的事情呢？假如说想要做之前作品的续作，大家想要做些什么呢？佐藤：最早没做出《死魂曲 3》还是觉得有点遗憾，但又因为一些当年的想法已经在《野狗子》中实现了，所以其实已经没什么遗憾了吧。与其说做之前作品的续作，倒不如说做点新的挑战一下自己。外山：我自己对VR技术还挺感兴趣的。其实在16-17年的时候就想用PS VR做点东西。但那时候尝试了很多想法，但最终并没有实现。希望未来可以有更多的机会吧。说到之前作品的续作……其实我也不是很想做其他的，因为以前这些作品的压力太大了。这次能自己做点新的也轻松一些。山冈：虽说刚做完《寂静岭 2 重制版》的音乐……从我个人的想法来说的话，果然还是想做《寂静岭 1》啊。大仓：我以前曾经做过《青之六号》的游戏版，想用最新的技术把这部作品重新做一遍。 Q. 最近这几年里，中国的投资团队这单机市场很感兴趣。这对于许多年轻的开发者而言是一些机遇。但也有一些团队会因为经验不足，导致开发成本不断上涨。而对于这些开发者们，您有什么建议吗？佐藤：日本的公司是“制作人管钱，导演管作品”这样的结构。所以这次《野狗子》能完成，我觉得其中一个原因就是因为在之前作品的开发过程中，我一直在做项目管理，能够计算出项目需要的人力物力和时间。所以这一次《野狗子》的开发，对我而言算是一次对之前工作的延长线。如果是一位没有经验的人来管钱管时间，项目就有很大可能会面临失败。从我的角度来看，我会非常频繁的跟导演们交流，确认当前的开发情况。毕竟很多导演为了打磨作品，会花费一些时间，所以到底是制作人妥协还是导演妥协，两者之间必须要好好交流，形成双方互相配合的良性循环。外山：我个人有几个斗胆建议吧：第一，人与人，作品与作品之间是会互相比较的。假如你作为一个开发者，承受不了来自大家的对比或者是批判的话，那不如来挑战一下大家从来没见过的、有新鲜感的东西。第二，从导演这个角度来看，我建议大家还是尽量去做自己想做的、有自己想法的东西。哪怕这个东西有许多有不成熟的地方，但只要足够独特，我相信也总是会有支持你、理解你的人在的。最后一点，就是不要在没钱的情况下让自己逞强做一些不合实际的东西，性价比暂且不谈，经过妥协做出来的作品一定不会是你想要的作品，所以在资金这一方面一定要要量力而行。 Q. 想问一点有趣的话题，佐藤先生作为制作人，您觉得外山先生是一位怎样的合作伙伴？是那种很难搞的还是比较听话的类型？佐藤：作为导演吧……有些时候外山先生确实有点年轻人的那种任性的感觉。当然，与其说是难搞，倒不如说是他作为导演，有不能退让的底线。在开发游戏这一方面，他有自己的坚持。 Q. 没想到的是，贵工作室竟然设立在了地下室，这是大家们的喜好吗？还是说有些别的客观原因？佐藤：最简单的就是因为房租便宜。（笑）不知道大家有没有去过品川那个SIE的办公室，那里是个很日系的大公司办公场所。现在既然独立出来了，然后有人就说既然机会难得，那就一定要做一个自己觉得可以工作，工作累了还能喝上一杯，有氛围的地方。之前我们真的去考察过居酒屋的，但最终没能选择的原因还是因为工作室有许多电脑，居酒屋那个地方电源电压负载不够。偶然间经朋友介绍发现了这样一个地方，不但租金合理，地段也不错。回想起来，当时建立工作室的时候恰好也是在疫情期间。所以能做出一个大家喜欢来上班的又能聚在负担得起的地方就好了。 Q. 刚才参观工作室的时候发现有好多《死魂曲》《重力异想世界》的周边，但这些不都是SIE的公司财产吗……外山：其实都是SIE送给我们的啦。毕竟我们是这些IP的创作者，SIE也非常大方，一股脑儿的就都拿过来了。比如有两个《重力异想世界》的雕像就是他们送的。 Q. 最后还是想问一下，《野狗子》马上就要发售了，那么工作室有没有开始考虑下一部作品的计划？佐藤：现在《野狗子》在做最后的收尾工作。至于下一步的计划嘛……虽然不能现在告诉大家，但我们肯定已经开始准备啦！ 我跟外山圭一郎掏心窝子地聊了《野狗子》，没诞生的《死魂曲 3》以及给开发者的建议https://www.gcores.com/articles/189939","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"温馨可爱肉鸽探索游戏《雾方福地》推出新群落更新，25%折扣同步开启","path":"/RSSBOX/rss/6787302f.html","content":"Daedalic Entertainment 与 Tiny Roar 的创意团队携手，隆重宣布其温馨类Roguelike探索游戏《雾方福地》的全新Dubious Dunes更新发布！《雾方福地》因其温馨但富有挑战性的世界而闻名，游戏为玩家提供了一个充满轻松探索的氛围，而这次的Dubious Dunes更新同样如此——但增添了冒险和神秘的元素。 &lt;内嵌内容，请前往机核查看&gt;欢迎来到Dubious Dunes准备好穿越被烈日炙烤的Dubious Dunes，一个神秘而危险的沙漠生物群落。这个为接近终局的老练玩家设计的新区域将挑战你在严酷的高温和潜伏其中的神秘生物中生存。这里充满了古老遗物和奇特角色，这片荒凉的沙漠不是胆小者的归宿。做好准备，迎接一场前所未有的寻宝冒险，只有准备充分的玩家才能平安回到村庄。 Dubious Dunes 更新有哪些新内容？玩家将探索诸如失落的地堡等新地点——这里藏有大量珍贵的发现，前提是你能避开它的诅咒！或者探索Junky Trunk，这是一个充满秘密的地方，等待你揭开其中的奥秘。遇见古怪的新NPC，如谨慎的历史学家、颇具怀疑心的Reptiloneat，以及出人意料的乐于助人的Hagwash。无论你是在寻找遗物还是与商贩交易，沙漠中总有新的事物在等着你。 新增特色与更新内容包括： · 新生物群落：探索危机四伏的沙漠Dubious Dunes，收获危险与宝藏。 · 新地点：在失落的地堡和Junky Trunk中发现隐藏的珍宝。 · 新NPC：从可爱却古怪的Hagwash到神秘的Reptiloneat，新的角色将为你的旅程增添深度。 · 新物品与升级：掌握Dubious Dunes，你将获得超过通常上限的财富与装备升级！ 游戏优化改进：除了全新内容外，更新还带来了一些玩家需求的修复和游戏改进。从更好的手柄导航到改善游戏过程中UI的清晰度，这些调整将确保更加顺畅、愉快的游戏体验。 Dubious Dunes更新现已上线，别犹豫，赶紧回到《雾方福地》温馨而神秘的世界吧！新的冒险、角色和宝藏正在等待着你，现在正是点亮你的余烬灯，踏上新旅程的最佳时机。 温馨可爱肉鸽探索游戏《雾方福地》推出新群落更新，25%折扣同步开启https://www.gcores.com/articles/189967","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"在试玩了《野狗子》之后，我确信它仍然充满了外山圭一郎最擅长的“邪典”味儿","path":"/RSSBOX/rss/decbb87a.html","content":"作者：雪豆备注：关于本次试玩会对外山先生的采访，请参见：《我跟外山圭一郎掏心窝子地聊了《野狗子》，没诞生的《死魂曲 3》以及给开发者的建议》我现在还能依稀记得2021年的TGA上，《野狗子》带给我的那股子“兴奋”劲儿。外山圭一郎是我特别喜欢的制作人，初代的《死魂曲》是我心目中恐怖游戏的“白月光”。当他的《野狗子》在放出第一个预告之后，配合着山冈晃负责的音乐，怎么想我都会觉得它应该会是个“能把我吓得晚上睡不着觉”的恐怖神作。 &lt;内嵌内容，请前往机核查看&gt;但结果想必大家也知道了，《野狗子》实际上是一款带了点“恐怖元素”的动作冒险游戏。以至于直到现在仍然有很多玩家觉得这游戏是“预告片欺诈”。当西总布问我要不要去Bokeh工作室采访的时候，我毫不犹豫地答应了下来 ——为什么《野狗子》是这样一款游戏？它到底好不好玩？以及《死魂曲》是不是还有出新作的可能？除了“追星”以外，我还有太多太多的问题想要得到外山先生的亲自解答。与其在发售前胡思乱想，倒不如真的自己玩一遍才能下判断。 Bokeh 工作室位于东京目黑区的目黑川旁，这里在春天樱花盛开的时候非常漂亮。但是工作室本身却是位于地下室内，这点倒是与我之前预想的不一样 —— 毕竟地下室这种地方，不见光也偏阴冷，感觉并不是一个很好的办公地点。但当我踏入工作室时，内部宛如酒吧一样的装潢却让我眼前一亮。巨大的《寂静岭 1》发售挂画似乎宣告着开发团队的资历，巨大的两尊《重力异想世界》的雕像看的让我有些眼红，门口的展示柜里面还摆满了各种《死魂曲》中曾经出现的海报、笔记本和工作证以及伊藤润二签名的外传漫画……没想到他们竟然真的把这些游戏中的东西做出来了。 打造成酒吧一样的接待处、休闲区上摆放的老漫画、门口已经设置好的一排《野狗子》的试玩台……让我感觉这里好像不是什么工作室，而是一个休闲网吧。让我紧张的心情一下子就放松了下来，也趁着还没开始试玩游戏之前，能够与外山先生和制作人佐藤一信先生稍微聊上两句。 佐藤先生说，其实他们特别希望能邀请中国媒体来试玩《野狗子》，因为这部作品发生在虚构的香港九龙，所以他们其实很担心游戏中的香港会表现的很奇怪。但无论是墙上挂着的设定图还是游戏中出现的那些海报和招牌，我都能感觉到他们在这一方面所下的功夫。 这让我不由得想起了《死魂曲》 —— 之所以它为什么到现在都这么让人印象深刻，就是因为在细节上的东西做的足够精致，才会让人相信这些东西是真实存在的。 初见印象开发团队为这次的试玩准备了《野狗子》的PC版本，而我们也是全球第一波能够玩到这部作品完整序章以及前中期部分的人。游戏自带三种难度，可以在游戏中随时调整。在点开《野狗子》菜单的一刹那，我就能感觉到一种“外山圭一郎”的味道 —— 诡异、奇妙、又带有一些真实色彩的开头动画，配上有些奇怪却非常带感的粤语歌，扑面而来的真就是一股低成本Cult片的味道。 建模并不细致的角色与细致到让人忍不住停下的场景，形成了一种又真又假的微妙感。至少在我目光所及的地方，我是没看到什么奇怪的“日式粤语”出现。游戏中，玩家将操作一个失去了记忆的黄色灵魂，只能通过附身到别人身上来苟延残喘下去。附身之后的灵魂可以去触碰画面中的“记忆碎片”，来逐渐唤醒它的使命 —— 去猎杀潜伏在人类中间的怪物“食头鬼”。顾名思义，食头鬼会杀害人类并以大脑为食，是相当凶恶也是相当危险的生物，片头动画中被警察发现的尸体便是遭其所害。 游戏刚开始没多久，食头鬼便会现身来猎杀人类。这时候玩家需要操作灵魂在短时间里找到下一个附身对象，同时也需要用快速多次附身来转移位置，躲避追杀。如果附身对象被杀，会减少一格灵魂的能量，当三格灵魂能量都失去时游戏便会结束，这一开始的流程还是相当紧张刺激的。 当玩家从小巷里逃到大街上之后，熙熙攘攘的人群似乎意味着危险已过。灵魂可以自由附身这些普通人。根据年龄性别的不同，这些人在操作手感上有点微妙的区别。虽然能跑能跳，但玩家可以通过附身特定的人来到铁栅栏的对面或者高处，将实现更便捷的移动。伴随着故事的推进，灵魂突然感应到食头鬼在追杀一个短发少女，此时可以通过感应食头鬼的视角和气息来判断它所在的位置前往营救。玩到这里我差点没在电脑前乐出声 —— 这不就是《死魂曲》里的经典系统“视界截取”么？ 核心战斗当玩家操纵的角色前往食头鬼所在地后，第一场正式的战斗也就随之打响。灵魂附身的普通人只有普通攻击，特殊攻击，回避，格挡这几个招式。而攻击或者防御所使用的武器被称为 “凝血武器”。 顾名思义，这把在制造出来的时候会消耗角色的血量，同时又具有一定的耐久度，格挡会消耗耐久度，耐久度为零之后则需要再制造一把才能继续攻击。另外每个附身的人身上还会拥有特殊能力，比如投掷血液炸弹之类的。有的会消耗灵力，有的会消耗血量。除了决定角色生死以外，《野狗子》中的血量似乎承载了更多的资源属性。在防御姿态下，敌人的一部分攻击会在画面中显示方向，当敌人攻击到角色的一瞬间该方向指示会发亮，此时只要向对应方向推动右摇杆即可弹反敌人的攻击。弹反能够化解伤害使敌人造成硬直、回复己方灵力、回复凝血武器耐久、甚至可以积累一个特殊能量槽，能量槽满了后还可以进入子弹时间尽情输出。 这种高收益并且玩起来有些乐趣的系统，一下子让我觉得《野狗子》原本看起来有些呆板的战斗有了不少乐趣。在最初的体验中，我其实非常依靠弹反这一系统。因为弹反的收益很大，同时弹反只有在防御中才能触发，相对来说角色自身也比较安全。另外弹反判定的窗口期并不是非常苛刻，只需要稍微练习一些就能够掌握。不过我会觉得这种“等着敌人”出招的玩法比较影响战斗的节奏，经常会有敌我双方都不出招，大眼瞪小眼的场面出现。佐藤先生解释说其实还是因为来试玩的大家都是游戏高手，所以没能频繁使用这一系统。通过玩家测试，他们发现弹反在实际使用上还是有些门槛，也并不会担心玩家会过于依赖这个系统。 另外，《野狗子》并不鼓励玩家只用一个普通人战斗，而是鼓励玩家连续附身到其他人身上。一来可以快速从敌人身上转移出去减少危险，二来转移到其他人时还可以暂时增加角色的HP上限。就这样前仆后继，只要人够多，葫芦娃也是能救出爷爷的。可正如我所预料的那样，普通人的攻击操作特别的“硬” —— 攻击方式单调不说，动作的硬直也很大，而且打到敌人身上的反馈也不是很明显，一言蔽之就是“不好用”。这让我在开始的时候适应了好一段时间，直到“奇才”这个角色登场才有了极大地缓解。 在游戏的世界中存在着一些有特殊能力的“奇才”，这些奇才有着不同的特性，同时也肩负了推动剧情的重任。比如游戏一开始解锁的“朱莉”（JULEE）可以利用血爪来对敌人快速造成大量伤害，她自带的特殊技巧和被动能力兼具爆发和远程等多种战斗场景，移动快和高血量也让角色用起来得心应手，只要三下五除二就可以将让普通人陷入苦战的食头鬼轻松地闹成齑粉。说实话，我觉得附身系统玩起来太像《寄生前夜 第三次生日》了 —— 女主角阿雅同样可以附身到周围的士兵上进行战斗。外山先生在得知了我的疑问之后比较震惊，他解释这一系统其实是《死魂曲 2》中木船郁子的能力“感应视”设计的延伸，最初是想用在《死魂曲3》中。而至于“其他生物视点看角色”这个设计，也是从《死魂曲》中拿过来的。 在结束了序章部分的试玩之后，工作人员帮我开启了游戏前中期的一段战斗内容。这时游戏已经解锁了一定的流程，除了之前登场的“朱莉”以外，还可以使用之前宣传片中骑着摩托戴着头盔的主人公“李佳龙”（ALEX）。玩家可以在菜单中选择听取他们的对话从而触发下一个剧情，也可以给他们升级能力、更改特殊招式、更换服装和面具。 每一个角色都有一段属于他们自己的故事线，有时候在同一时间下，不同的角色会有自己的行动。看着游戏中那有符号感的UI以及角色的正面肖像，我不禁又想起了《死魂曲》 —— 果然，这种多线叙事的剧情结构传递出来的，确实还是那股子熟悉的“外山味儿”。 这段剧情中，奇才们需要去找到一个故事中的关键人物。但当他们赶到这个名为“不夜楼阁”这个地点的时候，发现这里已经遭到食头鬼的入侵，所以玩家需要从楼底开始杀出一条血路，向最上层前进。 如果说游戏的开篇部分是一种“邪典片”的味道，那么在这里就有一股味道更浓的、“血腥片”的感觉了 —— 血淋淋的场景和一地的尸体所弥漫出来的味道蔓延在香港那种狭窄楼道中，拐角处猎头怪的影子又透露出了一种让人背后发毛的感觉。可说实在的，除了诡异以外，我确实没有感觉到任何一点能吓到我的元素。什么心理暗示？什么JUMP SCARE？统统没有，有的就是杀出一条血路的爽快感。 当来到楼顶后，玩家需要利用附身去追击敌人，并且击败一个大型BOSS。初见BOSS时还觉得难度不高，但进入二阶段后BOSS的攻击会变得更加频繁，有些时候甚至会使用出连续五六下的攻击，即便是奇才，一着不慎也会被干掉。如果奇才被高伤害的招式打中之后，甚至还会触发“断臂”的DEBUFF，让战斗形式变得愈发危险。所以此时要么就附身到普通人身上让奇才自己恢复，要么就是吸收打死敌人之后留下的血迹来恢复。当战斗强度上来之后，如果胡乱附身，普通人会被敌人接连打倒导致无身可附；而用吸收血迹来恢复，有些时候在敌人高频率的攻击面前又显的捉襟见肘。玩家需要在即时的动作性和全局战略性中找到平衡，才能够逐渐发现“附身”的乐趣所在。 必须承认的是，预告片中展现出来的那些“看起来不妙”的地方是真实存在的。但通篇玩下来，我却对《野狗子》有了很大的改观 —— 游戏中那些丰富的玩法和一些颇有趣味的小心思，大大减少了这些不妙给我带来的负面感觉。在几个小时的体验后，我仍然可以找到它所蕴藏着的、非常独特的那个味道，并且还会意外地觉得……这游戏，其实还挺有意思。 后来又聊了聊别的在结束了试玩之后，Bokeh工作室准备了丰盛的寿司招待我们，还请了我品尝了游戏中登场的啤酒“九龙鞭炮”。这款艾尔酒精度数为5°，入口偏酸，后劲有些辛辣，确实是我未曾感受过的味道。 借着酒劲，我又与外山先生和佐藤先生敞开话匣子聊了很久。几位制作人非常平易近人，完全没有任何架子，让我心里感觉暖暖的。其实他们自己也知道许多玩家对《野狗子》的预期并不像想象中那样，在一开始预告片公布之后，佐藤先生看到了玩家们的反应后也一直在喊冤“其实我们真的没想给人传递出一种这是一个‘特别恐怖’的作品，”他说：“《野狗子》在一开始就是想做一个动作游戏来着。” 此时再回头看看第一个预告片，好像也确实没最初感觉上那么恐怖了。我很清楚做恐怖游戏的“困境”，太恐怖的游戏卖不动，减弱恐怖感会遭到玩家的诟病，佐藤先生听到我的意见之后赶忙点头，解释道：“所以我们也不想又再做一个纯恐怖的游戏，毕竟之前也做过了”。而作为游戏开发主心骨的外山先生是一个特别喜欢尝试新东西的人，所以虽然他能够做出《寂静岭 1》和《死魂曲 1、2》，但他也可以做出两部与前者截然不同的《重力异想世界》。 如果真的要把外山视作一个“恐怖游戏大师”的话，似乎就会成为一种刻板印象。想到这里，我竟然会感觉到有一种愧疚，把原本对《死魂曲》续作的期待，强行加载到《野狗子》上并将它视作一种恐怖游戏的代餐，从归根结底上，这个想法就是错误的。外山先生表示，他们自己其实就是一个小团队而已，没有什么技术积累，也没有太多开发动作游戏的经验 ——游戏中那看似呆板的动作就是最好的证据。而他自己也深刻地知道目前《野狗子》中的那些不足，“没有配音这件事别说海外了，就连日本国内玩家意见也很大”他笑着说道。 可从这次的初步体验来看，我觉得《野狗子》很对我胃口的一点，就是游戏会用一些有趣的小机制，去扬长避短，弥补这些明显是技术上所导致的不足：战斗手感不好？那我就用弹反和附身去增加其他方面的趣味性；角色建模一般？那我就用细致的场景元素去打造独特的氛围感；可操纵角色太少？那我就加入更多的特殊技能丰富角色的特点……聊天的时候我发现外山先生在不停地捶腰，想到这里其实还是有些心疼：你说都都这么大把年纪了，这些老将们非要从大厂子出来在拼一把，还能整出这么多点子来，到底是图啥呢？ 我一直觉得外山先生的作品其实比较需要“对电波”，这部也同样不例外。喜欢的人会特别喜欢，不吃它这一套的人或许也需要面对一点门槛。但我可以说，《野狗子》绝对称不上“烂”，于我而言，大概多半只能用一个“怪”字来形容。本作将于11月8日在全平台推出，等到正式版推出之后，我再跟大家聊一聊更深层的感受吧。 在试玩了《野狗子》之后，我确信它仍然充满了外山圭一郎最擅长的“邪典”味儿https://www.gcores.com/articles/189945","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"战火再燃卷土重来！《角斗士2》定档11月22日","path":"/RSSBOX/rss/721e6d27.html","content":"由美国派拉蒙影片公司出品的电影《角斗士2》正式官宣定档11月22日。作为大师级导演雷德利·斯科特再执导筒续写罗马帝国传奇史诗新篇章，奥斯卡最佳影片得主、豆瓣8.6分影史经典《角斗士》续作《角斗士2》，时隔24年强势回归。 &lt;内嵌内容，请前往机核查看&gt;《角斗士2》不仅再续前作之恢弘磅礴，更迎来高能尺度、视觉场面的全面升级！电影围绕新主角卢修斯的复仇展开，当家园被罗马暴君占领，背负屈辱的他被迫进入斗兽场，生死一战。罗马的自由与荣耀，他的隐秘过往成为他一次次获取力量的源泉。预告片中，海战对决、刀光剑影、兵团厮杀亦或是凶猛的巨兽咆哮袭来，角斗场的大门开启之前，卢修斯永远不知道将面对的是什么，他将如何用肉身与鲜血赢回尊严？如海报所示，无惧权力阴谋和挥洒鲜血的角斗士，双手紧握勇气之剑，即将血战到底，亲身见证罗马帝国的崩塌与重生。 作为一部承载着历史厚重感与荣誉的史诗巨制，《角斗士》凭借引人入胜的剧情、震撼的场面和深刻的内核，一举揽下第73届奥斯卡最佳影片、最佳男主角、最佳视觉效果等5项大奖及12项提名。顶级名导雷德利·斯科特因《银翼杀手》、《异形》系列、《火星救援》等经典作品家喻户晓，此番再度归来，将书写更为精彩的《角斗士》新篇章。本片还汇聚了众多实力派演员，新晋主角由保罗·麦斯卡饰演，他曾主演电影《晒后假日》并获奥斯卡最佳男主提名，精湛演技和表现力不言而喻。 在《权力的游戏》中饰演“红毒蛇”、在《曼达洛人》中挑大梁的佩德罗·帕斯卡再度拿起兵器，化身骁勇善战的罗马大将军；更有奥斯卡影帝，被称为“好莱坞最具号召力演员之一”的丹泽尔·华盛顿出演足智多谋的神秘政客；而在《寂静之地：入侵日》中广受好评的约瑟夫·奎恩也在本片饰演新任罗马皇帝。 战火再燃卷土重来！《角斗士2》定档11月22日https://www.gcores.com/articles/189964","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"【抽送模型与Steam激活码】《无人深空》免费远征更新“诅咒”现已上线","path":"/RSSBOX/rss/2ddc446d.html","content":"《无人深空》的宇宙中一直隐含着一丝诡异的氛围。尤其是在Desolation更新中，幽暗的废弃飞船漂泊在遥远的太空深处，而“漂泊”（Adrift）更新中，整个宇宙化为一场孤寂的体验。然而，早在“阿特拉斯崛起”时期，现实间的边界开始崩塌，古老的传送门首次开启，而对于那些知情者来说，16这个数字的重要性也逐渐浮出水面…… 在带来钓鱼系统的一个多月后，《无人深空》这次又带来了第16次免费远征更新——诅咒（THE CURSED）。 &lt;内嵌内容，请前往机核查看&gt;本次“诅咒”远征发生在一个迟暮黄昏的领域中，徘徊在现实与虚幻的边界。旅行者将无法使用超光速引擎，这意味着无法实现星系间的跃迁。取而代之的是，星际旅行只能通过古老的传送门实现。 某些从其他维度传来的声音将为你提供指引，它们带来了信息、奇怪的图纸……以及更多谜团。你必须自己做出判断，这些声音属于谁，来自何方，是否值得信任。 在《无人深空》的常规玩法中，旅行者必须面对各种环境危害，如极端温度、辐射和有毒大气。然而，在“诅咒”远征中，你将面对一种更为危险却不易察觉的威胁：现实边界的削弱。为了保证你的外骨骼套装在当前现实中正常运作，将配备专门的边界抑制器。 幽灵异象边界间充斥着漂浮着的生物，如鬼魅般可怖，聚在一起注视着穿越它们领域的旅行者。通常，它们只是静静地观察，但要注意：随着边界抑制器功能逐渐减弱，时间将以非线性的方式流动，它们可能会变得充满敌意。 神秘灵药旅行者将需要准备多种异界灵药来应对“诅咒”远征中的挑战。使用玻璃灵药维持边界强度，用水银灵药找寻传送门位置，用血之灵药开启通道。在旅途的终点，面对最后的试炼，你要饮下净水灵药——而随之而来的后果，将由你亲自见证。 “诅咒”远征今天正式开启，将持续大约两周。穿越层层边界，抵御幽灵异象——或者，屈服于玻璃世界的召唤。 远征奖励印有边界异象、传送门裂隙和符文的海报。用这些装饰性的海报纪念你穿越边界的旅程，并且提醒那些想要进入传送门的人。 异象封印基地部件，在“诅咒”远征期间，这个古老的道具将释放出的幽灵能量，保护附近实体免受现实崩塌的影响。而在更稳定的宇宙中，它则作为一种来自边界之外的装饰纪念品。 墨染喷气背包，独家喷气背包定制装饰。喷气背包核心中的局部异象，令来自边界恐怖的幽灵的余辉渗透到现实世界中。 边界恐怖装饰，独特的外骨骼外观覆盖。将你的外观变为边界恐怖的一部分——它们是那些在传送门深处潜伏着的碎片之一。 荧光生物伙伴，一颗果冻般生物所产下的蛋，散发着令人毛骨悚然的苍白光芒。 边界先驱星舰飞船，在崩塌的现实中获取这艘独特的飞船，驾驶这艘碟形飞船在星空中翱翔。 Hello Games团队还表示将继续构建《无人深空》宇宙，更多内容即将推出。期待新作《无火之光》的玩家也可以将其加入愿望单，获得第一手资讯。 &lt;内嵌内容，请前往机核查看&gt; 【抽送模型与Steam激活码】《无人深空》免费远征更新“诅咒”现已上线https://www.gcores.com/articles/189960","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"《蜡笔小新 煤炭镇的小白》STEAM版今日发售，参展Weplay 2024","path":"/RSSBOX/rss/5f94efa4.html","content":"Tecmira Holdings株式会社旗下子公司 Neos 株式会社宣布：《蜡笔小新 煤炭镇的小白》（以下简称“本作”）的全球PC平台Steam版以及Nintendo Switch欧美地区下载版于今日正式发售。 &lt;内嵌内容，请前往机核查看&gt;为纪念发售，即日起至11月6日（周三），在两大平台上将举办本作以及前作《蜡笔小新 我与博士的暑假～永不结束的七日旅程～》（以下简称“前作”）的优惠捆绑促销活动。 在Steam平台上，除了提供“普通版”外，还推出了“数字豪华版”供玩家选择。数字豪华版除游戏软件外，还包含特制的数字艺术画册（PDF），内含精心打磨的背景美术与开发资料等，支持五种语言（简体中文、繁体中文、日语、英语、韩语）。此外，豪华版还收录了27首主题曲及背景音乐的原声带（MP3），玩家可自行下载收藏。对于已经拥有NintendoSwitch版本软件的用户，将以“数字内容套装”的形式单独贩售特别画册和原声带。 此外，在今年的 Weplay 2024（C04 &amp; 06）展位上，玩家可试玩到STEAM版本的《蜡笔小新 煤炭镇的小白》，还有机会获得纪念卡片，与小新、小白合影留念喔！ 本作自推出Nintendo Switch版本以来，在日本及亚洲地区广受好评，并在今年8月举办的游戏盛会“CEDEC AWARDS 2024”中获得了“视觉艺术部门优秀奖”，凭借其精美的画面和独特的世界观赢得了高度评价。 蜡笔小新不仅在日本广受欢迎，也在全球多个国家及地区拥有极高人气。例如，最新剧场版《蜡笔小新：我们的恐龙日记》已确认引进中国大陆，西班牙也将于10月18日上映电影《蜡笔小新：新次元！超能力大决战 飞吧飞吧手卷寿司》。基于全球关注度以及前作的反响，本次的Steam版本同样将在全球70多个国家推出。 Neos株式会社表示：未来将继续创造更多机会，加速推动国际化进程，让全球更多的玩家能够体验本作。 《蜡笔小新 煤炭镇的小白》STEAM版今日发售，参展Weplay 2024https://www.gcores.com/articles/189963","tags":["ACG","Gcores"],"categories":["ACG","Gcores"]},{"title":"BUNDLES ARE FINALLY in MINECRAFT!","path":"/RSSBOX/rss/f0bf06dd.html","content":"Bundles are finally here! Join our developers as we dig deeper into this space-saving item and learn more about how they can help you tame your ever-growing inventory stacks. Learn what ancient real-world item inspired this feature, why the crafting recipe was simplified, and why bundle-ception proved a tricky bug!Learn more here: https://www.minecraft.net/article/bundles-of-bravery BUNDLES ARE FINALLY IN MINECRAFT!https://www.youtube.com/watch?v=ocayRQ-JwHw","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"4.2 Tbps of Bad Packets and a Whole Lot More: Cloudflare's Q3 DDoS Report","path":"/RSSBOX/rss/5f05f99d.html","content":"Welcome to the 19th edition of the Cloudflare DDoS Threat Report. Released quarterly, these reports provide an in-depth analysis of the DDoS threat landscape as observed across the Cloudflare network. This edition focuses on the third quarter of 2024.With a 296 Terabit per second (Tbps) network located in over 330 cities worldwide, Cloudflare is used as a reverse proxy by nearly 20% of all websites. Cloudflare holds a unique vantage point to provide valuable insights and trends to the broader Internet community. Key insights The number of DDoS attacks spiked in the third quarter of 2024. Cloudflare mitigated nearly 6 million DDoS attacks, representing a 49% increase QoQ and 55% increase YoY.Out of those 6 million, Cloudflare’s autonomous DDoS defense systems detected and mitigated over 200 hyper-volumetric DDoS attacks exceeding rates of 3 terabits per second (Tbps) and 2 billion packets per second (Bpps). The largest attack peaked at 4.2 Tbps and lasted just a minute.The Banking &amp; Financial Services industry was subjected to the most DDoS attacks. China was the country most targeted by DDoS attacks, and Indonesia was the largest source of DDoS attacks.To learn more about DDoS attacks and other types of cyber threats, visit our Learning Center, access previous DDoS threat reports on the Cloudflare blog, or visit our interactive hub, Cloudflare Radar. There's also a free API for those interested in investigating these and other Internet trends. You can also learn more about the methodologies used in preparing these reports. Hyper-volumetric campaign In the first half of 2024, Cloudflare’s autonomous DDoS defense systems automatically detected and mitigated 8.5 million DDoS attacks: 4.5 million in Q1 and 4 million in Q2. In Q3, our systems mitigated nearly 6 million DDoS attacks bringing it to a total of 14.5 million DDoS attacks year-to-date. That’s an average of around 2,200 DDoS attacks every hour.Of those attacks, Cloudflare mitigated over 200 hyper-volumetric network-layer DDoS attacks that exceeded 1 Tbps or 1 Bpps. The largest attacks peaked at 3.8 Tbps and 2.2 Bpps. Read more about these attacks and how our DDoS defense systems mitigated them autonomously. Distribution of hyper-volumetric DDoS attacks over timeAs we were writing this blog post, our systems continued to detect and mitigate these massive attacks and a new record has just been broken again, only three weeks after our last disclosure. On October 21, 2024, Cloudflare’s systems autonomously detected and mitigated a 4.2 Tbps DDoS attack that lasted around a minute. 4.2 Tbps DDoS attack mitigated autonomously by Cloudflare DDoS attack types and characteristics Of the 6 million DDoS attacks, half were HTTP (application layer) DDoS attacks and half were network layer DDoS attacks. Network layer DDoS attacks increased by 51% QoQ and 45% YoY, and HTTP DDoS attacks increased by 61% QoQ and 68% YoY. Attack duration 90% of DDoS attacks, including the largest of attacks, were very short-lived. We did see, however, a slight increase (7%) in attacks lasting more than an hour. These longer attacks accounted for 3% of all attacks. Attack vectors In Q3, we saw an even distribution in the number of network-layer DDoS attacks compared to HTTP DDoS attacks. Of the network-layer DDoS attacks, SYN flood was the top attack vector followed by DNS flood attacks, UDP floods, SSDP reflection attacks, and ICMP reflection attacks.On the application layer, 72% of HTTP DDoS attacks were launched by known botnets and automatically mitigated by our proprietary heuristics. The fact that 72% of DDoS attacks were mitigated by our home-grown heuristics showcases the advantages of operating a large network. The volume of traffic and attacks that we see let us craft, test, and deploy robust defenses against botnets.Another 13% of HTTP DDoS attacks were mitigated due to their suspicious or unusual HTTP attributes, and another 9% were HTTP DDoS attacks launched by fake browsers or browser impersonators. The remaining 6% of “Other” includes attacks that targeted login endpoints and cache busting attacks.One thing to note is that these attack vectors, or attack groups, are not necessarily exclusive. For example, known botnets also impersonate browsers and have suspicious HTTP attributes, but this breakdown is our attempt to categorize the HTTP DDoS attacks in a meaningful way. Distribution of DDoS attacks in 2024 Q3In Q3, we observed a 4,000% increase in SSDP amplification attacks compared to the previous quarter. An SSDP (Simple Service Discovery Protocol) attack is a type of reflection and amplification DDoS attack that exploits the UPnP (Universal Plug and Play) protocol. Attackers send SSDP requests to vulnerable UPnP-enabled devices such as routers, printers, and IP-enabled cameras, and spoof the source IP address to be the victim’s IP address. These devices respond to the victim’s IP address with large amounts of traffic, overwhelming the victim’s infrastructure. The amplification effect allows attackers to generate massive traffic from small requests, causing the victim’s service to go offline. Disabling UPnP on unnecessary devices and using DDoS mitigation strategies can help defend against this attack. Illustration of an SSDP amplification attack User agents used in HTTP DDoS attacks When launching HTTP DDoS attacks, threat actors want to blend in to avoid detection. One tactic to achieve this is to spoof the user agent. This lets them appear as a legitimate browser or client if done successfully.In Q3, 80% of HTTP DDoS attack traffic impersonated the Google Chrome browser, which was the most common user agent observed in attacks. More specifically, Chrome 118, 119, 120, and 121 were the most common versions.In second place, no user agent was seen for 9% of HTTP DDoS attack traffic.In third and fourth place, we observed attacks using the Go-http-client and fasthttp user agents. The former is the default HTTP client in Go’s standard library and the latter is a high-performance alternative. fasthttp is used to build fast web applications, but is often used for DDoS attacks and web scraping too. Top user agents used in DDoS attacksThe user agent hackney came in fifth place. It’s an HTTP client library for Erlang. It's used for making HTTP requests and is popular in Erlang/Elixir ecosystems.An interesting user agent shows up in the sixth place: HITV_ST_PLATFORM. This user agent appears to be associated with smart TVs or set-top boxes. Threat actors typically avoid using uncommon user agents, as evidenced by the frequent use of Chrome user agents in cyberattacks. Therefore, the presence of HITV_ST_PLATFORM likely suggests that the devices in question are indeed compromised smart TVs or set-top boxes.In seventh place, we saw the uTorrent user agent being used in attacks. This user agent is associated with a popular BitTorrent client that’s used for downloading files.Lastly, okhttp was the least common user agent in DDoS attacks despite its popularity as an HTTP client for Java and Android applications. HTTP attack attributes While 89% of HTTP DDoS attack traffic used the GET method, it is also the most commonly used HTTP method. So when we normalize the attack traffic by dividing the number of attack requests by total request per HTTP method, we get a different picture.Almost 12% of all requests that used the DELETE method were part of an HTTP DDoS attack. After DELETE, we see that HEAD, PATCH and GET are the methods most commonly used in DDoS attack requests. While 80% of DDoS attack requests were over HTTP/2 and 19% were over HTTP/1.1, they represented a much smaller portion when normalized by the total traffic by version. When we normalize the attack requests by all requests by version, we see a different picture. Over half of traffic to the non-standard or mislabeled “HTTP/1.2” version was malicious and part of DDoS attacks. It's important to note that “HTTP/1.2” is not an official version of the protocol. The vast majority of HTTP DDoS attacks are actually encrypted — almost 94% — using HTTPS. Targets of DDoS attacks Top attacked locations China was the most attacked location in the third quarter of 2024. The United Arab Emirates was ranked second, with Hong Kong in third place, followed closely by Singapore, Germany, and Brazil. Canada was ranked seventh, followed by South Korea, the United States, and Taiwan as number ten. Top attacked industries In the third quarter of 2024, Banking &amp; Financial Services was the most targeted by DDoS attacks. Information Technology &amp; Services was ranked in second place, followed by the Telecommunications, Service Providers, and Carriers sector. Cryptocurrency, Internet, Gambling &amp; Casinos, and Gaming followed closely behind as the next most targeted industries. Consumer Electronics, Construction &amp; Civil Engineering, and the Retail industries rounded out the top ten most attacked industries. Sources of DDoS attacks Threat actors For a few years now, we’ve been surveying our customers that have been subjected to DDoS attacks. The survey covers various factors, such as the nature of the attack and the threat actors. In the case of threat actors, while 80% of survey respondents said that they don’t know who attacked them, 20% said they did. Of those, 32% said that the threat actors were extortionists. Another 25% said a competitor attacked them, and another 21% said that a disgruntled customer or user was behind the attack. 14% of respondents said that the attacks were carried out by a state or a state-sponsored group. Lastly, 7% said that they mistakenly attacked themselves. One example of when a self-DDoS attack occurs is a post-firmware update for IoT devices that causes all devices to phone home at the same time, resulting in a flood of traffic. Distribution of the top threat actorsWhile extortionists were the most common threat actor, overall, reports of Ransom DDoS attacks decreased by 42% QoQ, but increased 17% YoY. A total of 7% of respondents reported being subjected to a Ransom DDoS attack or threatened by the attacker. In August, however, that figure increased to 10% — that’s one out of ten. Reports of Ransom DDoS attacks by quarter Top source locations of DDoS attacks Indonesia was the largest source of DDoS attacks in the third quarter of 2024. The Netherlands was the second-largest source, followed by Germany, Argentina, and Colombia. The next five largest sources included Singapore, Hong Kong, Russia, Finland, and Ukraine. Top source networks of DDoS attacks For service providers that operate their own networks and infrastructure, it can be difficult to identify who is using their infrastructure for malicious intent, such as generating DDoS attacks. For this reason, we provide a free threat intelligence feed to network operators. This feed provides service providers information on IP addresses from within their networks that we’ve seen participate in subsequent DDoS attacks.On that note, Hetzner (AS24940), a German-based IT provider, was the largest source of HTTP DDoS attacks in the third quarter of 2024. Linode (AS63949), a cloud computing platform acquired by Akamai in 2022, was the second-largest source of HTTP DDoS attacks. Vultr (AS64515), a Florida-based service provider, came in third place.Netcup (AS197540), another German-based IT provider, came in fourth place. Google Cloud Platform (AS15169) followed in fifth place. DigitalOcean (AS14061) came in sixth place, followed by French provider OVH (AS16276), Stark Industries (AS44477), Amazon Web Services (AS16509), and Microsoft (AS8075). Networks that were that largest sources of HTTP DDoS attacks in 2024 Q3 Key takeaways This quarter, we observed an unprecedented surge in hyper-volumetric DDoS attacks, with peaks reaching 3.8 Tbps and 2.2 Bpps. This mirrors a similar trend from the same period last year, when application layer attacks in the HTTP/2 Rapid Reset campaign exceeded 200 million requests per second (Mrps). These massive attacks are capable of overwhelming Internet properties, particularly those relying on capacity-limited cloud services or on-premise solutions.The increasing use of powerful botnets, fueled by geopolitical tensions and global events, is expanding the range of organizations at risk — many of which were not traditionally considered prime targets for DDoS attacks. Unfortunately, too many organizations reactively deploy DDoS protections after an attack has already caused significant damage.Our observations confirm that businesses with well-prepared, comprehensive security strategies are far more resilient against these cyberthreats. At Cloudflare, we’re committed to safeguarding your Internet presence. Through significant investment in our automated defenses and a robust portfolio of security products, we ensure proactive protection against both current and emerging threats — so you don’t have to. 4.2 Tbps of bad packets and a whole lot more: Cloudflare's Q3 DDoS reporthttps://blog.cloudflare.com/ddos-threat-report-for-2024-q3","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"Fearless SSH: Short-Lived Certificates Bring Zero Trust to Infrastructure","path":"/RSSBOX/rss/8d1eb49f.html","content":"BastionZero joined Cloudflare in May 2024. We are thrilled to announce Access for Infrastructure as BastionZero’s native integration into our SASE platform, Cloudflare One. Access for Infrastructure will enable organizations to apply Zero Trust controls in front of their servers, databases, network devices, Kubernetes clusters, and more. Today, we’re announcing short-lived SSH access as the first available feature. Over the coming months we will announce support for other popular infrastructure access target types like Remote Desktop Protocol (RDP), Kubernetes, and databases. Applying Zero Trust principles to infrastructure Organizations have embraced Zero Trust initiatives that modernize secure access to web applications and networks, but often the strategies they use to manage privileged access to their infrastructure can be siloed, overcomplicated, or ineffective. When we speak to customers about their infrastructure access solution, we see common themes and pain points:Too risky: Long-lived credentials and shared keys get passed around and inflate the risk of compromise, excessive permissions, and lateral movementToo clunky: Manual credential rotations and poor visibility into infrastructure access slow down incident response and compliance effortsSome organizations have dealt with the problem of privileged access to their infrastructure by purchasing a Privileged Access Management (PAM) solution or by building a homegrown key management tool. Traditional PAM solutions introduce audit logging and session recording features that capture user interactions with their servers and other infrastructure and/or centralized vaults that rotate keys and passwords for infrastructure every time a key is used. But this centralization can introduce performance bottlenecks, harm usability, and come with a significant price tag. Meanwhile, homegrown solutions are built from primitives provided by cloud providers or custom infrastructure-as-code solutions, and can be costly and tiresome to build out and maintain. We believe that organizations should apply Zero Trust principles to their most sensitive corporate resources, which naturally includes their infrastructure. That’s why we’re augmenting Cloudflare’s Zero Trust Network Access (ZTNA) service with Access to Infrastructure to support privileged access to sensitive infrastructure, and offering features that will look somewhat similar to those found in a PAM solution:Access: Connect remote users to infrastructure targets via Cloudflare’s global network.Authentication: Eliminate the management of credentials for servers, containers, clusters, and databases and replace them with SSO, MFA, and device posture. Authorization: Use policy-based access control to determine who can access what target, when, and under what role. Auditing: Provide command logs and session recordings to allow administrators to audit and replay their developers’ interactions with the organization’s infrastructure.At Cloudflare, we are big believers that unified experiences produce the best security outcomes, and because of that, we are natively rebuilding each BastionZero feature into Cloudflare’s ZTNA service. Today, we will cover the recently-released feature for short-lived SSH access. Secure Shell (SSH) and its security risks SSH (Secure Shell) is a protocol that is commonly used by developers or system administrators to secure the connections used to remotely administer and manage (usually Linux/Unix) servers. SSH access to a server often comes with elevated privileges, including the ability to delete or exfiltrate data or to install or remove applications on the server. Modern enterprises can have tens, hundreds, or even thousands of SSH targets. Servers accessible via SSH can be targeted in cryptojacking or proxyjacking attacks. Manually tracking, rotating, and validating SSH credentials that grant access is a chore that is often left undone, which creates risks that these long-lived credentials could be compromised. There’s nothing stopping users from copying SSH credentials and sharing them with other users or transferring them to unauthorized devices.Although many organizations will gate access to their servers to users that are inside their corporate network, this is no longer enough to protect against modern attackers. Today, the principles of Zero Trust demand that an organization also tracks who exactly is accessing their servers with SSH, and what commands they are running on those servers once they have access. In fact, the elevated privileges that come along with SSH access mean that compliance frameworks like SOC2, ISO27001, FedRAMP and others have criteria that require monitoring who has access with SSH and what exactly they are doing with that access. Introducing SSH with Access for Infrastructure We’ve introduced SSH with Access for Infrastructure to provide customers with granular control over privileged access to servers via SSH. The feature provides improved visibility into who accessed what service and what they did during their SSH session, while also eliminating the risk and overhead associated with managing SSH credentials. Specifically, this feature enables organizations to:Eliminate security risk and overhead of managing SSH keys and instead use short-lived SSH certificates issued by a Cloudflare-managed certificate authority (CA).Author fine-grained policy to govern who can SSH to your servers and through which SSH user(s) they can log in as.Monitor infrastructure access with Access and SSH command logs, supporting regulatory compliance and providing visibility in case of security breach.Avoid changing end-user workflows. SSH with Access for Infrastructure supports whatever native SSH clients end users happen to be using. SSH with Access for Infrastructure is supported through one of the most common deployment models of Cloudflare One customers. Users can connect using our device client (WARP), and targets are made accessible using Cloudflare Tunnel (cloudflared or the WARP connector). This architecture allows customers with existing Cloudflare One deployments to enable this feature with little to no effort. The only additional setup will be configuring your target server to accept a Cloudflare SSH certificate. Cloudflare One already offers multiple ways to secure organizations' SSH traffic through network controls. This new SSH with Access for Infrastructure aims to incorporate the strengths of those existing solutions together with additional controls to authorize ports, protocols, and specific users as well as a much improved deployment workflow and audit logging capabilities. Eliminating SSH credentials using an SSH CA How does Access for Infrastructure eliminate your SSH credentials? This is done by replacing SSH password and SSH keys with an SSH Certificate Authority (CA) that is managed by Cloudflare. Generally speaking, a CA’s job is to issue certificates that bind an entity to an entity’s public key. Cloudflare’s SSH CA has a secret key that is used to sign certificates that authorize access to a target (server) via SSH, and a public key that is used by the target (server) to cryptographically validate these certificates. The public key for the SSH CA can be obtained by querying the Cloudflare API. And the secret key for the SSH CA is kept secure by Cloudflare and never exposed to anyone. To use SSH with Access for Infrastructure to grant access via SSH to a set of targets (i.e. servers), you need to instruct those servers to trust the Cloudflare SSH CA. Those servers will then grant access via SSH whenever they are presented with an SSH certificate that is validly signed by the Cloudflare SSH CA.The same Cloudflare SSH CA is used to support SSH access for all of your developers and engineers to all your target servers. This greatly simplifies key management. You no longer need to manage long-lived SSH keys and passwords for individual end users, because access to targets with SSH is granted via certificates that are dynamically issued by the Cloudflare SSH CA. And, because the Cloudflare SSH CA issued short-lived SSH certificates that expire after 3 minutes, you also don’t have to worry about creating or managing long-lived SSH credentials that could be stolen by attackers. The 3-minute time window on the SSH certificate only applies to the time window during which the user has to authenticate to the target server; it does not apply to the length of the SSH session, which can be arbitrarily longer than 3 minutes. This 3-minute window was chosen because it was short enough to reduce the risk of security compromise and long enough to ensure that we don’t miss the time window of the user’s authentication to the server, especially if the user is on a slow connection. Centrally managing policies down to the specific Linux user One of the problems with traditional SSH is that once a user has an SSH key or password installed on a server, they will have access to that server forever — unless an administrator somehow remembers to remove their SSH key or password from the server in question. This leads to privilege creep, where too many people have standing access to too many servers, creating a security risk if an SSH key or password is ever stolen or leaked.Instead, SSH with Access for Infrastructure allows you to centrally write policies in the Cloudflare dashboard specifying exactly what (set of) users has access to what (set of) servers. Users may be authenticated by SSO, MFA, device posture, location, and more, which provides better security than just authenticating them via long-lived SSH keys or passwords that could be stolen by attackers.Moreover, the SSH certificates issued by the Cloudflare CA include a field called ValidPrinciples which indicates the specific Linux user (e.g. root, read-only, ubuntu, ec2-user) that can be assumed by the SSH connection. As such, you can write policies that specify the (set of) Linux users that a given (set of) end users may access on a given (set of) servers, as shown in the figure below. This allows you to centrally control the privileges that a given end user has when accessing a given target server. (The one caveat here is that the server must also be pre-configured to already know about the specific Linux user (e.g. root) that is specified in the policies and presented in the SSH certificate. Cloudflare is NOT managing the Linux users on your Linux servers.)As shown below, you could write a policy that says users in Canada, the UK, and Australia that are authenticated with MFA and face recognition can access the root and ec2-user Linux users on a given set of servers in AWS. How does Cloudflare capture SSH command logs? Cloudflare captures SSH command logs because we built an SSH proxy that intercepts the SSH connections. The SSH proxy establishes one SSH connection between itself and the end user’s SSH client, and another SSH connection between itself and the target (server). The SSH proxy can therefore inspect the SSH commands and log them. SSH commands are encrypted at rest using a public key that the customer uploads via the Cloudflare API. Cloudflare cannot read SSH command logs at rest, but they can be extracted (in encrypted form) from the Cloudflare API and decrypted by the customer (who holds the corresponding private key). Instructions for uploading the encryption public key are available in our developer documentation. How the SSH interception works under the hood How does generic SSH work? To understand how Cloudflare’s SSH proxy works, we first must review how a generic SSH connection is established.First off, SSH runs over TCP, so to establish an SSH connection, we first need to complete a TCP handshake. Then, once the TCP handshake is complete, an SSH key exchange is needed to establish an ephemeral symmetric key between the client and the server that will be used to encrypt and authenticate their SSH traffic. The SSH key exchange is based on the server public key, also known as the hostkey. If you’ve ever used SSH, you’ve probably seen this message — that is the SSH server telling your SSH client to trust this hostkey for all future SSH interactions. (This is also known as TOFU or Trust On First Use.) Finally, the client needs to authenticate itself to the server. This can be done using SSH passwords, SSH keys, or SSH certificates (as described above). SSH also has a mode called none, which means that the client does NOT need to authenticate itself to the server at all. So how does Cloudflare’s SSH proxy work? To understand this, we note that whenever you set up SSH with Access for Infrastructure in the Cloudflare dashboard, you first need to create the set of targets (i.e. servers) that you want to make accessible via SSH. Targets can be defined by IP address or hostname. You then create an Access for Infrastructure application that captures the TCP ports (e.g. port 22) that SSH runs over for those targets, and write policies for those SSH connections, as we already described above and in our developer documentation.This setup allows Cloudflare to know the set of IP addresses and ports for which it must intercept SSH traffic. Thus, whenever Cloudflare sees a TCP handshake with an IP address and port that must be intercepted, it sends traffic for that TCP connection to the SSH proxy. The SSH proxy leverages the client’s already authenticated identity from the WARP client, and enforces the configured Access for Infrastructure policies against it. If the policies do not allow the identity to connect to the target under the requested Linux user (e.g. root), the SSH proxy will reject the connection and log an Access denied event to the Access logs. Otherwise, if policies do allow the identity to connect, the the SSH proxy will establish the following two SSH connections: SSH connection from SSH proxy to targetSSH connection from end user’s SSH client (via Cloudflare’s WARP client) to SSH proxyLet’s take a look at each of these SSH connections, and the cryptographic material used to set them up. To establish the SSH connection from SSH proxy to the target, the SSH proxy acts as a client in the SSH key exchange between itself and the target server. The handshake uses the target server’s hostkey to establish an ephemeral symmetric key between the client and the server that will encrypt and authenticate their SSH traffic. Next, the SSH proxy must authenticate itself to the target server. This is done by presenting the server with a short-lived SSH certificate, issued by the Cloudflare SSH CA, for the specified Linux user that is requested for this connection as we already described above. Because the target server has been configured to trust the Cloudflare SSH CA, the target server will be able to successfully validate the certificate and the SSH connection will be established.To establish the SSH connection from the end-user's SSH client to SSH proxy, the SSH proxy acts as a server in the SSH key exchange between itself and the end-user’s SSH client. To do this, the SSH proxy needs to inform the end user’s SSH client about the hostkey that will be used to establish this connection. But what hostkey should be used? We cannot use the same hostkey used by the target server, because that hostkey is the public key that corresponds to a private key that is known only to the target server, and not known to the SSH proxy. So, Cloudflare’s SSH proxy needs to generate its own hostkey. We don’t want the end user to randomly see warnings like the one shown below, so the SSH proxy should provide the same hostkey each time the user wants to access a given target server. But, if something does change with the hostkey of the target server, we do want the warning below to be shown. To achieve the desired behavior, the SSH proxy generates a hostkey and its corresponding private key by hashing together (a) a fixed secret value valid that associated with the customer account, along with (b) the hostkey that was provided by this target server (in the connection from SSH proxy to target server). This part of the design ensures that the end user only needs to see the TOFU notification the very first time it connects to the target server via WARP, because the same hostkey is used for all future connections to that target. And, if the hostkey of the target server does change as a result of a Monster-In-The-Middle attack, the warning above will be shown to the user.Finally, during the SSH key exchange handshake from WARP client to SSH proxy, the SSH proxy informs that end user’s native SSH client that it is using none for client authentication. This means that the SSH client does NOT need to authenticate itself to the server at all. This part of the design ensures that the user need not enter any SSH passwords or store any SSH keys in its SSH configuration in order to connect to the target server via WARP. Also, this does not compromise security, because the SSH proxy has already authenticated the end user via Cloudflare’s WARP client and thus does not need to use the native SSH client authentication in the native SSH client.Put this all together, and we have accomplished our goal of having end users authenticate to target servers without any SSH keys or passwords, using Cloudflare’s SSH CA instead. Moreover, we also preserve the desired behaviors of the TOFU notifications and warnings built into native SSH clients! All the keys Before we wrap up, let’s review the cryptographic keys you need in order to deploy SSH with Access for Infrastructure. There are two keys:Public key of the SSH CA. The private key of the SSH CA is only known to Cloudflare and not shared with anyone. The public key of the SSH CA is obtained from the Cloudflare API and must be installed on all your target servers. The same public key is used for all of your targets. This public key does not need to be kept secret.Private key for SSH command log encryption. To obtain logs of SSH commands, you need to generate a public-private key pair, and upload the public key to Cloudflare. The public key will be used to encrypt your SSH commands logs at REST. You need to keep the private key secret, and you can use it to decrypt your SSH command logs. That’s it! No other keys, passwords, or credentials to manage! Try it out today At Cloudflare, we are committed to providing the most comprehensive solution for ZTNA, which now also includes privileged access to sensitive infrastructure like servers accessed over SSH.Organizations can now treat SSH like any other Access application and enforce strong MFA, device context, and policy-based access prior to granting user access. This allows organizations to consolidate their infrastructure access policies into their broader SSE or SASE architecture.You can try out Access for Infrastructure today by following these instructions in our developer documentation. Access for Infrastructure is currently available free to teams of under 50 users, and at no extra cost to existing pay-as-you-go and Contract plan customers through an Access or Zero Trust subscription. Expect to hear about a lot more features from us as we continue to natively rebuild BastionZero’s technology into Cloudflare’s Access for Infrastructure service! Fearless SSH: short-lived certificates bring Zero Trust to infrastructurehttps://blog.cloudflare.com/intro-access-for-infrastructure-ssh","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"Training a Million Models per Day to Save Customers of All Sizes From DDoS Attacks","path":"/RSSBOX/rss/53d87c14.html","content":"Our always-on DDoS protection runs inside every server across our global network. It constantly analyzes incoming traffic, looking for signals associated with previously identified DDoS attacks. We dynamically create fingerprints to flag malicious traffic, which is dropped when detected in high enough volume — so it never reaches its destination — keeping customer websites online.In many cases, flagging bad traffic can be straightforward. For example, if we see too many requests to a destination with the same protocol violation, we can be fairly sure this is an automated script, rather than a surge of requests from a legitimate web browser.Our DDoS systems are great at detecting attacks, but there’s a minor catch. Much like the human immune system, they are great at spotting attacks similar to things they have seen before. But for new and novel threats, they need a little help knowing what to look for, which is an expensive and time-consuming human endeavor.Cloudflare protects millions of Internet properties, and we serve over 60 million HTTP requests per second on average, so trying to find unmitigated attacks in such a huge volume of traffic is a daunting task. In order to protect the smallest of companies, we need a way to find unmitigated attacks that may only be a few thousand requests per second, as even these can be enough to take smaller sites offline.To better protect our customers, we also have a system to automatically identify unmitigated, or partially mitigated DDoS attacks, so we can better shore up our defenses against emerging threats. In this post we will introduce this anomaly detection pipeline, we’ll provide an overview of how it builds statistical models which flag unusual traffic and keep our customers safe. Let’s jump in! A naive volumetric model A DDoS attack, by definition, is characterized by a higher than normal volume of traffic destined for a particular destination. We can use this fact to loosely sketch out a potential approach. Let’s look at an example website, and look at the request volume over the course of a day, broken down into 1 minute intervals. We can plot this same data as a histogram: The data follows a bell-shaped curve, also known as a normal distribution. We can use this fact to flag observations which appear outside the usual range. By first calculating the mean and standard deviation of our dataset, we can then use these values to rate new observations by calculating how many standard deviations (or sigma) the data is from the mean.This value is also called the z-score — a z-score of 3 is the same as 3-sigma, which corresponds to 3 standard deviations from the mean. A data point with a high enough z-score is sufficiently unusual that it might signal an attack. Since the mean and standard deviation are stationary, we can calculate a request volume threshold for each z-score value, and use traffic volumes above these thresholds to signal an ongoing attack. Trigger thresholds for z-score of 3, 4 and 5Unfortunately, it’s incredibly rare to see traffic that is this uniform in practice, as user load will naturally vary over a day. Here I’ve simulated some traffic for a website which runs a meal delivery service, and as you might expect it has big peaks around meal times, and low traffic overnight since it only operates in a single country. Our volume data no longer follows a normal distribution and our 3-sigma threshold is now much further away, so smaller attacks could pass undetected.Many websites elastically scale their underlying hardware based upon anticipated load to save on costs. In the example above the website operator would run far fewer servers overnight, when the anticipated load is low, to save on running costs. This makes the website more vulnerable to attacks during off-peak hours as there would be less hardware to absorb them. An attack as low as a few hundred requests per minute may be enough to overwhelm the site early in the morning, even though the peak-time infrastructure could easily absorb this volume.This approach relies on traffic volume being stable over time, meaning it’s roughly flat throughout the day, but this is rarely true in practice. Even when it is true, benign increases in traffic are common, such as an e-commerce site running a Black Friday sale. In this situation, a website would expect a surge in traffic that our model wouldn’t anticipate, and we may incorrectly flag real shoppers as attackers.It turns out this approach makes too many naive assumptions about what traffic should look like, so it’s impossible to choose an appropriate sigma threshold which works well for all customers. Time series forecasting Let’s continue with trying to determine a volumetric baseline for our meal delivery example. A reasonable assumption we could add is that yesterday’s traffic shape should approximate the expected shape of traffic today. This idea is called “seasonality”. Weekly seasonality is also pretty common, i.e. websites see more or less traffic on certain weekdays or on weekends.There are many methods designed to analyze a dataset, unpick the varying horizons of seasonality within it, and then build an appropriate predictive model. We won’t go into them here but reading about Seasonal ARIMA (SARIMA) is a good place to start if you are looking for further information.There are three main challenges that make SARIMA methods unsuitable for our purposes. First is that in order to get a good idea of seasonality, you need a lot of data. To predict weekly seasonality, you need at least a few weeks worth of data. We’d require a massive dataset to predict monthly, or even annual, patterns (such as Black Friday). This means new customers wouldn’t be protected until they’d been with us for multiple years, so this isn’t a particularly practical approach.The second issue is the cost of training models. In order to maintain good accuracy, time series models need to be frequently retrained. The exact frequency varies between methods, but in the worst cases, a model is only good for 2–3 inferences, meaning we’d need to retrain all our models every 10–20 minutes. This is feasible, but it’s incredibly wasteful.The third hurdle is the hardest to work around, and is the reason why a purely volumetric model doesn’t work. Most websites experience completely benign spikes in traffic that lie outside prior norms. Flash sales are one such example, or 1,000,000 visitors driven to a site from Reddit, or a Super Bowl commercial. A better way? So if volumetric modeling won’t work, what can we do instead? Fortunately, volume isn’t the only axis we can use to measure traffic. Consider the end users’ browsers for example. It would be reasonable to assume that over a given time interval, the proportion of users across the top 5 browsers would remain reasonably stationary, or at least within a predictable range. More importantly, this proportion is unlikely to change too much during benign traffic surges.Through careful analysis we were able to discover about a dozen such variables with the following features for a given zone: They follow a normal distributionThey aren’t correlated, or are only loosely correlated with volumeThey deviate from the underlying normal distribution during “under attack” eventsRecall our initial volume model, where we used z-score to define a cutoff. We can expand this same idea to multiple dimensions. We have a dozen different time series (each feature is a single time series), which we can imagine as a cloud of points in 12 dimensions. Here is a sample showing 3 such features, with each point representing the traffic readings at a different point in time. Note that both graphs show the same cloud of points from two different angles. To use our z-score analogy from before, we’d want our points to be spherical, since our multidimensional- z-score is then just the distance from the centre of the cloud. We could then use this distance to define a cutoff threshold for attacks.For several reasons, a perfect sphere is unlikely in practice. Our various features measure different things, so they have very different scales of ‘normal’. One property might vary between 100-300 whereas another property might usually occupy the interval 0-1. A change of 3 in this latter property would be a significant anomaly, whereas in the first this would just be within the normal range.More subtly, two or more axes may be correlated, so an increase in one is usually mirrored with a proportional increase/decrease in another dimension. This turns our sphere into an off-axis disc shape, as pictured above.Fortunately, we have a couple of mathematical tricks up our sleeve. The first is scale normalization. In each of our n dimensions, we subtract the mean, and divide by the standard deviation. This makes all our dimensions the same size and centres them around zero. This gives a multidimensional analogue of z-score, but it won’t fix the disc shape.What we can do is figure out the orientation and dimensions of the disc, and for this we use a tool called Principal Component Analysis (PCA). This lets us reorient our disc, and rescale the axes according to their size, to make them all the same.Imagine grabbing the disc out of the air, then drawing new X and Y axes on the top surface, with the origin at the center of the disc. Our new Z-axis is the thickness of the disc. We can compare the thickness to the diameter of the disc, to give us a scaling factor for the Z direction. Imagine stretching the disc along the z-axis until it’s as tall as the length across the diameter.In reality there’s nothing to say that X &amp; Y have to be the same size either, but hopefully you get the general idea. PCA lets us draw new axes along these lines of correlation in an arbitrary number of dimensions, and convert our n-dimensional disc into a nicely behaved sphere of points (technically an n-dimensional sphere).Having done all this work, we can uniquely define a coordinate transformation which takes any measurement from our raw features, and tells us where it should lie in the sphere, and since all our dimensions are the same size we can generate an anomaly score purely based on its distance from the centre of the cloud.As a final trick, we can also use a final scaling operation to ensure the sphere for dataset A is the same size as the sphere generated from dataset B, meaning we can do this same process for any traffic data and define a cutoff distance λ which is the same across all our models. Rather than fine-tuning models for each individual customer zone, we can tune this to a value which applies globally.Another name for this measurement is Mahalanobis distance. (Inclined readers can understand this equivalence by considering the role of the covariance matrix in PCA and Mahalanobis distance. Further discussion can be found on this StackExchange post.) We further tune the process to discard dimensions with little variance — if our disc is too thin we discard the thickness dimension. In practice, such dimensions were too sensitive to be useful. We’re left with a multidimensional analogue of the z-score we started with, but this time our variables aren’t correlated with peacetime traffic volume. Above we show 2 output dimensions, with coloured circles which show Mahalanobis distances of 4, 5 and 6. Anything outside the green circle will be classified as an attack. How we train ~1 million models daily to keep customers safe The approach we’ve outlined is incredibly parallelizable: a single model requires only the traffic data for that one website, and the datasets needed can be quite small. We use 4 weeks of training data chunked into 5 minute intervals which is only ~8k rows/website.We run all our training and inference in an Apache Airflow deployment in Kubernetes. Due to the parallelizability, we can scale horizontally as needed. On average, we can train about 3 models/second/thread. We currently retrain models every day, but we’ve observed very little intraday model drift (i.e. yesterday’s model is the same as today’s), so training frequency may be reduced in the future.We don’t consider it necessary to build models for all our customers, instead we train models for a large sample of representative customers, including a large number on the Free plan. The goal is to identify attacks for further study which we then use to tune our existing DDoS systems for all customers. Join us! If you’ve read this far you may have questions, like “how do you filter attacks from your training data?” or you may have spotted a handful of other technical details which I’ve elided to keep this post accessible to a general audience. If so, you would fit in well here at Cloudflare. We’re helping to build a better Internet, and we’re hiring. Training a million models per day to save customers of all sizes from DDoS attackshttps://blog.cloudflare.com/training-a-million-models-per-day-to-save-customers-of-all-sizes-from-ddos","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"1.21.3 正式版更新","path":"/RSSBOX/rss/c64fd829.html","content":"1.21.3 正式版更新 1.21.3 正式版更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"Cybersecurity Spotlight on Bug Bounty Researcher @Adrianoapj","path":"/RSSBOX/rss/11f80b10.html","content":"As we wrap up Cybersecurity Awareness Month, the GitHub Bug Bounty team is excited to spotlight one of the top performing security researchers who participates in the GitHub Security Bug Bounty Program—@adrianoapj! And don’t miss our previous post highlighting @imrerad. As home to over 100 million developers and 420 million repositories, GitHub maintains a strong dedication to ensuring the security and reliability of the code that powers daily development activities. The GitHub Bug Bounty Program continues to play a pivotal role in advancing the security of the software ecosystem, empowering developers to create and build confidently on our platform and with our products. We firmly believe that the foundation of a successful bug bounty program is built on collaboration with skilled security researchers. As we continue to celebrate an amazing 10 years of the GitHub Security Bug Bounty program, we are also looking towards the future. As such, we are looking at ways to better engage with the research community to advance the security landscape. We remain excited about opportunities to meet people and give back to the security community. We love learning new things from our researchers that help us ship even more secure products, and we are always eager to make sure that our products are the best in class. As we look forward, we are truly thrilled and excited about what our future together holds. As we conclude Cybersecurity Awareness Month, we’re interviewing one of the top contributing researchers to our bug bounty program. Follow along to learn more about their methodology, techniques, and experiences hacking on GitHub. @adrianoapj specializes in information disclosures and has submitted many interesting and unique issues. How did you get involved with Bug Bounty? What has kept you coming back to it? Long before getting started with bug bounty, I was already in tech for some years. I got into Bug Bounty when I first learned about it from a Brazilian cybersecurity YouTube channel and then I started to do some capture the flag exercises (CTFs) and watched videos from Hacker101, which helped me a lot to get all the knowledge that I needed to get started. The GitHub Bug Bounty program was the first program that I started to hack on and is currently the program where I send most of my reports. Something that kept me really motivated at the start of my journey was the defense-in-depth class of bugs at GitHub’s program. In short, the first vulnerability that I reported to GitHub had a really low severity, but GitHub still decided to reward it with a bonus as a reward for the effort. Currently, what motivates me to keep hunting is the challenge of finding bugs, the visible impact of the issues that I find and report, and of course, the bounties! What do you enjoy doing when you aren’t hacking? Most of the time that I’m not searching for bugs I am actually working as a full-time Infosec analyst. But in my free time, I like going out with some friends from church and playing video games. This year I also started running, so that’s something that I like to do to relieve some stress. How do you keep up with and learn about vulnerability trends? Mostly, I read write-ups and public bug reports that have been disclosed on HackerOne. I am also a big fan of HackTheBox and HackTheBox Academy, where I go to learn new classes of bugs, challenge myself to improve techniques, and get new ideas to test on bug bounty programs. What are your favorite classes of bugs to research and why? My favorite class of bugs to research is information disclosure because they normally present a significant impact and they are easy to spot sometimes. You’ve found some complex and significant bugs in your work. Can you talk a bit about your process? I usually start by choosing the feature or website that I’m going to test. GitHub Stars was one of the first GitHub websites I ever tested. I’d learned about it from a post on X and decided it would be a good target since it was a new project and there likely wouldn’t be a lot of people searching it for bugs. I was right as I found a lot of great vulnerabilities there. I also like to look at the GitHub Changelog so I can test new features or changes, which are a great place to start for finding bugs, in my experience. For my process, I rarely use automated tools. Instead, I start learning everything I can about the specific feature or project that I am searching for bugs. After getting a deep understanding about it, I write some possibilities and/or assumptions about what entry points for vulnerabilities could be. Then, I start to test these possibilities, and I’m either able to find bugs or I iterate and think about new tests, until I find something. Do you have any advice or recommended resources for researchers looking to get involved with Bug Bounty? Yes! I would say that perseverance is really necessary for researchers, especially when you are starting to get into Bug Bounty. Sometimes, researching for bugs can be frustrating, especially when you are dealing with duplicate or informative reports, or end up going down rabbit holes. But it’s important to know that every service is subject to bugs, and when you keep searching for them, sometimes you find something! I have some recommendations for learning the necessary knowledge for Bug Bounty: Hacker101 (for people that are starting to learn about cybersecurity and Bug Bounty). HackTheBox and HackTheBox Academy (to everyone looking to improve skills and test knowledge on offensive cybersecurity). Write-ups in general, including blog or X posts, and public reports available on HackerOne. Do you have any social media platforms you’d like to share with our readers? My GitHub profile is adrianoapj and my LinkedIn profile is /in/adrianoapj Thank you, @adrianoapj, for participating in GitHub’s bug bounty researcher spotlight! Each submission to our bug bounty program is a chance to make GitHub, our products, and our customers more secure, and we continue to welcome and appreciate collaboration with the security research community. So, if this inspired you to go hunting for bugs, feel free to report your findings through HackerOne. Interested in helping us secure GitHub products and services? Check out our open roles! The post Cybersecurity spotlight on bug bounty researcher @adrianoapj appeared first on The GitHub Blog. Cybersecurity spotlight on bug bounty researcher @adrianoapjhttps://github.blog/security/vulnerability-research/cybersecurity-spotlight-on-bug-bounty-researcher-adrianoapj/","tags":["Blogs","Github"],"categories":["Blogs","Github"]},{"title":"Diversity, Inclusion, and Belonging at GitHub in 2024","path":"/RSSBOX/rss/b5fc2f2b.html","content":"At GitHub, diversity isn’t just a metric we track—it’s the fuel that powers our innovation. With a goal to be the home of one billion developers, we remain committed to creating an inclusive environment where every voice is heard and every background is valued. In 2024, our Diversity, Inclusion, and Belonging (DI&amp;B) strategy remained robust, as we supported the diversity of both our employees and the greater developer community. This blog highlights our ongoing efforts and progress this past year, as we strive to build a more equitable company and tech industry. Together, we can create a future where everyone has the opportunity to contribute, collaborate, and thrive. Talent, tools, and transformation Over the past year, we’ve maintained our commitment to a diverse workforce. In the U.S., there was a steady increase in our U.S. Asian population of +1.0 percentage points. We also saw an increase of +0.2 percentage points in our U.S. Hispanic and Latinx populations. Globally, we increased the number of women represented from the previous year by +1.4 percentage points. We also maintained our focus on empowering a pipeline of employees to grow professionally and personally. We built out the GitHub Early in Profession program for individuals who have less than three years of professional experience. Our GitHub Intern Program, designed to recruit university talent for potential full-time roles after graduation, grew by 184% from last fiscal year. Additionally, 38% of our U.S. intern cohort were women, and 41% were Hispanic/Latino or African American/Black. These programs foster an inclusive workforce, supporting talent as the company scales. Community, code, and culture Here at GitHub, we know it’s not just about us—we have a responsibility to help the greater community access the goodness of open source and tech. To that end, we delivered impactful programs, collaborated with key partners, and supported thousands of individuals worldwide. In 2024, we skilled more than 1,700 learners through the Social Impact team’s depth-driven programming of open source curriculum, hackathons, virtual leadership and mentorship opportunities, and more. In addition, together with Major League Hacking, we distributed $20,000 in grants to fund over 30 events, supporting 2,500 learners to develop hands-on experience as a part of career pathing in open source. We also launched All In Africa. This initiative is dedicated to making open source education accessible to everyone across the continent. It also supports a future where open source projects and global tech companies can tap into an expanding skilled workforce and meet the demands of a rapidly evolving tech landscape while furthering the prosperity of African economies. At GitHub, we have nine Communities of Belonging (CoB) that help nurture our culture. These communities support a positive employee experience by generating energy within their respective groups and across the company. They create safe spaces for employees to connect and be supported personally and professionally, while also providing opportunities for members to lead diversity initiatives that align with the business’ innovation and corporate social responsibility goals. In all, our CoBs cultivate a culture of inclusion, connection, and shared growth—ensuring everyone at GitHub can thrive. Voices, vision, and values AI is changing the world of software, and we’re committed to ensuring that developers can benefit from it without contributing to societal harms. As such, 2024 has been a defining year for supporting responsible AI programs. We joined the AI Elections Accord, a cross-industry initiative to combat the deceptive use of AI in elections (see our progress update for more). GitHub co-hosted a workshop on responsible practices for open source AI with the Partnership on AI, which led to a report on risk mitigation strategies for the open foundation model value chain. We have also continued our broad support of research and data that advances the understanding of developers’ contributions to innovation, development, and societal resilience throughout the world. Following the launch of the GitHub Innovation Graph, we now have four full years of data available for anyone to analyze and explore, and have made clarifying updates in response to feedback. In our most recent data release, we featured a conversation with economic researchers who are using GitHub Innovation Graph data to estimate the impact of generative AI tools on software development activity. Innovation, inclusion, and impact As the world’s home for open source and the greater developer community, we ensure that everyone can use their ingenuity and creativity to build great things—including those with disabilities. In 2024, we continued to invest in accessibility as one of our engineering fundamentals, which serves as the foundation for accessibility governance at GitHub. Our new Accessibility Design Bootcamp, completed by over 50% of our design team so far, provided exercises and discussions to raise awareness of web accessibility best practices and to empower designers to create more accessible products. In addition, we launched company-wide accessibility training and continued to improve on our accessible interview process globally. Finally, we continued to amplify the voices of disabled developers by publishing another four installments of the Coding Accessibility series and the accessibility playlist on our YouTube channel. Continuing our journey together Looking ahead, our priority is to further enrich the employee experience at GitHub, as well as help create a more inclusive and diverse industry. We’ll continue to provide learning, education, networking, and social impact opportunities to our employees. Also on our roadmap, we’ll partner with open source programs worldwide, nurture our CoBs, lead the change in leveraging AI ethically, support research that advances our understanding of the greater developer community, and create more accessible software. We’re excited to reach even more developers and the innovation opportunities that will inevitably arise. Together, let’s build what’s next. To learn more about how GitHub is advancing our DI&amp;B strategy read the full report &gt; The post Diversity, inclusion, and belonging at GitHub in 2024 appeared first on The GitHub Blog. Diversity, inclusion, and belonging at GitHub in 2024https://github.blog/news-insights/company-news/diversity-inclusion-and-belonging-at-github-in-2024/","tags":["Blogs","Github"],"categories":["Blogs","Github"]},{"title":"Bundles of Bravery - Official Trailer","path":"/RSSBOX/rss/66945abb.html","content":"Get ready to muster your courage and remaster your inventory, because bundles and Hardcore mode are finally here! Tame your inventory with dyeable, space-saving bundles, or embrace the chaos with Hardcore mode – where death is permanent, and the game is locked to the highest difficulty setting. Play the Bundles of Bravery drop today! Bundles of Bravery - Official Trailerhttps://www.youtube.com/watch?v=Fyf9gibpX1c","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"Is This Thing On? Using OpenBMC and ACPI Power States for Reliable Server Boot","path":"/RSSBOX/rss/be31291e.html","content":"Introduction At Cloudflare, we provide a range of services through our global network of servers, located in 330 cities worldwide. When you interact with our long-standing application services, or newer services like Workers AI, you’re in contact with one of our fleet of thousands of servers which support those services.These servers which provide Cloudflare services are managed by a Baseboard Management Controller (BMC). The BMC is a special purpose processor — different from the Central Processing Unit (CPU) of a server — whose sole purpose is ensuring a smooth operation of the server.Regardless of the server vendor, each server has this BMC. The BMC runs independently of the CPU and has its own embedded operating system, usually referred to as firmware. At Cloudflare, we customize and deploy a server-specific version of the BMC firmware. The BMC firmware we deploy at Cloudflare is based on the Linux Foundation Project for BMCs, OpenBMC. OpenBMC is an open-sourced firmware stack designed to work across a variety of systems including enterprise, telco, and cloud-scale data centers. The open-source nature of OpenBMC gives us greater flexibility and ownership of this critical server subsystem, instead of the closed nature of proprietary firmware. This gives us transparency (which is important to us as a security company) and allows us faster time to develop custom features/fixes for the BMC firmware that we run on our entire fleet.In this blog post, we are going to describe how we customized and extended the OpenBMC firmware to better monitor our servers’ boot-up processes to start more reliably and allow better diagnostics in the event that an issue happens during server boot-up. Server subsystems Server systems consist of multiple complex subsystems that include the processors, memory, storage, networking, power supply, cooling, etc. When booting up the host of a server system, the power state of each subsystem of the server is changed in an asynchronous manner. This is done so that subsystems can initialize simultaneously, thereby improving the efficiency of the boot process. Though started asynchronously, these subsystems may interact with each other at different points of the boot sequence and rely on handshake/synchronization to exchange information. For example, during boot-up, the UEFI (Universal Extensible Firmware Interface), often referred to as the BIOS, configures the motherboard in a phase known as the Platform Initialization (PI) phase, during which the UEFI collects information from subsystems such as the CPUs, memory, etc. to initialize the motherboard with the right settings. Figure 1: Server Boot ProcessWhen the power state of the subsystems, handshakes, and synchronization are not properly managed, there may be race conditions that would result in failures during the boot process of the host. Cloudflare experienced some of these boot-related failures while rolling out open source firmware (OpenBMC) to the Baseboard Management Controllers (BMCs) of our servers. Baseboard Management Controller (BMC) as a manager of the host A BMC is a specialized microprocessor that is attached to the board of a host (server) to assist with remote management capabilities of the host. Servers usually sit in data centers and are often far away from the administrators, and this creates a challenge to maintain them at scale. This is where a BMC comes in, as the BMC serves as the interface that gives administrators the ability to securely and remotely access the servers and carry out management functions. The BMC does this by exposing various interfaces, including Intelligent Platform Management Interface (IPMI) and Redfish, for distributed management. In addition, the BMC receives data from various sensors/devices (e.g. temperature, power supply) connected to the server, and also the operating parameters of the server, such as the operating system state, and publishes the values on its IPMI and Redfish interfaces. Figure 2: Block diagram of BMC in a server system.At Cloudflare, we use the OpenBMC project for our Baseboard Management Controller (BMC).Below are examples of management functions carried out on a server through the BMC. The interactions in the examples are done over ipmitool, a command line utility for interacting with systems that support IPMI. # Check the sensor readings of a server remotely (i.e. over a network) $ ipmitool &lt;some authentication&gt; &lt;bmc ip&gt; sdr PSU0_CURRENT_IN | 0.47 Amps | ok PSU0_CURRENT_OUT | 6 Amps | ok PSU0_FAN_0 | 6962 RPM | ok SYS_FAN | 13034 RPM | ok SYS_FAN1 | 11172 RPM | ok SYS_FAN2 | 11760 RPM | ok CPU_CORE_VR_POUT | 9.03 Watts | ok CPU_POWER | 76.95 Watts | ok CPU_SOC_VR_POUT | 12.98 Watts | ok DIMM_1_VR_POUT | 29.03 Watts | ok DIMM_2_VR_POUT | 27.97 Watts | ok CPU_CORE_MOSFET | 40 degrees C | ok CPU_TEMP | 50 degrees C | ok DIMM_MOSFET_1 | 36 degrees C | ok DIMM_MOSFET_2 | 39 degrees C | ok DIMM_TEMP_A1 | 34 degrees C | ok DIMM_TEMP_B1 | 33 degrees C | ok … check the power status of a server remotely (i.e. over a network)ipmitool &lt;some authentication&gt; &lt;bmc ip&gt; power statusChassis Power is off power on the serveripmitool &lt;some authentication&gt; &lt;bmc ip&gt; power onChassis Power Control: On Switching to OpenBMC firmware for our BMCs gives us more control over the software that powers our infrastructure. This has given us more flexibility, customizations, and an overall better uniform experience for managing our servers. Since OpenBMC is open source, we also leverage community fixes while upstreaming some of our own. Some of the advantages we have experienced with OpenBMC include a faster turnaround time to fixing issues, optimizations around thermal cooling, increased power efficiency and supporting AI inference.While developing Cloudflare’s OpenBMC firmware, however, we ran into a number of boot problems.Host not booting: When we send a request over IPMI for a host to power on (as in the example above, power on the server), ipmitool would indicate the power status of the host as ON, but we would not see any power going into the CPU nor any activity on the CPU. While ipmitool was correct about the power going into the chassis as ON, we had no information about the power state of the server from ipmitool, and we initially falsely assumed that since the chassis power was on, the rest of the server components should be ON. The System Event Log (SEL), which is responsible for displaying platform-specific events, was not giving us any useful information beyond indicating that the server was in a soft-off state (powered off), working state (operating system is loading and running), or that a “System Restart” of the host was initiated. # System Event Logs (SEL) showing the various power states of the server $ ipmitool sel elist | tail -n3 4d | Pre-Init |0000011021| System ACPI Power State ACPI_STATUS | S5_G2: soft-off | Asserted 4e | Pre-Init |0000011022| System ACPI Power State ACPI_STATUS | S0_G0: working | Asserted 4f | Pre-Init |0000011023| System Boot Initiated RESTART_CAUSE | System Restart | Asserted In the System Event Logs shown above, ACPI is the acronym for Advanced Configuration and Power Interface, a standard for power management on computing systems. In the ACPI soft-off state, the host is powered off (the motherboard is on standby power but CPU/host isn’t powered on); according to the ACPI specifications, this state is called S5_G2. (These states are discussed in more detail below.) In the ACPI working state, the host is booted and in a working state, also known in the ACPI specifications as status S0_G0 (which in our case happened to be false), and the third row indicates the cause of the restart was due to a System Restart. Most of the boot-related SEL events are sent from the UEFI to the BMC. The UEFI has been something of a black box to us, as we rely on our original equipment manufacturers (OEMs) to develop the UEFI firmware for us, and for the generation of servers with this issue, the UEFI firmware did not implement sending the boot progress of the host to the BMC.One discrepancy we observed was the difference in the power status and the power going into the CPU, which we read with a sensor we call CPU_POWER. # Check power status $ ipmitool &lt;some authentication&gt; &lt;bmc ip&gt; power status Chassis Power is on However, checking the power into the CPU shows that the CPU was not receiving any power. # Check power going into the CPU $ ipmitool &lt;some authentication&gt; &lt;bmc ip&gt; sdr | grep CPU_POWER CPU_POWER | 0 Watts | ok The CPU_POWER being at 0 watts contradicts all the previous information that the host was powered up and working, when the host was actually completely shut down.Missing Memory Modules: Our servers would randomly boot up with less memory than expected. Computers can boot up with less memory than installed due to a number of problems, such as a loose connection, hardware problem, or faulty memory. For our case, it happened not to be any of the usual suspects, but instead was due to both the BMC and UEFI trying to simultaneously read from the memory modules, leading to access contentions. Memory modules usually contain a Serial Presence Detect (SPD), which is used by the UEFI to dynamically detect the memory module. This SPD is usually located on an inter-integrated circuit (i2c), which is a low speed, two write protocol for devices to talk to each other. The BMC also reads the temperature of the memory modules via the i2c. When the server is powered on, amongst other hardware initializations, the UEFI also initializes the memory modules that it can detect via their (i.e. each individual memory modules) Serial Presence Detect (SPD), the BMC could also be trying to access the temperature of the memory module at the same time, over the same i2c protocol. This simultaneous attempted read denies one of the parties access. When the UEFI is denied access to the SPD, it thinks the memory module is not available and skips over it. Below is an example of the related i2c-bus contention logs we saw in the journal of the BMC when the host is booting. kernel: aspeed-i2c-bus 1e78a300.i2c-bus: irq handled != irq. expected 0x00000021, but was 0x00000020 The above logs indicate that the i2c address 1e78a300 (which happens to be connected to the serial presence detect of the memory modules) could not properly handle a signal, known as an interrupt request (irq). When this scenario plays out on the UEFI, the UEFI is unable to detect the memory module. Figure 3: I2C diagram showing I2C interconnection of the server’s memory modules (also known as DIMMs) with the BMC DIMM in Figure 3 refers to Dual Inline Memory Module, which is the type of memory module used in servers.Thermal telemetry: During the boot-up process of some of our servers, some temperature devices, such as the temperature sensors of the memory modules, would show up as failed, thereby causing some of the fans to enter a fail-safe Pulse Width Modulation (PWM) mode. PWM is a technique to encode information delivered to electronic devices by adjusting the frequency of the waveform signal to the device. It is used in this case to control fan speed by adjusting the frequency of the power signal delivered to the fan. When a fan enters a fail-safe mode, PWM is used to set the fan speeds to a preset value, irrespective of what the optimized PWM setting of the fans should be, and this could negatively affect the cooling of the server and power consumption. Implementing host ACPI state on OpenBMC In the process of studying the issues we faced relating to the boot-up process of the host, we learned how the power state of the subsystems within the chassis changes. Part of our learnings led us to investigate the Advanced Configuration and Power Interface (ACPI) and how the ACPI state of the host changed during the boot process.Advanced Configuration and Power Interface (ACPI) is an open industry specification for power management used in desktop, mobile, workstation, and server systems. The ACPI Specification replaces previous power management methodologies such as Advanced Power Management (APM). ACPI provides the advantages of:Allowing OS-directed power management (OSPM).Having a standardized and robust interface for power management.Sending system-level events such as when the server power/sleep buttons are pressed Hardware and software support, such as a real-time clock (RTC) to schedule the server to wake up from sleep or to reduce the functionality of the CPU based on RTC ticks when there is a loss of power.From the perspective of power management, ACPI enables an OS-driven conservation of energy by transitioning components which are not in active use to a lower power state, thereby reducing power consumption and contributing to more efficient power management.The ACPI Specification defines four global “Gx” states, six sleeping “Sx” states, and four “Dx” device power states. These states are defined as follows: Gx Name Sx Description G0 Working S0 The run state. In this state the machine is fully running G1 Sleeping S1 A sleep state where the CPU will suspend activity but retain its contexts. S2 A sleep state where memory contexts are held, but CPU contexts are lost. CPU re-initialization is done by firmware. S3 A logically deeper sleep state than S2 where CPU re-initialization is done by device. Equates to Suspend to RAM. S4 A logically deeper sleep state than S3 in which DRAM is context is not maintained and contexts are saved to disk. Can be implemented by either OS or firmware. G2 Soft off but PSU still supplies power S5 The soft off state. All activity will stop, and all contexts are lost. The Complex Programmable Logic Device (CPLD) responsible for power-up and power-down sequences of various components e.g. CPU, BMC is on standby power, but the CPU/host is off. G3 Mechanical off PSU does not supply power. The system is safe for disassembly. Dx Name Description D0 Fully powered on Hardware device is fully functional and operational D1 Hardware device is partially powered down Reduced functionality and can be quickly powered back to D0 D2 Hardware device is in a deeper lower power than D1 Much more limited functionality and can only be slowly powered back to D0. D3 Hardware device is significantly powered down or off Device is inactive with perhaps only the ability to be powered back on The states that matter to us are:S0_G0_D0: often referred to as the working state. Here we know our host system is running just fine.S2_D2: Memory contexts are held, but CPU context is lost. We usually use this state to know when the host’s UEFI is performing platform firmware initialization.S5_G2: Often referred to as the soft off state. Here we still have power going into the chassis, however, processor and DRAM context are not maintained, and the operating system power management of the host has no context.Since the issues we were experiencing were related to the power state changes of the host — when we asked the host to reboot or power on — we needed a way to track the various power state changes of the host as it went from power off to a complete working state. This would give us better management capabilities over the devices that were on the same power domain of the host during the boot process. Fortunately, the OpenBMC community already implemented an ACPI daemon, which we extended to serve our needs. We added an ACPI S2_D2 power state, in which memory contexts are held, but CPU context is lost, to the ACPI daemon running on the BMC to enable us to know when the host’s UEFI is performing firmware initialization, and also set up various management tasks for the different ACPI power states.An example of a power management task we carry out using the S0_G0_D0 state is to re-export our Voltage Regulator (VR) sensors on S0_G0_D0 state, as shown with the service file below: cat /lib/systemd/system/Re-export-VR-device.service [Unit] Description=RE Export VR Device Process Wants=xyz.openbmc_project.EntityManager.service After=xyz.openbmc_project.EntityManager.service Conflicts=host-s2-state.target [Service]Type&#x3D;simpleExecStart&#x3D;&#x2F;bin&#x2F;bash -c ‘set -a &amp;&amp; source &#x2F;usr&#x2F;bin&#x2F;Re-export-VR-device.sh on’SyslogIdentifier&#x3D;Re-export-VR-device.service [Install]WantedBy&#x3D;host-s0-state.target Having set this up, OpenBMC has a Net Function (ipmiSetACPIState) in phosphor-host-ipmid that is responsible for setting the ACPIState of the host on the BMC. This command is called by the host using the standard ipmi command with the corresponding NetFn=0x06 and Cmd=0x06.In the event of an immediate power cycle (i.e. host reboots without operating system shutdown), the host is unable to send its S5_G2 state to the BMC. For this case, we created a patch to OpenBMC’s x86-power-control to let the BMC become aware that the host has entered the ACPI S5_G2 state (i.e. soft-off). When the host comes out of the power off state, the UEFI performs the Power On Self Test (POST) and sends the S2_D2 to the BMC, and after the UEFI has loaded the OS on the host, it notifies the BMC by sending the ACPI S0_G0_D0 state. Fixing the issues Going back to the boot-up issues we faced, we discovered that they were mostly caused by devices which were in the same power domain of the CPU, interfering with the UEFI/platform firmware initialization phase. Below is a high level description of the fixes we applied.Servers not booting: After identifying the devices that were interfering with the POST stage of the firmware initialization, we used the host ACPI state to control when we set the appropriate power mode state for those devices so as not to cause POST to fail.Memory modules missing: During the boot-up process, memory modules (DIMMs) are powered and initialized in S2_D2 ACPI state. During this initialization process, UEFI firmware sends read commands to the Serial Presence Detect (SPD) on the DIMM to retrieve information for DIMM enumeration. At the same time, the BMC could be sending commands to read DIMM temperature sensors. This can cause SMBUS collisions, which could either cause DIMM temperature reading to fail or UEFI DIMM enumeration to fail. The latter case would cause the system to boot up with reduced DIMM capacity, which could be mistaken as a failing DIMM scenario. After we had discovered the race condition issue, we disabled the BMC from reading the DIMM temperature sensors during S2_D2 ACPI state and set a fixed speed for the corresponding fans. This solution allows our UEFI to retrieve all the necessary DIMM subsystems information for enumeration, and our servers now boot up with the correct size of memory.Thermal telemetry: In S0_G0 power state, when sensors are not reporting values back to the BMC, the BMC assumes that devices may be overheating and puts the fan controller into fail-safe mode where fan speeds are ramped up to maximum speed. However, in S5_G2 state, some thermal sensors like CPU temperature, NIC temperature, etc. are not powered and not available. Our solution is to set these thermal sensors as non-functional in their exported configuration when in S5_G2 state and during the transition from S5_G2 state to S2_D2 state. Setting the affected devices as non-functional in their configuration, instead of waiting for thermal sensor read commands to error out, prevents the controller from entering the fail-safe mode. Moving forward Aside from resolving issues, we have seen other benefits from implementing ACPI Power State on our BMC firmware. An example is in the area of our automated firmware regression testing. Various parts of our tests require rebooting/power cycling the servers over a hundred times, during which we monitor the ACPI power state changes of our servers as against using a boolean (running or not running, pingable or not pingable) to assert the status of our servers.Also, it has given us the opportunity to learn more about the complex subsystems in a server system, and the various power modes of the different subsystems. This is an aspect that we are still actively learning about as we look to further optimize various aspects of the boot sequence of our servers.In the course of time, implementing ACPI states is helping us achieve the following:All components are enabled by end of boot sequence,BIOS and BMC are able to retrieve component information,And the BMC is aware when thermal sensors are in a non-functional state. For better observability of the boot progress and “last state” of our systems, we have also started the process of adding the BootProgress object of the Redfish ComputerSystem Schema into our systems. This will give us an opportunity for pre-operating system (OS) boot observability and an easier debug starting point when the UEFI has issues (such as when the server isn’t coming on) during the server platform initialization.With each passing day, Cloudflare’s OpenBMC team, which is made up of folks from different embedded backgrounds, learns about, experiments with, and deploys OpenBMC across our global fleet. This has been made possible by relying on the OpenBMC community’s contribution (as well as upstreaming some of our own contributions), and our interaction with our various vendors, thereby giving us the opportunity to make our systems more reliable, and giving us the ownership and responsibility of the firmware that powers the BMCs that manage our servers. If you are thinking of embracing open-source firmware in your BMC, we hope this blog post written by a team which started deploying OpenBMC less than 18 months ago has inspired you to give it a try. For those who are interested in considering making the jump to open-source firmware, check it out here! Is this thing on? Using OpenBMC and ACPI power states for reliable server boothttps://blog.cloudflare.com/how-we-use-openbmc-and-acpi-power-states-to-monitor-the-state-of-our-servers","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"1.21.2 正式版更新","path":"/RSSBOX/rss/41e9136a.html","content":"1.21.2 正式版更新 1.21.2 正式版更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"1.21.2-Rc2 快照更新","path":"/RSSBOX/rss/2cc6ab20.html","content":"1.21.2-rc2 快照更新 1.21.2-rc2 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"Securing the Open Source Supply Chain: The Essential Role of CVEs","path":"/RSSBOX/rss/51a1944f.html","content":"As security continues to shift left, developers are increasing as the first line of defense against vulnerabilities. In fact, open source developers now spend nearly 3x more time on security compared with a few years ago—which is an incredible development, considering how much the world relies on open source software. I’m Madison Oliver and I manage the team that curates the vulnerability data here within the GitHub Security Lab, where developers and security professionals come together to secure the open source software (OSS) we all rely on. At GitHub, we’re dedicated to supporting open source—including securing it. Our researchers are constantly keeping the OSS ecosystem aware of vulnerabilities by discovering and disclosing new vulnerabilities, educating the community with research, performing variant analysis for OSS projects, and curating the GitHub Advisory Database, our own vulnerability reporting database dedicated to open source. We also regularly assign and publish vulnerabilities to the broader vulnerability management ecosystem on behalf of OSS maintainers. Much of this is done through Common Vulnerabilities and Exposures (CVEs). Whether you’re contributing to open source software, maintaining a project, or just relying on open source like most developers, CVEs help keep that software secure. CVEs and the broader vulnerability landscape have grown and changed drastically in recent years, but we’ve kept pace by empowering the open source community to improve their software security through policies, products, open source solutions, and security automation tools. Let’s jump in. What is a CVE? A CVE is a unique identifier for a vulnerability published in the CVE List, a catalog of vulnerability records. MITRE, a non-profit spun out of the MIT Lincoln Laboratory, maintains the catalog and the CVE Program with the goal of identifying, defining, and cataloging publicly-disclosed cybersecurity vulnerabilities. We actively contribute back to the CVE Program through serving on working groups and the CVE Program board, providing feedback and helping ensure that this critical security program is aligned with open source developers’ needs. At GitHub, we have a unique perspective on CVEs: we not only use CVE data for our internal vulnerability management and for our Advisory Database, which feeds into Dependabot, code scanning, npm audit, and more, but also create it. Since 2019, we have managed two CVE Numbering Authorities (CNAs)—special entities allowed to assign and publish CVE IDs—where we produce CVE data, much of it provided by OSS maintainers. In 2019, we published 29 CVEs (CVE-2019-16760 was the first) and last year, we published more than 1,700. We’re not the only ones seeing an increase in vulnerability data. In fact, it’s one of the bigger industry-wide changes of the past decade, and it has implications for how we keep software secure. Despite some misconceptions that we’ve covered before, the CVE list is a critical source of vulnerability information across the industry, and commonly serves as the foundation for automated vulnerability management tools. We focus on the CVE list throughout this blog, as it’s a catalog of vulnerabilities that can be considered the absolute minimum of publicly known security vulnerabilities. We recognize vulnerabilities have been publicly disclosed on the internet in places like the formerly long-running Bugtraq mailing list since before the CVE system was established. But the CVE List offers a much more comprehensive data set to demonstrate the sheer increase in the volume of data over time. The nearly 80,000 messages on Bugtraq between 1993 and 2021 don’t even come close to the 240,000+ CVE records published since 1999. Besides, not all Bugtraq messages contain unique vulnerabilities. The double-edged sword of increased vulnerability data When the CVE Program originated in 1999, it published 321 CVE records. Last year, it published more than 28,900, increasing 460% in the past decade—and the amount is expected to continue growing. This growth means downstream consumers of vulnerability data—like yourself and our Advisory Database curation team—have more and more vulnerability data to sift through each day, which can lead to information overload. It also means increased vulnerability transparency (that is, making vulnerability information publicly available), which is fundamental to improving security across the industry. After all, you can’t address a vulnerability if you don’t even know about it. So, while this increase in data may seem overwhelming, it also means we are becoming much more aware. But we’re not just dealing with more data; we’re facing a larger variety of vulnerabilities that have a greater impact through network effect. Thankfully, better data sources and increased automation can help manage the deluge. But first, let’s better understand the problem. New vulnerability types and their widening impact through the software supply chain When a novel vulnerability type is disclosed, it very often spurs researchers to seek, and find, more examples of this new class, leading to a flood of new information. For example, an abundance of speculative execution vulnerabilities followed the Spectre and Meltdown disclosures, and, to the chagrin of many open source developers, regular expression denial-of-service (ReDoS) attacks have increased since 2021 (though they’ve been around much longer than that). We’ve also seen an increase in cloud-related vulnerability disclosures—not because the cloud is a new concept, but because disclosing vulnerabilities in cloud-related products has long been a point of contention. The prevailing reasoning against disclosing cloud vulnerabilities was to avoid generating alerts that don’t require user action to remediate. But as cloud-based technologies have become integral to modern development, the need for transparency has outweighed the perceived negatives. The CVE rules were updated this year to encourage disclosures, and major vendors like Microsoft have publicly stated that they’re making this change to support transparency, learning, safety, and resilience of critical cloud software. Beyond cloud vulnerabilities, the 2021 edition of the OWASP Top 10 also saw an increase in vulnerabilities due to broken access controls, insecure design, security misconfigurations, vulnerable or outdated components (like dependencies), and security logging and monitoring failures. Whatever the vulnerability type, new categories of vulnerabilities often require new remediation and prevention tactics that developers must stay on top to keep their project secure. When a spur of research leads to a dramatic increase in vulnerability disclosures, development teams may need to spend significant effort to validate, deduplicate, and remediate these unfamiliar vulnerabilities. This creates a huge, time-sensitive burden for those responding, during an urgent time when delays should be avoided. To complicate matters even further, these new vulnerability types don’t even need to be in your code to affect you—they can be several layers down. The open source libraries, frameworks, and other tools that your project depends on to function—its dependencies—form the foundation of its supply chain. Just like vulnerabilities in your own code, vulnerabilities in your supply chain can pose a security risk, compromising the security of your project and its users. OSS maintainers can learn more about how to mitigate these vulnerabilities and secure your supply chain, including your developer accounts, code, and build systems, to keep your software safe while maintaining and even increasing developer productivity. For example, our 2020 Octoverse report found that the median JavaScript project on GitHub used just 10 open source dependencies directly. That same repository, however, can have 683 transitive dependencies. In other words, even if you only directly include 10 dependencies, those dependencies come with their own transitive dependencies that you inherit. Lacking awareness of transitive dependencies can leave you and your users unknowingly at risk, and the sheer number of transitive dependencies for the average project requires automation to scale. More security data coming straight from open source Since starting as a CNA in 2019, the open source GitHub CNA has grown so much that we are now the 5th largest CVE publisher of all time. This shows that open source maintainers—who are the source of this data—want to play a role in the security landscape. This is a big shift, and it’s a boon for open source, and everyone who relies on it. Indulge me for a brief history lesson to illustrate the degree of this change. For a significant portion of the CVE Program’s history, MITRE was the primary entity authorized to create, assign, and publish CVE IDs. While CNAs have existed since the program’s inception, their only duty until 2016 was to assign IDs. As the demands on the program grew, they led to scalability issues and data quality concerns, so the program expanded CNA duties to include curating and publishing CVE record details. Since then, the program has made significant efforts to increase the number of CNAs engaged in the program, and now has over 400 partners from 40 countries. Nowadays, more than 80% of CVE data originates directly from CNAs like ours. The explosion in CNAs has helped scale the program to better support the growing number of requests for CVEs, and almost more importantly, also means more primary sources of vulnerability data. Since CNAs must have a specific scope of coverage, as opposed to CNAs of Last Resort (CNA-LR) like MITRE, whose scope is everything else that isn’t already covered, a CNA’s scope tends to include software that it owns or is heavily invested in securing. The overwhelming benefit of this structure is that subject matter experts can control the messaging and vulnerability information shared with unique communities, leading to higher quality data and lower false positives. For us, this means a larger focus on securing open source software—and for any developer who contributes to or uses open source, that means more secure outcomes. Maintainers can ensure that they’re the primary source of vulnerability information on GitHub by leveraging repository security advisories to notify their end users and enabling private vulnerability reporting to help ensure that new vulnerabilities reach them first. Looking to level up your skills in software security? Check out the GitHub Secure Code Game developed by our Security Lab. It provides an in-repository learning experience where users can secure intentionally vulnerable code! Tackling data overload with automation Let’s return to that double-edged sword we started with: there’s more vulnerability data than ever before, which means more visibility and transparency, but also more data to sort through. The answer isn’t to reduce the amount of data. It’s to use automation to support easier curation, consumption, and prioritization of vulnerability data. Keeping track of the number of direct dependencies at scale can be burdensome, but the sheer number of transitive dependencies can be overwhelming. To keep up, software bill of materials (SBOM) formats like SPDX and CycloneDX allow users to create a machine-readable inventory of a project’s dependencies and information like versions, package identifiers, licenses, and copyright. SBOMs help reduce supply chain risks by: Providing transparency about the dependencies used by your repository. Allowing vulnerabilities to be identified early in the process. Providing insights into the security, license compliance, or quality issues that may exist in your code base. Enabling you to better comply with various data protection standards through automation. When it comes to reporting vulnerabilities, CVE Services has been extremely helpful in reducing the friction for CNAs to reserve CVE IDs and publish CVE Records by providing a self-service web interface—and that CVE data is a critical data source for us. It accounts for over 92% of the data feeding into our Advisory Database, so anything that helps ensure this information is published faster and more efficiently benefits those using the data downstream, like our team, and by proxy, developers on GitHub! At GitHub, we leverage APIs from our vulnerability data providers to ingest data for review, export our data in the machine-readable Open Source Vulnerability (OSV) format for consumption by others, and notify our users automatically through Dependabot alerts. While the increased automation around vulnerability data has allowed for easier reporting and consumption of this data, it’s also led to an increased need to automate the downstream impact for developers—finding and fixing vulnerabilities in both your own code and within your dependencies. Automated software composition analysis (SCA) tools like Dependabot help identify and mitigate security vulnerabilities in your dependencies by automatically updating your packages to the latest version or filing pull requests for security updates. Keep in mind that the coverage of SCA tools can often vary in scope—the GitHub Advisory Database and osv-scanner emphasize vulnerabilities in open source software, while grype focuses on container scanning and file systems—and ensure that their SCA solution supports the types of software in their production environment. Prioritization features in SCA tools, like Dependabot’s preset and custom auto-triage rules, can help users more efficiently manage data overload by helping determine which alerts should be addressed. Static application security testing (SAST) and SCA tools will both help you detect vulnerabilities. While SCA is geared towards addressing open source dependencies, SAST focuses more on vulnerabilities in your proprietary code. GitHub’s SAST tools like code scanning and CodeQL help find vulnerabilities in your code while Copilot Autofix simplifies vulnerability remediation by providing natural language explanations for vulnerabilities and suggesting code changes. As the vulnerability landscape continues to evolve and aspects of vulnerability management shift left, it’s critical that open source developers are empowered to engage in security. The double-edged sword of increased vulnerability data means more awareness, but requires automation to manage properly, especially when considering the wider impact that supply chain vulnerabilities can cause. At GitHub and within the Security Lab, we are committed to continuing to support and secure open source by providing tools, guidance, and educational resources for the community as software and vulnerabilities progress. Learn more Research from the Cyentia institute (inspired this blog) The GitHub Security Lab How to work with security advisories Securing your end-to-end supply chain Removing the stigma of a CVE How to mitigate OWASP vulnerabilities while staying in the flow SCA vs SAST: what are they and which one is right for you? Fixing security vulnerabilities with AI Coordinated vulnerability disclosure (CVD) for open source projects OpenSSF Guide to implementing a coordinated vulnerability disclosure process for open source projects 5 myths about CVEs The Value of Assigning CVEs The post Securing the open source supply chain: The essential role of CVEs appeared first on The GitHub Blog. Securing the open source supply chain: The essential role of CVEshttps://github.blog/security/supply-chain-security/securing-the-open-source-supply-chain-the-essential-role-of-cves/","tags":["Blogs","Github"],"categories":["Blogs","Github"]},{"title":"WHICH MINECRAFT SOUNDS DID I MAKE?","path":"/RSSBOX/rss/2baf551.html","content":"#minecraft #minecraftshorts #minecraftsounds #shortsvideo #shortvideo #ytshort #ytshorts #sounds WHICH MINECRAFT SOUNDS DID I MAKE?https://www.youtube.com/watch?v=mQ5dbfagBbA","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"WHICH POTION DO YOU GET?","path":"/RSSBOX/rss/ba466e3c.html","content":"#minecraftshorts #minecraft #shortsvideo #ytshorts #gaming WHICH POTION DO YOU GET?https://www.youtube.com/watch?v=YzSKYIIBCrI","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"The Story of Web Framework Hono, From the Creator of Hono","path":"/RSSBOX/rss/431bf226.html","content":"Hono is a fast, lightweight web framework that runs anywhere JavaScript does, built with Web Standards. Of course, it runs on Cloudflare Workers.It was three years ago, in December 2021. At that time, I wanted to create applications for Cloudflare Workers, but the code became verbose without using a framework, and couldn't find a framework that suited my needs. Itty-router was very nice but too simple. Worktop and Sunder did the same things I wanted to do, but their APIs weren't quite to my liking. I was also interested in creating a router — a program that determines which action is executed based on the HTTP method and URL path of the Request — made of a Trie tree structure because it’s fast. So, I started building a web framework with a Trie tree-based router. “While trying to create my applications, I ended up creating my framework for them.” — a classic example of yak shaving. However, Hono is now used by many developers, including Cloudflare, which uses Hono in core products. So, this journey into the depths of yak shaving was ultimately meaningful. Write once, run anywhere Hono truly runs anywhere — not just on Cloudflare Workers. I’ll discuss why later in the post, but Hono also runs on Deno, Bun, and Node.js. This is because Hono does not depend on external libraries, but uses only the Web Standards API, and each runtime supports Web Standards.It's a delight for developers to know that the same code can run across different runtimes. For instance, the following src/index.ts code will run on Cloudflare Workers, Deno, and Bun. import &#123; Hono &#125; from 'hono' const app &#x3D; new Hono()app.get(‘&#x2F;hello’, (c) &#x3D;&gt; c.text(‘Hello Hono!’)) export default app To run it on Cloudflare Workers, you execute the Wrangler command: wrangler dev src/index.ts The same code works on Deno: deno serve src/index.ts And it works on Bun too: bun run src/index.ts This is only a simple \"Hello World\" example, but more complex applications with middleware and helpers that are discussed below can be run on Cloudflare Workers or the other runtimes. As proof of this, almost all our test code for Hono itself can run the same way on these runtimes. This is a genuine \"write once, run anywhere\" experience. Who is using Hono? Hono is now used by many developers and companies. For example, Unkey deploys their application built with Hono's OpenAPI feature to Cloudflare Workers. The following is a list of companies using Hono, based on my survey \"Who is using Hono in production?”.CloudflareNodecraftOpenStatusUnkeyGoensNOT A HOTELCyberAgentAI shiftHanabi.restBaseAI There are many, many more companies not listed here. And major web services or libraries, such as Prisma, Resend, Vercel AI SDK, Supabase, and Upstash, use Hono in their examples. There are also several influencers who like Hono and use it as an alternative to Express.Of course, at Cloudflare, we also use Hono. D1 uses Hono for the internal Web API running on Workers. Workers Logs is based on code from Baselime (acquired by Cloudflare) and uses Hono to migrate the applications from their original infrastructure to Cloudflare Workers. All Workers Logs internal or customer-facing APIs are run on Workers using Hono. We also use Hono as part of the internals of many other products, such as KV and Queues. Why are you making a “multi-runtime” framework? You might wonder “Why is an employee of Cloudflare creating a framework that runs everywhere?” Initially, Hono was designed to work exclusively with Cloudflare Workers. However, starting with version 2, I added support for Deno and Bun. This was a very wise decision. If Hono had been targeted only at Cloudflare Workers, it might not have attracted as many users. By running on more runtimes, it gains more users, leading to the discovery of bugs and receiving more feedback, which ultimately leads to higher quality software. Hono and Cloudflare are a perfect combo The combination of Hono and Cloudflare offers a delightful developer experience.Many websites, including our Cloudflare Docs, introduce the following \"vanilla\" JavaScript as a \"Hello World\" for Cloudflare Workers: export default &#123; fetch: () =&gt; &#123; return new Response('Hello World!') &#125; &#125; This is primitive and good for understanding the Workers principle. However, if you want to create an endpoint that \"returns a JSON response for GET requests that come to /books\", you need to write something like this: export default &#123; fetch: (req) =&gt; &#123; const url = new URL(req.url) if (req.method === 'GET' &amp;&amp; url.pathname === '/books') &#123; return Response.json(&#123; ok: true &#125;) &#125; return Response.json( &#123; ok: false &#125;, &#123; status: 404 &#125; ) &#125; &#125; If you use Hono, you can write it like the following: import &#123; Hono &#125; from 'hono' const app &#x3D; new Hono() app.get(‘&#x2F;books’, (c) &#x3D;&gt; &amp;#123; return c.json(&amp;#123; ok: true &amp;#125;)&amp;#125;) export default app It is short. And you can understand that “it handles GET accesses to /books” intuitively.If you want to handle GET requests to /authors/yusuke and get \"yusuke\" from the path — \"yusuke\" is variable, you have to add something more complicated. The below is \"vanilla\" JavaScript example: if (req.method === 'GET') &#123; const match = url.pathname.match(/^\\/authors\\/([^\\/]+)/) if (match) &#123; const author = match[1] return Response.json(&#123; Author: author &#125;) &#125; &#125; If you use Hono, you don't need if statements. Just add the endpoint definition to the app. Also, you don't need to write a regular expression to get \"yusuke\". You can get it with the function c.req.param(): app.get('/authors/:name', (c) =&gt; &#123; const author = c.req.param('name') return c.json(&#123; Author: author &#125;) &#125;) One or two routes may be fine, but any more than that and maintenance becomes tricky. Code becomes more complex and bugs are harder to find. Using Hono, the code is very neat.It is also easy to handle bindings to Cloudflare products, such as KV, R2, D1, etc. as Hono uses a \"context model\". A context is a container that holds the application's state until a request is received, and a response is returned. You can use a context to retrieve a request object, set response headers, and create custom variables. It also holds Cloudflare bindings. For example, if you set up a Cloudflare KV namespace with the name MY_KV, you can access it as follows, with TypeScript type completion. import &#123; Hono &#125; from 'hono' type Env &#x3D; &amp;#123; Bindings: &amp;#123; MY_KV: KVNamespace &amp;#125;&amp;#125; const app &#x3D; new Hono&lt;Env&gt;() app.post(‘&#x2F;message’, async (c) &#x3D;&gt; &amp;#123; const message &#x3D; c.req.query(‘message’) ?? ‘Hi’ await c.env.MY_KV.put(‘message’, message) return c.text(message is set, 201)&amp;#125;) Hono lets you write code in a simple and intuitive way, but that doesn't mean there are limitations. You can do everything possible with Cloudflare Workers using Hono. Add it when you want to use it Hono is tiny. With the smallest preset, hono/tiny, you can write a \"Hello World\" application in just 12 KB. This is because it uses only the Web Standards API built into the runtime and has minimal functions. In comparison, the bundle size of Express is 579 KB. However, there is much that you can do.You can easily add functions using middleware. For example, it is a bit tedious to implement Basic Authentication from scratch, but with the built-in Basic Auth middleware, you can apply Basic Authentication to the path /auth/page with just this: import &#123; Hono &#125; from 'hono' import &#123; basicAuth &#125; from 'hono/basic-auth' const app &#x3D; new Hono() app.use( ‘&#x2F;auth&#x2F;*’, basicAuth(&amp;#123; username: ‘hono’, password: ‘acoolproject’, &amp;#125;)) app.get(‘&#x2F;auth&#x2F;page’, (c) &#x3D;&gt; &amp;#123; return c.text(‘You are authorized’)&amp;#125;) Hono's package also includes built-in middleware that allows Bearer and JWT authentication, and easy configuration of CORS, etc. These built-in middleware components do not depend on external libraries, but there is also many 3rd-party middleware that allow the use of external libraries, such as authentication middleware using Clerk and Auth.js, and validators using Zod and Valibot.There are also a number of built-in helpers, including the Streaming helper, which is useful for implementing AI. These can be added when you want to use them, and the file size increases only when they are added.In Cloudflare Workers, there is a limit to a file size of a Worker. Keeping the core small and extending functions with middleware and helpers makes a lot of sense. Onion structure The important concepts of Hono are ”handler” and \"middleware”.A handler is a place to write a function that receives a request and returns a response, as specified by the user. For example, you can write a handler that gets a value of a query parameter, retrieves data from a database, and returns the result in JSON. Middleware can handle the requests that come to the handler and the responses that the handler returns. You can combine middleware with other middleware to build more large and complex applications. It is structured like an onion. In a remarkably simple way, you can create middleware. For example, a custom logger that logs the request can be written as follows: app.use(async (c, next) =&gt; &#123; console.log(`[$&#123;c.req.method&#125;] $&#123;c.req.path&#125;`) await next() &#125;) If you want to add a custom header to the response, write the following: app.use(async (c, next) =&gt; &#123; await next() c.header('X-Message', 'Hi, this is Hono!') &#125;) It would be interesting to combine this with HTMLRewriter. If an endpoint returns HTML, the middleware that modifies the HTML tags in it can be written as follows: app.get('/pages/*', async (c, next) =&gt; &#123; await next() class AttributeRewriter &amp;#123; constructor(attributeName) &amp;#123; this.attributeName &#x3D; attributeName &amp;#125; element(element) &amp;#123; const attribute &#x3D; element.getAttribute(this.attributeName) if (attribute) &amp;#123; element.setAttribute(this.attributeName, attribute.replace(‘oldhost’, ‘newhost’)) &amp;#125; &amp;#125; &amp;#125; const rewriter &#x3D; new HTMLRewriter().on(‘a’, new AttributeRewriter(‘href’)) const contentType &#x3D; c.res.headers.get(‘Content-Type’) if (contentType!.startsWith(‘text&#x2F;html’)) &amp;#123; c.res &#x3D; rewriter.transform(c.res) &amp;#125;&amp;#125;) There is very little to remember to create middleware. All you have to do is to work with the context, which you should already know. The RPC is like magic Hono has a strong type system. One feature that uses this is RPC (Remote Procedure Call). With RPC, you can express server-side API specifications as TypeScript types. When these types are loaded as generics in a client, the paths, arguments, and return types of each API endpoint are inferred. It's like magic.For example, imagine an endpoint for creating a blog post. This endpoint takes a number type id and a string type title. Using Zod, one of the validator libraries that support TypeScript inference, you can define the schema like this: import &#123; z &#125; from 'zod' const schema &#x3D; z.object(&amp;#123; id: z.number(), title: z.string()&amp;#125;) You create a handler that receives this object in JSON format via a POST request to the path /posts. Using Zod Validator, you check if it matches the schema. The response will have a property called message of type string. import &#123; zValidator &#125; from '@hono/zod-validator' const app &#x3D; new Hono().basePath(‘&#x2F;v1’) &#x2F;&#x2F; … const routes &#x3D; app.post(‘&#x2F;posts’, zValidator(‘json’, schema), (c) &#x3D;&gt; &amp;#123; const data &#x3D; c.req.valid(‘json’) return c.json(&amp;#123; message: $&amp;#123;data.id.toString()&amp;#125; is $&amp;#123;data.title&amp;#125; &amp;#125;)&amp;#125;) This is a “typical” Hono handler. However, the TypeScript type you can get from the typeof for the routes will contain the information about its Web API specification. In this case, it includes the endpoint for creating blog posts — sending a POST request to the path /posts returns a JSON object. export type AppType = typeof routes Now, let's create a client. You pass the earlier AppType as generics to a Hono client object. import &#123; hc &#125; from 'hono/client' import &#123; AppType &#125; from '.' const client &#x3D; hc&lt;AppType&gt;(‘http://localhost:8787&#39;) With this setup, you're ready. It's magic time. Code completion works perfectly. When you write client-side code, you no longer need to know the API specifications completely, which also helps eliminate mistakes. Server-side JSX is fun Hono provides built-in JSX, a syntax that allows you to write code in JavaScript that looks like HTML tags. When you hear the term JSX, you may think of React, a front-end UI library. However, Hono's JSX was initially developed to run only on the server side. When we first started developing Hono, we were looking for template engines to render HTML. Most template engines, such as Handlebars and EJS, use eval internally and are incompatible with Cloudflare Workers, which does not support it. Then we came up with the idea of using JSX.Hono's JSX is unique in that it treats the tags as a string. So the following strange code actually works. console.log((&lt;h1&gt;Hello!&lt;/h1&gt;).toString()) There is no need to do renderToString() as in React. If you want to render HTML, just return this as is. app.get('/', (c) =&gt; c.html(&lt;h1&gt;Hello&lt;/h1&gt;)) Very interesting is the creation of Suspense — a feature in React that allows you to display a fallback UI while waiting for an asynchronous component to load — without any client implementation. The asynchronous components are running in a server-only implementation. Server-side JSX is a better developer experience than you might imagine. You can use the toolchains for React's JSX in the same way for Hono's JSX, including the ability to complete tags in the editor. They bring mature front-end technology to the server side. Testing is important Testing is important. Fortunately, you can write tests easily when using Hono.For example, let's write a test for an endpoint. To test for a 200 response status of a request coming to / with the GET method, you can write the following: it('should return 200 response', async () =&gt; &#123; const res = await app.request('/') expect(res.status).toBe(200) &#125;) Simple, right? The beauty of this test is that you don't have to bring up the server. The Web Standard API black boxes the server layer. The internal tests of Hono have 20,000 lines of code, but most of them are written in the same style as above, without the server up and running. Going to full-stack We released a new major version 4 in February 2024. There are three main features that stand out:Static site generationClient componentsFile-based routingWith these features, you can create full-stack applications with a user interface in Hono.The introduction of client components allows JSX to work in the client. Now you can add interactions to your pages. Static site generation allows you to create blogs, etc. without having to bundle them into a single JavaScript file. We have also started an experimental project called HonoX. This is a meta-framework using Hono and Vite that provides file-based routing and a mechanism to hydrate client-side components to server-side generated HTML. It is easier to create larger applications that are a great match for Cloudflare Pages or Workers.In addition to that, plans are underway to run it as a base server for existing full-stack frameworks such as Remix and Qwik.In contrast to the Next.js framework, which started from the client-side with React, Hono is trying to become a full-stack framework starting from the server-side. Hono Conference On June 22, 2024, I held the \"Hono Conference\" in Tokyo, the first event to consist entirely of Hono-focused talks. One hundred people attended, and the event was a great success.It was my dream to do this event. Now, there are 200 contributors to the honojs/hono repository on GitHub. If you include other Hono related repositories, there are many more. Creating \"the most invincible framework we could think of\" is a lot of fun for contributors and users.Below is a group photo taken at the end of the event. This is my treasure. I want to make the 2nd event a global event. Hono is 炎 I haven't mentioned the origin of the name Hono yet. The name Hono is from the Japanese word for \"炎\". It is similar to the word \"flare\". Hono now runs on a variety of runtimes, but I said that it was first created to create Cloud\"flare\" Workers applications. It is an honor for Cloudflare that it has remained in its name.That is all that the creator of Hono has to say about Hono. Just try it Everyone who has experienced application development with Hono and Cloudflare Workers says \"the developer experience is a great experience\". If you haven't experienced it yet, just try it.See the Hono website for how to get started. If you are interested in reporting issues or contributing, please see the GitHub project. Plus, you can watch my interview about Hono on the YouTube Cloudflare Developers channel. The story of web framework Hono, from the creator of Honohttps://blog.cloudflare.com/the-story-of-web-framework-hono-from-the-creator-of-hono","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"1.21.2-Rc1 快照更新","path":"/RSSBOX/rss/50a78efb.html","content":"1.21.2-rc1 快照更新 1.21.2-rc1 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"Career Growth, Learning, and Fun, Oh My! Your Guide to GitHub Universe 2024","path":"/RSSBOX/rss/5117513d.html","content":"Excitement is in the air as we gear up for our most community-driven gathering of the year! With just weeks to go until our biggest developer event, the anticipation is building. ✨ As one attendee from last year said, “This is actually my first time at Universe and it has been awesome. Not only the event itself and how it looks and feels, but the people and the energy. Everyone is really excited to do really nerdy things, and that’s been my favorite.” We’re thrilled to welcome you to the Fort Mason Center in San Francisco on October 29-30 for a milestone celebration—10 years of GitHub Universe. This is not just a look back at how far the software development industry has come, but a chance to shape what comes next, together. Plus, be the first to hear about GitHub’s newest innovations and biggest ships of the year, straight from our CEO at the day 1 keynote. In this blog, you’ll get a sneak peek at some of the exciting activities, networking opportunities, and interactive learning experiences we have lined up for you over two incredible days. And trust us: we have more surprises in store! If you haven’t already secured your tickets, now is the time—don’t miss out on connecting with fellow developers, gaining fresh insights, and immersing yourself in the vibrant GitHub community. Tickets are selling fast, so grab yours today for an experience you won’t forget. Let’s dive right in. Get tickets now 🗺️ Map out your sessions With over 100 sessions to choose from, it’s a good idea to explore the agenda and pick out the talks, workshops, and events that excite you the most. Once you’re logged into your attendee portal, use our schedule builder tool to “favorite” your must-attend sessions and create your own customized schedule. If sorting through all the sessions feels overwhelming, don’t worry—we’ve got you covered! Take a look at our curated agendas tailored to your experience level, industry, and topics of interest. We even have agendas from influential community leaders like Brian Douglas, founder of OpenSauced, for you to follow along with. 🕹️ Fuel your curiosity with innovation and play In between sessions, Universe ‘24 will have interactive zones to help inspire your creativity and challenge your thinking. Here are four activations you definitely don’t want to miss: Open source zone Visit the open source zone and explore live demos from rising stars in the GitHub Accelerator program, and connect with leaders from the Maintainer Community and passionate opensource creators from around the world. With projects like Home Assistant, Kubernetes, and Node featured across the two days, this is your chance to witness firsthand the transformative power of open source. GitHub Copilot Extensions demo For an interactive tech experience, head to the GitHub Copilot Extensions demo! Explore the endless possibilities of extensions—whether you’re browsing the GitHub Marketplace for ready-made extensions or learning how to build your own. The Copilot Extensions team will be on hand to help you unlock the full potential of what you can create. Don’t miss this chance to power up your coding experience! Logitech activation Curious about what it means to get into your flow state? Then visit the Logitech activation! Using an EEG headset and biometric data, the Logitech team will analyze your brain activity and create a personalized digital visualization of your unique brainwave patterns when you’re in peak productivity. It will be a mind-bending experience that combines science and art to show you what your flow looks like. Hack your badge station Unleash your creativity at the hack your badge station, where you can personalize your GitHub Universe badge with hands-on tutorials and guidance. Every in-person Universe ticket comes with a hackable badge, and here’s your chance to make it uniquely yours. Whether you’re a seasoned badge hacker or a first-timer, dive into tutorials and guidance to personalize your badge with flair. It’s a fun way to show off your skills and take home a one-of-a-kind keepsake from GitHub Universe! ♥️ Explore the heart of Universe Next, find GitHub Central—your hub for discovering the latest GitHub features and networking with your peers. Here, you’ll see interactive product demos and zones for each content track—AI, the developer experience, and security—all with access to engage directly with GitHub experts. While you’re there, get competitive in our fast-paced Bug Bash game or pick up a one-of-a-kind postcard generated from your contributions over the years. Plus, take advantage of networking moments at the “meet the Universe speakers” area and the Stars Lounge—connecting with GitHub’s top thought leaders and influential community members. You also won’t want to miss going to The GitHub Shop. Whether you’re after the latest Universe apparel, unique GitHub Copilot collectibles, or our beloved invertocat hoodies, the shop has something for every GitHub fan—including an all-new collection. (Plus, the famous ugly holiday sweater is making a comeback with a brand new design!) If you’re one of the first 1,000 visitors, you’ll receive a special gift with your purchase. So, come early and snag your favorite items before they’re gone! Get a sneak peek of the new collection below: 🤝 Meet the minds behind GitHub Connecting with experts is easier than ever with key areas designed to spark conversations, share insights, and offer personalized guidance. Whether you’re looking for technical support, future-forward ideas, or startup wisdom, we’ve got you covered. Start at the GitHub Expert Center, where specialists are on hand to help you master every corner of GitHub’s offerings. From optimizing workflows with GitHub Actions to scaling Copilot or enhancing security practices with GitHub Advanced Security, this is the go-to place for tailored advice. This year, you can even schedule a slot ahead of time, making it easy to get one-on-one time with the pros in the areas that matter most to you. For a glimpse into the future of software development, visit GitHub Next, where the team behind Copilot and GitHub Copilot Workspace will share their latest prototypes. Engage with cutting-edge tools and get a sneak peek at how GitHub is pushing the boundaries of development. If you’re an entrepreneur or startup enthusiast, the Startup Lounge is where you’ll want to be. Network with founders, share ideas, and collaborate with fellow builders as you explore ways to grow and scale your startup. It’s the perfect space to gain insights and make connections that could shape the future of your business. ↗️ Level up your career Take your career to the next level at Universe with a range of opportunities designed to help you grow and stand out! Start by visiting the Career Corner, where you can engage in private consultations with talent experts. Whether you need help refining your resume, optimizing your LinkedIn and GitHub profiles, or getting tips for your next big interview, these one-on-one sessions offer personalized guidance to boost your career search and set you up for success. Looking to sharpen your technical skills? Don’t miss our workshops, led by industry experts, where you can gain hands-on experience and enhance your knowledge in key areas of development. And for those ready to validate their expertise, our on-site certification testing allows you to become GitHub-certified in areas like GitHub Foundations, GitHub Actions, GitHub Advanced Security, and even GitHub Copilot. You can still add a workshop or on-site certification testing to your Universe ticket. Don’t miss your chance to showcase your skills and stand out from the crowd! 🌍 Your Universe experience awaits As we look forward to celebrating 10 years of GitHub Universe, prepare for a wealth of knowledge, connections, and inspiration. This event will mark the continuation of your journey—the skills, insights, and relationships you build here will benefit your career and projects well beyond the two days you spend with us. We’re so excited to see you forge new paths and innovate beyond your limits! If you haven’t secured your place yet, it’s not too late to get tickets now and join us in person at San Francisco’s Fort Mason Center for our biggest event yet! Be sure to follow along on the blog and our social media channels in the coming weeks for post-event recaps, including highlights and key takeaways from Universe ’24. The post Career growth, learning, and fun, oh my! Your guide to GitHub Universe 2024 appeared first on The GitHub Blog. Career growth, learning, and fun, oh my! Your guide to GitHub Universe 2024https://github.blog/news-insights/company-news/career-growth-learning-and-fun-oh-my-your-guide-to-github-universe-2024/","tags":["Blogs","Github"],"categories":["Blogs","Github"]},{"title":"GitHub for Nonprofits: Drive Social Impact One Commit at a Time","path":"/RSSBOX/rss/f08c47f4.html","content":"Over the past few years, we’ve seen the number of nonprofits leveraging GitHub grow—and grow. Technology is increasingly becoming a critical part of nonprofit’s strategies to drive forward their missions, accelerate human progress, and take big strides toward the Sustainable Development Goals. And we’re here to make this process easier and more accessible for all—from local grassroots organizations to global nonprofits. Welcome to GitHub for Nonprofits. This new portal makes the sign up process seamless, with exclusive discounts automatically applied to your account.** Verified nonprofits are eligible for free access to the GitHub Team plan or 25% off the GitHub Enterprise Cloud plan.** This includes nonprofit organizations that are 501(c)(3) or equivalent and are non-governmental, non-academic, non-commercial, non-political in nature, and have no religious affiliation. Wondering how GitHub could practically help your nonprofit? Here are a few ways they’ll help you drive your mission forward: Manage your projects. Investing in GitHub is not just about adopting a tool; it’s about leveraging technology to reach goals in a collaborative way. Increase visibility and widen impact. By hosting projects on GitHub, nonprofits can increase their visibility and reach a wider audience. Whether it’s sharing code libraries, publishing research, or showcasing success stories, GitHub provides nonprofits with a platform to amplify their impact and attract support from donors, funders, volunteers, and partners. Connect with the open source community. GitHub is home to the largest open source communities on the planet. By hosting your projects on GitHub, you can tap into this incredible pool of talent and expertise. Need help with a tricky problem? Want to attract volunteers to your cause? GitHub has you covered. This seamless way of signing up may be new, but the adoption of GitHub by nonprofit organizations is tried and true. We have customers around the world using GitHub for good, and they have the results to show it: “GitHub provides us with a platform to amplify the critical needs of forcibly displaced persons and attract support from donors, volunteers, and partners, while also tapping into skills and resources of an incredible developer community.” – Seema Iyer, USA for UNHCR “As a nonprofit, when we want to develop something new, we have to figure out a way to do that with limited resources and a small team. GitHub is a big part of our productivity every single day and we’ve recently leveraged different features to develop an algorithm to bring a new level of impact reporting to our donors. This has helped us unlock new efforts that we wouldn’t have been able to do otherwise and has enabled our team to start imagining new opportunities for the future.” – Christa Stelzmuller, charity: water “With GitHub, we unite developers worldwide, turning open source technology into a force for good—fighting poverty, one line of code at a time.” – Sandino Scheidegger, Switzerland for Social Income Join GitHub for Nonprofits, where technology meets purpose, and together, let’s create a more sustainable and equitable future for all. Sign up your nonprofit today. The post GitHub for Nonprofits: Drive social impact one commit at a time appeared first on The GitHub Blog. GitHub for Nonprofits: Drive social impact one commit at a timehttps://github.blog/news-insights/product-news/github-for-nonprofits-drive-social-impact-one-commit-at-a-time/","tags":["Blogs","Github"],"categories":["Blogs","Github"]},{"title":"1.21.2-Pre5 快照更新","path":"/RSSBOX/rss/2c4ae175.html","content":"1.21.2-pre5 快照更新 1.21.2-pre5 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"5 Tips and Tricks When Using GitHub Copilot Workspace","path":"/RSSBOX/rss/bfc3cafa.html","content":"Can you believe it has already been five months since we announced the technical preview of GitHub Copilot Workspace? Throughout that time, the GitHub Next team has been listening and learning from the community and steadily iterating on it. For those joining us at GitHub Universe, we will be talking about GitHub Next’s journey to create Copilot Workspace, along with some valuable concepts and learnings gleaned along the way. But while we put the finishing touches on our talk, we wanted to share a few tips and tricks that we have picked up, both from ourselves and from the amazing community of developers using Copilot Workspace every day. Before we get started, let’s briefly recap Copilot Workspace and how it works. What is Copilot Workspace, and how does it work? Copilot Workspace is a Copilot-native dev environment designed to help you complete everyday coding tasks. From GitHub, you can open a GitHub issue, pull request, template repository, or an ad-hoc task in Copilot Workspace and start working on the problem. Did you know?You can start a Copilot Workspace session outside of the above routes. Copilot Workspace sessions take a handful of query parameters, including the task and repository. So, you could build your own entry point. For example, the GitHub Next team built an extension for Raycast! Once you’ve started a session, you work with Copilot Workspace to iterate through a set of stages to solve your problem. These include: Spec/Brainstorming (Optional): You can use Copilot Workspace as a thought partner by asking questions about how your codebase currently works and to explore ideas on how to solve your task. Plan: Copilot Workspace figures out which files it must alter, and exactly what it must accomplish in each file in order to complete the task. These are completely editable, allowing you to add, edit, and remove steps or file adjustments. Implementation: Once you have iterated through these stages, Copilot Workspace will stream coding suggestions into the environment. You can use the built-in terminal or a codespace to verify the changes before you create a pull request to merge your changes back into your codebase. You can keep iterating and bouncing between the stages as you need, re-prompting Copilot Workspace or regenerating suggestions. Once you have landed on an idea, you can raise a pull request, create a new branch, or in some cases, create a new repository. Did you know?There is a Copilot Workspace user manual repository that contains additional tips, a log of recent changes, and more! Now that we’ve learned the foundations, what tips have we gathered from the team and the community on how to get the most out of Copilot Workspace? Tip 1: Be specific about what “done” means, and provide additional context A crisply defined goal is important whether you’re working with a colleague or with GitHub Copilot. The more clearly you can articulate what “done” looks like for a task, the more likely you are to get a result that meets those conditions. AI is better at handling natural language than any other software in history, but ambiguity is still the number one path to poor results. Working with AI can sometimes feel like a slot machine—it’s not always clear how to coax reliably good results from the tools. Supplying clear goals and context not only improves the quality of results, it also makes those positive results happen consistently. In fact, this is a similar tip that we’ve published before when working with GitHub Copilot in general! So, what does that mean in practice for Copilot Workspace? When you’re writing your task (such as a GitHub Issue), write some bullet points that capture your criteria for success. It doesn’t need to be fancy or long! Here’s an example of an issue that gives Copilot Workspace clear context and goals. If you know specific files, classes, components, or directories that need to be changed, include them in your task description. That kind of detail is fantastic context for guiding Copilot Workspace towards a solution. But you’re not just limited to specifying filenames or classes. Copilot Workspace was recently updated with the ability to pull information when you reference URLs such as issues, pull requests, and repository files. It can even take context from public URLs (such as docs) that might provide additional instructions to help with what you’re trying to accomplish. Overall, the more context you can provide to Copilot Workspace, the more likely you will get a result closer to your expectations. So, if you have information about style guides, coding guidelines for unit tests, or any other relevant documentation, try linking to that in your task! Tip 2: Decompose into smaller tasks Kudos to Willem from our Discord community who wrote a post which included this one. It’s also similar to another tip that we’ve published before when working with GitHub Copilot. When you have a larger task, the desired state may not be easy to articulate. Consider breaking the task down into smaller parts so it’s easier to define success for the task. This also aligns with practices we’re used to as developers, such as one commit for one change. Tip 3: Iteratively review and refine Copilot Workspace has been designed with a number of principles in mind (If you want to learn about those, then you should check out our talk at GitHub Universe!). One of those principles is allowing users to steer Copilot Workspace, so that you can iterate over ideas and edit the returned suggestions. Therefore, it’s important to consider Copilot Workspace’s suggestions as a first attempt or a first draft. Revisions are a core part of making that interaction succeed. With that mental model, you begin using Copilot Workspace as a tool to iterate on ideas and potential solutions, helping to bootstrap a first attempt at a solution. What does this mean in practice? When you launch a Copilot Workspace session, review the topic and make sure that Copilot Workspace is focused on solving the right problem. If not, then edit it! Copilot Workspace is designed to be steered at every step. Each stage is deliberately editable so that you can adjust course as needed. If the spec or plan doesn’t look quite right, then edit them, or regenerate them by editing an earlier step. Iterate until you have something closer to your expectations. And remember, you can undo those suggestions and return to a previous state if needed. If you ask another developer for a solution, you probably wouldn’t expect a perfect answer on their first attempt. It might take a bit of time, iteration, and rephrasing of questions to get what you need, whether you’re working with a colleague or working with Copilot! Tip 4: You can edit code directly Once you’ve been through the iteration process with Copilot Workspace, you may have some code that looks good but needs some minor tweaks. Remember that you can edit the code directly in the Copilot Workspace editor (or even in a codespace). At some point, it might make more sense to make those changes directly rather than asking Copilot Workspace by revising the task, spec, or plan. Additionally, you can use Copilot’s ghost text capabilities within the environment, as well as other language services, to help you make those changes! Tip 5: Build, test, and run inside Copilot Workspace And finally, many of us are used to the inner loop of development. This is the idea that when we write code, we check that it still passes any builds and tests locally before we raise a pull request to run it through a more rigorous set of checks in our continuous integration (CI) process and have it peer reviewed. You can (and should) run builds and tests within Copilot Workspace as well! You can use the built-in terminal, and terminal assist capabilities to help you fix any errors or issues. That way, you can validate your changes within Copilot Workspaces and keep iterating until you meet a certain level of quality before proceeding through your full CI process. Wrap-up We’ve learned a lot from our community throughout the Copilot Workspace technical preview, and we hope that these tips help as you continue to explore and experiment! If you want to learn more about GitHub Copilot Workspace, be sure to check out our session at GitHub Universe, join GitHub Next’s Discord Community and of course, sign up to the waitlist for the preview! The post 5 tips and tricks when using GitHub Copilot Workspace appeared first on The GitHub Blog. 5 tips and tricks when using GitHub Copilot Workspacehttps://github.blog/ai-and-ml/github-copilot/5-tips-and-tricks-when-using-github-copilot-workspace/","tags":["Blogs","Github"],"categories":["Blogs","Github"]},{"title":"Minecraft X Dr Seuss DLC","path":"/RSSBOX/rss/d0108584.html","content":"Digging through stories from books that you know – oh, the blocky places you’ll go! Dr. Seuss is coming to Marketplace in a poetic new DLC. Wake up in a wobbly world and wander down three paths to experience The Cat in the Hat, The Lorax, and Oh, the Places You’ll Go! in a whole new 8-bit way. Navigate a parkour puzzle, solve an escape room-styled mission, and work your way through a massive maze! Head to the Dressing Room for a creative collection of Dr. Seuss inspired Character Creator items and skins, bound to keep you riddling and rhyming all day!Download here: https://www.minecraft.net/en-us/marketplace/pdp?id=f644f74d-e79d-4327-a324-0a2db0eb568b Minecraft x Dr Seuss DLChttps://www.youtube.com/watch?v=_S8ku_ZHvtk","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"Analysis of the EPYC 145% Performance Gain in Cloudflare Gen 12 Servers","path":"/RSSBOX/rss/383d68c3.html","content":"Cloudflare's network spans more than 330 cities in over 120 countries, serving over 60 million HTTP requests per second and 39 million DNS queries per second on average. These numbers will continue to grow, and at an accelerating pace, as will Cloudflare’s infrastructure to support them. While we can continue to scale out by deploying more servers, it is also paramount for us to develop and deploy more performant and more efficient servers.At the heart of each server is the processor (central processing unit, or CPU). Even though many aspects of a server rack can be redesigned to improve the cost to serve a request, CPU remains the biggest lever, as it is typically the primary compute resource in a server, and the primary enabler of new technologies.Cloudflare’s 12th Generation server with AMD EPYC 9684-X (codenamed Genoa-X) is 145% more performant and 63% more efficient. These are big numbers, but where do the performance gains come from? Cloudflare’s hardware system engineering team did a sensitivity analysis on three variants of 4th generation AMD EPYC processor to understand the contributing factors.For the 4th generation AMD EPYC Processors, AMD offers three architectural variants: mainstream classic Zen 4 cores, codenamed Genoaefficiency optimized dense Zen 4c cores, codenamed Bergamocache optimized Zen 4 cores with 3D V-cache, codenamed Genoa-X Figure 1 (from left to right): AMD EPYC 9654 (Genoa), AMD EPYC 9754 (Bergamo), AMD EPYC 9684X (Genoa-X)Key features common across the 4th Generation AMD EPYC processors:Up to 12x Core Complex Dies (CCDs)Each core has a private 1MB L2 cacheThe CCDs connect to memory, I/O, and each other through an I/O dieConfigurable Thermal Design Power (cTDP) up to 400WSupport up to 12 channels of DDR5-4800 1DPCSupport up to 128 lanes PCIe Gen 5Classic Zen 4 Cores (Genoa):Each Core Complex (CCX) has 8x Zen 4 Cores (16x Threads)Each CCX has a shared 32 MB L3 cache (4 MB/core)Each CCD has 1x CCXDense Zen 4c Cores (Bergamo):Each CCX has 8x Zen 4c Cores (16x Threads)Each CCX has a shared 16 MB L3 cache (2 MB/core)Each CCD has 2x CCXClassic Zen 4 Cores with 3D V-cache (Genoa-X):Each CCX has 8x Zen 4 Cores (16x Threads)Each CCX has a shared 96MB L3 cache (12 MB/core)Each CCD has 1x CCXFor more information on 4th generation AMD EPYC Processors architecture, see: https://www.amd.com/system/files/documents/4th-gen-epyc-processor-architecture-white-paper.pdf The following table is a summary of the specification of the AMD EPYC 7713 CPU in our Gen 11 server against the three CPU candidates, one from each variant of the 4th generation AMD EPYC Processors architecture: CPU Model AMD EPYC 7713 AMD EPYC 9654 AMD EPYC 9754 AMD EPYC 9684X Series Milan Genoa Bergamo Genoa-X # of CPU Cores 64 96 128 96 # of Threads 128 192 256 192 Base Clock 2.0 GHz 2.4 GHz 2.25 GHz 2.4 GHz All Core Boost Clock ~2.7 GHz* 3.55 Ghz 3.1 Ghz 3.42 Ghz Total L3 Cache 256 MB 384 MB 256 MB 1152 MB L3 cache per core 4 MB / core 4 MB / core 2 MB / core 12 MB / core Maximum configurable TDP 240W 400W 400W 400W * AMD EPYC 7713 all core boost clock is based on Cloudflare production data, not the official specification from AMD cf_benchmark Readers may remember that Cloudflare introduced cf_benchmark when we evaluated Qualcomm's ARM chips, using it as our first pass benchmark to shortlist AMD’s Rome CPU for our Gen 10 servers and to evaluate our chosen ARM CPU Ampere Altra Max against AWS Graviton 2. Likewise, we ran cf_benchmark against the three candidate CPUs for our 12th Gen servers: AMD EPYC 9654 (Genoa), AMD EPYC 9754 (Bergamo), and AMD EPYC 9684X (Genoa-X). The majority of cf_benchmark workloads are compute bound, and given more cores or higher CPU frequency, they score better. The graph and the table below show the benchmark performance comparison of the three CPU candidates with Genoa 9654 as the baseline, where &gt; 1.00x indicates better performance. Genoa 9654 (baseline) Bergamo 9754 Genoa-X 9684X openssl_pki 1.00x 1.16x 1.01x openssl_aead 1.00x 1.20x 1.01x luajit 1.00x 0.86x 1.00x brotli 1.00x 1.11x 0.98x gzip 1.00x 0.87x 1.01x go 1.00x 1.09x 1.00x Bergamo 9754 with 128 cores scores better in openssl_pki, openssl_aead, brotli, and go benchmark suites, and performs less favorably in luajit and gzip benchmark suites. Genoa-X 9684X (with significantly more L3 cache) doesn’t offer a significant boost in performance for these compute-bound benchmarks.These benchmarks are representative of some of the common workloads Cloudflare runs, and are useful in identifying software scaling issues, system configuration bottlenecks, and the impact of CPU design choices on workload-specific performance. However, the benchmark suite is not an exhaustive list of all workloads Cloudflare runs in production, and in reality, the workloads included in the benchmark suites are almost certainly not the exclusive workload running on the CPU. In short, though benchmark results can be informative, they do not represent a good indication of production performance when a mix of these workloads run on the same processor. Performance simulation To get an early indication of production performance, Cloudflare has an internal performance simulation tool that exercises our software stack to fetch a fixed asset repeatedly. The simulation tool can be configured to fetch a specified fixed-size asset and configured to include or exclude services like WAF or Workers in the request path. Below, we show the simulated performance between the three CPUs for an asset size of 10 KB, where &gt;1.00x indicates better performance. Milan 7713 Genoa 9654 Bergamo 9754 Genoa-X 9684X Lab simulation performance multiplier 1.00x 2.20x 1.95x 2.75x Based on these results, Bergamo 9754, which has the highest core count, but smallest L3 cache per core, is least performant among the three candidates, followed by Genoa 9654. The Genoa-X 9684X with the largest L3 cache per core is the most performant. This data suggests that our software stack is very sensitive to L3 cache size, in addition to core count and CPU frequency. This is interesting and worth a deep dive into a sensitivity analysis of our workload against a few (high level) CPU design points, especially core scaling, frequency scaling, and L2/L3 cache sizes scaling. Sensitivity analysis Core sensitivity Number of cores is the headline specification that practically everyone talks about, and one of the easiest improvements CPU vendors can make to increase performance per socket. The AMD Genoa 9654 has 96 cores, 50% more than the 64 cores available on the AMD Milan 7713 CPUs that we used in our Gen 11 servers. Is more always better? Does Cloudflare’s primary workload scale with core count and effectively utilize all available cores?The figure and table below shows the result of a core scaling experiment performed on an AMD Genoa 9654 configured with 96 cores, 80 cores, 64 cores, and 48 cores, which was done by incrementally disabling 2x CCD (8 cores/CCD) at each step. The result is GREAT, as Cloudflare’s simulated primary workload scales linearly with core count on AMD Genoa CPUs. Core count Core increase Performance increase 48 1.00x 1.00 64 1.33x 1.39x 80 1.67x 1.71x 96 2.00x 2.05x TDP sensitivity Thermal Design Power (TDP), is the maximum amount of heat generated by a CPU that the cooling system is designed to dissipate, but more commonly refers to the power consumption of the processor under the maximum theoretical loads. AMD Genoa 9654’s default TDP is 360W, but can be configured up to 400W TDP. Is more always better? Does Cloudflare continue to see meaningful performance improvement up to 400W, or does performance stagnate at some point?The chart below shows the result of sweeping the TDP of the AMD Genoa 9654 (in power determinism mode) from 240W to 400W. (Note: x-axis step size is not linear). Cloudflare’s simulated primary workload continues to see incremental performance improvements up to the maximum configurable 400W, albeit at a less favorable perf/watt ratio.Looking at TDP sensitivity data is a quick and easy way to identify if performance stagnates at some power point, but what does power sensitivity actually measure? There are several factors contributing to CPU power consumption, but let's focus on one of the primary factors: dynamic power consumption. Dynamic power consumption is approximately CV2f, where C is the switched load capacitance, V is the regulated voltage, and f is the frequency. In modern processors like the AMD Genoa 9654, the CPU dynamically scales its voltage along with frequency, so theoretically, CPU dynamic power is loosely proportional to f3. In other words, measuring TDP sensitivity is measuring the frequency sensitivity of a workload. Does the data agree? Yes! cTDP All core boost frequency (GHz) Perf (rps) / baseline 240 2.47 0.78x 280 2.75 0.87x 320 2.93 0.93x 340 3.13 0.97x 360 3.3 1.00x 380 3.4 1.03x 390 3.465 1.04x 400 3.55 1.05x Frequency sensitivity Instead of relying on an indirect measure through the TDP, let’s measure frequency sensitivity directly by sweeping the maximum boost frequency. At above 3GHz, the data shows that Cloudflare’s primary workload sees roughly 2% incremental improvement for every 0.1GHz all core average frequency increment. We hit the 400W power cap at 3.545GHz. This is notably higher than the typical all core boost frequency that Cloudflare Gen 11 servers with AMD Milan 7713 at 2.7GHz see in production, or at 2.4GHz in our performance simulation, which is amazing! L3 cache size sensitivity What about L3 cache size sensitivity? L3 cache size is one of the primary design choices and major differences between the trio of Genoa, Bergamo, and Genoa-X. Genoa 9654 has 4 MB L3/core, Bergamo 9754 has 2 MB L3/core, and Genoa-X has 12 MB L3/core. L3 cache is the last and largest “memory” bank on-chip before having to access memory on DIMMs outside the chip that would take significantly more CPU cycles.We ran an experiment on the Genoa 9654 to check how performance scales with L3 cache size. L3 cache size per core is reduced through MSR writes (but could also be done using Intel RDT) and L3 cache per core is increased by disabling physical cores in a CCD (which reduces the number of cores sharing the fixed size 32 MB L3 cache per CCD effectively growing the L3 cache per core). Below is the result of the experiment, where &gt;1.00x indicates better performance: L3 cache size increase vs baseline 4MB per core 0.25x 0.5x 0.75x 1x 1.14x 1.33x 1.60x 2.00x rps/core / baseline 0.67x 0.78x 0.89x 1.00x 1.08x 1.15x 1.25x 1.31x L3 cache miss rate per CCD 56.04% 39.15% 30.37% 23.55% 22.39% 19.73% 16.94% 14.28% Even though the expectation was that the impact of a different L3 cache size gets diminished by the faster DDR5 and larger memory bandwidth, Cloudflare’s simulated primary workload is quite sensitive to L3 cache size. The L3 cache miss rate dropped from 56% with only 1 MB L3 per core, to 14.28% with 8 MB L3/core. Changing the L3 cache size by 25% affects the performance by approximately 11%, and we continue to see performance increase to 2x L3 cache size, though the performance increase starts to diminish when we get to 2x L3 cache per core.Do we see the same behavior when comparing Genoa 9654, Bergamo 9754 and Genoa-X 9684X? We ran an experiment comparing the impact of L3 cache size, controlling for core count and all core boost frequency, and we also saw significant deltas. Halving the L3 cache size from 4 MB/core to 2 MB/core reduces performance by 24%, roughly matching the experiment above. However, increasing the cache 3x from 4 MB/core to 12 MB/core only increases performance by 25%, less than the indication provided by previous experiments. This is likely because the performance gain we saw on experiment result above could be partially attributed to less cache contention due to reduced number of cores based on how we set up the test. Nevertheless, these are significant deltas! L3/core 2MB/core 4MB/core 12MB/core Perf (rps) / baseline 0.76x 1x 1.25x Putting it all together The table below summarizes how each factor from sensitivity analysis above contributes to the overall performance gain. There are an additional 6% to 14% of unaccounted performance improvement that are contributed by other factors like larger L2 cache, higher memory bandwidth, and miscellaneous CPU architecture changes that improve IPC. Milan 7713 Genoa 9654 Bergamo 9754 Genoa-X 9684X Lab simulation performance multiplier 1x 2.2x 1.95x 2.75x Performance multiplier due to Core scaling 1x 1.5x 2x 1.5x Performance multiplier due to Frequency scaling (*Note: Milan 7713 all core frequency is ~2.4GHz when running simulated workload at 100% CPU utilization) 1x 1.32x 1.21x 1.29x Performance multiplier due to L3 cache size scaling 1x 1x 0.76x 1.25x Performance multiplier due to other factors like larger L2 cache, higher memory bandwidth, miscellaneous CPU architecture changes that improve IPC 1x 1.11x 1.06x 1.14x Performance evaluation in production How do these CPU candidates perform with real-world traffic and an actual production workload mix? The table below summarizes the performance of the three CPUs in lab simulation and in production. Genoa-X 9684X continues to outperform in production.In addition, the Gen 12 server equipped with Genoa-X offered outstanding performance but only consumed 1.5x more power per system than our Gen 11 server with Milan 7713. In other words, we see a 63% increase in performance per watt. Genoa-X 9684X provides the best TCO improvement among the 3 options, and was ultimately chosen as the CPU for our Gen 12 server. Milan 7713 Genoa 9654 Bergamo 9754 Genoa-X 9684X Lab simulation performance multiplier 1x 2.2x 1.95x 2.75x Production performance multiplier 1x 2x 2.15x 2.45x Production performance per watt multiplier 1x 1.33x 1.38x 1.63x The Gen 12 server with AMD Genoa-X 9684X is the most powerful and the most power efficient server Cloudflare has built to date. It serves as the underlying platform for all the incredible services that Cloudflare offers to our customers globally, and will help power the growth of Cloudflare infrastructure for the next several years with improved cost structure. Hardware engineers at Cloudflare work closely with our infrastructure engineering partners and externally with our vendors to design and develop world-class servers to best serve our customers. Come join us at Cloudflare to help build a better Internet! Analysis of the EPYC 145% performance gain in Cloudflare Gen 12 servershttps://blog.cloudflare.com/analysis-of-the-epyc-145-performance-gain-in-cloudflare-gen-12-servers","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"Protect Against Identity-Based Attacks by Sharing Cloudflare User Risk Scores With Okta","path":"/RSSBOX/rss/9b6a20e3.html","content":"Cloudflare One, our secure access service edge (SASE) platform, is introducing a new integration with Okta, the identity and access management (IAM) vendor, to share risk indicators in real-time and simplify how organizations can dynamically manage their security posture in response to changes across their environments.For many organizations, it is becoming increasingly challenging and inefficient to adapt to risks across their growing attack surface. In particular, security teams struggle with multiple siloed tools that fail to share risk data effectively with each other, leading to excessive manual effort to extract signals from the noise. To address this complexity, Cloudflare launched risk posture management capabilities earlier this year to make it easier for organizations to accomplish three key jobs on one platform: Evaluating risk posed by people by using first-party user entity and behavior analytics (UEBA) modelsExchanging risk telemetry with best-in-class security tools, andEnforcing risk controls based on those dynamic first- and third-party risk scores.Today’s announcement builds on these capabilities (particularly job #2) and our partnership with Okta by enabling organizations to share Cloudflare’s real-time user risk scores with Okta, which can then automatically enforce policies based on that user’s risk. In this way, organizations can adapt to evolving risks in less time with less manual effort. Cloudflare’s user risk scoring Introduced earlier this year, Cloudflare’s user risk scoring analyzes real-time telemetry of user activities and behaviors and assigns a risk score of high, medium, or low. For example, if Cloudflare detects risky or suspicious activity from a user — such as impossible travel, where a user logs in from multiple geographically dispersed locations within a short time frame, data loss prevention (DLP) detections, or endpoint detections suggesting that the device is infected — the user’s risk score will increase. The activity leading to that scoring is logged for analysis.Cloudflare includes predefined risk behaviors to help you get started. Administrators can create policies based on specific risk behaviors and adjust the risk level for each behavior based on their company’s tolerance. Share risk scores with Okta and take action automatically Customers that opt in to this new integration will be able to share continually updated Cloudflare user risk scores with Identity Threat Protection with Okta AI. If a user is deemed too risky, Okta will automatically take action to mitigate the risk, such as enforcing multi-factor authentication (MFA) verification or universally logging the user out from all applications. For example, a user has a low risk score from Cloudflare that was shared with Okta, but after exhibiting “impossible travel” behavior, the user’s risk level is raised to high. Cloudflare sends the updated score to Okta, which triggers a Universal Logout and an MFA challenge if the user attempts to log in again. Access to sensitive systems may be revoked completely until the user is verified. How it works: continuous risk evaluation and exchange Figure 1. Diagram showing risky behavior by a user, resulting in sign-out.We begin by detecting risky behavior from a user (such as an “impossible travel” event between two geographic locations). Instances of risky behavior are called Risk Events. We perform two actions when we observe a Risk Event: logging the event and evaluating whether further action is required. For customers that have enabled Risk Score Sharing with Okta, any change in Risk Score is transmitted to Okta’s Identity Threat Protection (ITP).Upon receiving a new event, Okta evaluates the change in user risk against the organization's policies. These policies may include actions such as re-authenticating the user if they become high risk.When we design new features, we aim for them to be extensible across the industry. For this reason, we chose the OpenID Shared Signals Framework Specification (SSF) to be the foundation of our transmission format. By doing this, we are able to leverage current and future providers that support the standard. The core functionality of SSF revolves around sharing Security Event Tokens (SETs), a specialized version of a JSON Web Token (JWT). Providers can produce and consume Security Event Tokens, forming a “network” of shared user risk information between providers. Figure 2. Diagram showing a Security Event Token being transmitted from Cloudflare to Okta.The diagram above (Figure 2) details the process of sharing risk. When sharing Risk Score changes with Okta, we bundle metadata about the risk event and user into the body of a Security Event Token. Following this, the JWT/SET is signed using our private key. This is an important step, as the signature is used to verify the sender's identity (cryptographic authenticity) and that the payload body has not been tampered with (cryptographic integrity). In plain terms, this signature is used by Okta to verify that the event is unaltered and was sent by Cloudflare.Once Okta has verified the authenticity and integrity of the SET token, they may use the risk metadata within the body to execute Identity Threat Protection policies defined by the customer. These policies could include actions such as “if a high risk score is received from Cloudflare, sign out the offending user”.Learn more about the Shared Signals Framework and CAEP in Okta’s announcement blog post. Get started today Cloudflare customers can easily enable risk score sharing from the Cloudflare One SSO setup page. This is available to customers whether you’ve already integrated with Okta or are setting up the integration for the first time. You will also be able to confirm that the feature was enabled in your audit logs.If you’ve already integrated Okta within your Cloudflare One dashboard:As an admin, navigate to Settings &gt; Authentication and select the Okta login method.Select “send risk score to Okta.”If you haven’t yet integrated Okta within your Cloudflare One dashboard:As an admin, navigate to Settings &gt; Authentication and select a new login method.Follow the instructions to add Okta as an SSO.Select “send risk score to Okta.”Now, whenever a user’s risk score changes within the organization, information is sent to Okta automatically and an audit log is documented. Uphold Zero Trust principles In conclusion, the ability to incorporate rich context is essential for making accurate and informed access decisions. With vast amounts of data — including user logins, logouts, websites visited, and emails sent — human analysts would struggle to keep pace with modern security challenges. Cloudflare provides context in the form of a risk score, enabling Okta’s risk engine to make more informed policy decisions about users. This sharing of information powers the continuous evaluation required to enforce Zero Trust policies within your organization, ultimately strengthening your organization’s security posture.Not yet a Cloudflare One customer? Reach out for a consultation or contact your account manager. Protect against identity-based attacks by sharing Cloudflare user risk scores with Oktahttps://blog.cloudflare.com/protect-against-identity-based-attacks-by-sharing-cloudflare-user-risk-with-okta","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"1.21.2-Pre4 快照更新","path":"/RSSBOX/rss/b1450003.html","content":"1.21.2-pre4 快照更新 1.21.2-pre4 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"FIND the 10 ERRORS","path":"/RSSBOX/rss/58993a10.html","content":"#minecraftshorts #minecraft #minecraftquiz #shortsvideo #gaming FIND THE 10 ERRORShttps://www.youtube.com/watch?v=fs9YlqjUEQw","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"1.21.2-Pre3 快照更新","path":"/RSSBOX/rss/d488aac3.html","content":"1.21.2-pre3 快照更新 1.21.2-pre3 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"DO YOU KNOW THESE MINECRAFT SOUNDS? - PART 4","path":"/RSSBOX/rss/5567284c.html","content":"#minecraft #minecraftshorts #shortvideo #shortsvideo #ytshort #ytshorts #minecraftsounds #sounds DO YOU KNOW THESE MINECRAFT SOUNDS? - PART 4https://www.youtube.com/watch?v=1kWIxElCKr8","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"CJ Desai: Why I Joined Cloudflare as President of Product and Engineering","path":"/RSSBOX/rss/96b5015b.html","content":"I am thrilled to embark on this journey to run Product and Engineering at Cloudflare, driving forward the mission of helping build a better Internet. A little about me While I was a graduate student at University of Illinois, the university introduced the Mosaic web browser to students. In addition to being super easy to install and use, it displayed pictures next to text for the first time. This may not seem impressive today, but back then it felt like a magical step forward.This simple but powerful upgrade opened up the once niche user base from academics to the masses, transforming the world wide web to become an Internet phenomenon. Since then, I’ve always sought to be part of teams that worked on transformational technologies, including Software-as-a-Service, cloud computing, and AI. Innovation is the life blood of every technology company. To this day, I’m inspired by building products and technology that get adopted at mass scale. Why Cloudflare The world is in a very interesting moment for technological innovation: the AI landscape is uncharted and developing at an exponential rate; the urgency for enterprises to reduce tech debt and reliance on legacy applications is at an all time high; multi-cloud deployments are becoming a reality for optimal performance and global scale; and high performance connectivity is table stakes. Cloudflare finds itself in a compelling position at the intersection of these key themes. We are uniquely poised to drive disruption and empower customers to do the same as they transform their businesses, and I’m excited to use my experience to help move our mission forward. Organic growth via a platform approach In order to reach massive scale, platforms need to be extensible (i.e., they need to be able to grow easily). The best way to do this is by knowing your customers’ biggest pain points and improving the core experience while creating adjacent products that solve their most challenging problems. Combined with being obsessed with your customers getting value and having a deep understanding of how the technology landscape is evolving around you, it becomes a powerful growth strategy.This was the approach that I took during my time at ServiceNow, and one of the reasons why we were able to build the product business from $1.5 billion to $10+ billion in annualized revenue.Cloudflare’s connectivity cloud — an innovative cloud native platform of security, network connectivity, and developer solutions — provides a unique foundation for driving value, no matter the industry. I’m excited to help Cloudflare further grow and refine our product offerings, while ensuring they are purpose-built and scalable to meet the needs of our growing customer base. Together, we can create solutions that empower any organization to enhance its global online presence while maintaining security and performance at the forefront. Putting customers first It was very clear as I met many amazing people at Cloudflare, including co-founders Matthew Prince and Michelle Zatlyn, that the team takes pride in solving deep technical Internet infrastructure problems while putting customers first. In addition to working with world-class product and engineering teams, I am looking forward to collaborating with the wider business, our partners, and our customers to realize the potential of Cloudflare's people, platform, and organic growth. I’m looking forward to helping build a better Internet with Cloudflare! CJ Desai: Why I joined Cloudflare as President of Product and Engineeringhttps://blog.cloudflare.com/cj-desai-why-i-joined-cloudflare","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"1.21.2-Pre2 快照更新","path":"/RSSBOX/rss/49874bb5.html","content":"1.21.2-pre2 快照更新 1.21.2-pre2 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"GitHub Availability Report: September 2024","path":"/RSSBOX/rss/85d08535.html","content":"In September, we experienced three incidents that resulted in degraded performance across GitHub services. September 16 21:11 UTC (lasting 57 minutes) On September 16, 2024, between 21:11 UTC and 22:08 UTC, GitHub Actions and GitHub Pages services were degraded. Customers who deploy Pages from a source branch experienced delayed runs. We determined the root cause to be a misconfiguration in the service that manages runner connections that led to CPU throttling and performance degradation in that service. Actions jobs experienced average delays of 23 minutes, with some jobs experiencing delays as high as 45 minutes. During the course of the incident, 17% of runs were delayed by more than five minutes. At peak, as many as 80% of runs experienced delays exceeding five minutes. We mitigated the incident by diverting runner connections away from the misconfigured nodes, starting at 21:16 UTC. In addition to addressing the configuration issue we discovered through this, we have improved our general monitoring to reduce the risk of a similar recurrence and reduce our time to automated detection and mitigation of issues like this in the future. September 24 08:20 UTC (lasting 44 minutes) On September 24, 2024 from 08:20 UTC to 09:04 UTC the GitHub Codespaces service experienced an interruption in network connectivity, leading to an approximate 25% error rate for the outage period. We traced the cause to an interruption in network connectivity caused by Source Network Address Translation (SNAT) port exhaustion following a deployment, causing individual codespaces to lose their connection to the service. To mitigate the impact, we increased port allocations to give enough buffer for increased outbound connections shortly after deployments. We will be scaling up our outbound connectivity in the near future, as well as adding improved monitoring of network capacity to prevent future regressions. September 30 10:43 UTC (lasting 43 minutes) On September 30, 2024 from 10:43 UTC to 11:26 UTC GitHub Codespaces customers in the Central India region were unable to create new codespaces. Resumes were not impacted and there was no impact to customers in other regions. We traced the cause to storage capacity constraints in the region and mitigated by temporarily redirecting create requests to other regions. Afterwards, we added additional storage capacity to the region and traffic was routed back. We also identified a bug that caused some available capacity not to be utilized, artificially constraining capacity and halting creations in the region prematurely. We have since fixed this bug as well so that available capacity scales as expected according to our capacity planning projections. Please follow our status page for real-time updates on status changes and post-incident recaps. To learn more about what we’re working on, check out the GitHub Engineering Blog. The post GitHub Availability Report: September 2024 appeared first on The GitHub Blog. GitHub Availability Report: September 2024https://github.blog/news-insights/company-news/github-availability-report-september-2024/","tags":["Blogs","Github"],"categories":["Blogs","Github"]},{"title":"What’s New in Cloudflare One: Digital Experience (DEX) Monitoring Notifications and Seamless Access to Cloudflare Gateway With China Express","path":"/RSSBOX/rss/86f9c1ad.html","content":"At Cloudflare, we are constantly innovating and launching new features and capabilities across our product portfolio. We are introducing roundup blog posts to ensure that you never miss the latest updates across our platform. In this post, we are excited to share two new ways that our customers can continue to keep their web properties performant and secure with Cloudflare One: new Digital Experience Monitoring (DEX) notifications help proactively identify issues that can affect the end-user digital experience, and integration with China Express enables secure access to China-hosted sites for Cloudflare Gateway customers. Using DEX Notifications for proactive monitoring with Cloudflare Zero Trust Digital Experience Monitoring (DEX) offers device, application, and network performance monitoring, providing IT administrators with insights to quickly identify and resolve issues. With DEX notifications , account administrators can create configurable alert rules based on available algorithms (z-score, SLO) and existing DEX filters. When notification criteria are satisfied, customers are notified via email, Pagerduty, or WebhooksAs with other notification types, DEX notifications can be configured and reviewed from Cloudflare dashboard notifications. What problem does it solve? DEX notifications address the challenge of proactively identifying issues affecting the digital experience of your end users. By monitoring device health and conducting synthetic tests from WARP clients deployed on your fleet's end-user devices, DEX provides valuable insights. These notifications empower IT administrators to quickly identify and address connectivity and application performance problems before they impact a wide range of users.By proactively notifying administrators when problems arise, DEX helps minimize user disruption and provides peace of mind. Instead of actively refreshing and looking for issues as before, administrators can now receive immediate notifications. Management is simple, as notifications can be easily configured through the Cloudflare dashboard.Administrators can now create three new notification types:1) Device Connectivity AnomalyAre you tired of manually monitoring your end-users' device connectivity? Do you want to be notified immediately when there's a sudden change? Our new DEX notification for Device Connectivity Anomaly alerts you when there's a significant increase or decrease in the number of monitored devices connecting or disconnecting to the WARP Client. This can be filtered by various characteristics such as data center (“colo”), platform (operating system), and WARP Client version.We use a statistical method called z-score to detect anomalies in monitored device count. A z-score measures how many standard deviations a data point is from the mean. By comparing the current five minutes of data to the past four hours, we can calculate the mean and standard deviation. If the z-score value exceeds 3.5 or falls below -3.5, a notification is triggered.Here's an example of a notification configuration for macOS devices in the UK using WARP Client version 2023.7.24: 2) DEX Test Latency Ever worry application performance is slow? We're thrilled to introduce DEX Test Latency notifications, which are designed for administrators who want to stay ahead of the curve when it comes to application performance. This notification proactively alerts you of significant spikes or drops in latency based on:HTTP Test: Resource Fetch Time measures the time it takes for a web browser to retrieve a specific resource from your application and deliver it to the end user.Traceroute Test: Round Trip Time measures the average time it takes for data packets to travel from your device to a specific destination IP address and back (when successful). Traceroute tests focus on the overall network performance between the test client/device and your application.This notification can be filtered by various characteristics such as data center (“colo”), platform (operating system), WARP Client version, and test name.In this example, you have a DEX test monitoring the latency of the website www.cloudflarestatus.com. This test, named \"Cloudflare Status,\" uses an HTTP GET request and runs on Windows devices connecting through the Lisbon colo (data center). 3) DEX Test Low AvailabilityIs application downtime causing headaches for you and your users? DEX Test Low Availability notifications help maintain optimal application health by notifying you when availability falls below a given threshold. This notification monitors the success rate of HTTP or Traceroute requests sent to an application through pre-configured DEX tests. These synthetic tests simulate user traffic and measure the percentage of successful interactions with your application.You define the Service Level Objective (SLO) — a specific availability threshold — for each notification. When the percentage of successful requests falls below this threshold, you'll receive immediate notification, allowing you to proactively address issues before they impact a wide range of end users.This can be filtered by various characteristics such as colo (data center), platform (operating system), WARP Client version, and test name.In this example, a DEX test is targeting www.google.com. This Traceroute test runs on Chrome OS devices connecting through the Tel Aviv colo. The example notification is configured to alert you whenever the availability (percentage of successful requests) drops below 98%, allowing you to investigate potential issues and take corrective action quickly. Get started today DEX notifications are now available for Cloudflare One customers. They can be configured by going to Cloudflare Dashboard &gt; Account home &gt; Notifications &gt; Add, and then selecting any of the three DEX notification types. For more information, refer to Create a notification. DEX notifications are one of the many ways the Cloudflare One suite of solutions work seamlessly together as a unified platform to find and fix security issues across SaaS applications. Get started now with Cloudflare’s Zero Trust platform by signing up here. Seamless access to Cloudflare Gateway with China Express In January 2023, we proudly launched China Express with multiple partners in China to extend Cloudflare One into China and provide connectivity to ensure that customers within the country could enjoy the same level of access to global services as the rest of the world. Our goal was simple: to deliver a consistent experience for customers and employees everywhere.Over the past year, we've observed a notable increase in demand from enterprise customers seeking secure access to China-hosted sites. These customers, who often require consistent zero trust security policies applied through Cloudflare Gateway, including device posture checks, have faced challenges like scenic routing, where Internet traffic passes through multiple countries or networks, leading to significant packet loss when connecting to these websites. Understanding the problem For example, a global company with offices in both Hong Kong and San Jose has implemented Cloudflare One to implement a unified Zero Trust platform globally, with all employees using WARP on their devices to manage Internet access. As part of their daily operations, employees need to access websites hosted in mainland China. However, they have experienced unstable connections, particularly when accessing the AWS web console in China. Further investigation revealed long and sometimes unpredictable network routes, contributing to the instability.Global Internet traffic to and from China flows through a limited number of international links, tightly regulated by government authorities, often leading to significant instability and fluctuations. To address these challenges, our China Express partners offer the 'Reverse Tunnel' solution, a reliable service that ensures stable access to Chinese websites, effectively mitigating connectivity issues. Reverse tunnel Today, we are thrilled to announce a significant enhancement to China Express: a new offering tailored to the needs of global Cloudflare Gateway customers accessing China-hosted sites. This enhancement introduces a dedicated tunnel configuration, ensuring safe and predictable connectivity while maintaining stringent zero trust security policies.By partnering with JD Cloud, one of our trusted local providers in China, we've developed a solution that seamlessly integrates with Cloudflare's Zero Trust Firewall DNS Policies by:Directly routing through our Cloudflare Hong Kong data center: When global Cloudflare Gateway customers attempt to access China-hosted sites, their traffic is routed directly to our Hong Kong data center. This strategic routing point allows us to apply Zero Trust policies before the traffic continues its journey into China.Using JD Cloud's connectivity tunnel: From our Cloudflare Hong Kong data center, the traffic is then securely transmitted through JD Cloud's private tunnel infrastructure, ensuring reliable and efficient connectivity into China. This partnership with JD Cloud leverages their local expertise and infrastructure capabilities, further enhancing the reliability and performance of the connection.Note: This premium service is exclusive to China Network customers and requires a dedicated reverse tunnel contract with JD Cloud. Key benefits This solution offers several key benefits for our customers:Improved stability: By directing all traffic to a dedicated tunnel, customers experience more reliable connections to websites within China.Enhanced security: Zero Trust policies are consistently applied to all traffic, regardless of its destination, ensuring the highest level of security for customers accessing China-hosted sites.Seamless customer experience: With a dedicated tunnel configuration, customers can access websites in China with confidence, knowing that their connections are both safe and predictable. Whether it’s multinational corporations expanding into the Chinese market, e-commerce platforms serving Chinese customers, or remote workers accessing corporate resources from within China, Cloudflare's China Express with JD Cloud partnership provides a solution tailored to their specific needs. Conclusion By having companies implement a DNS host override policy in Cloudflare Gateway for origins in China, which routes traffic through the China Express Reverse Tunnel instead of using public Internet routes, companies can ensure more stable and reliable connections for their employees.Looking ahead, we remain committed to continuously improving and expanding our offerings within China Express. Future developments may include further enhancements to performance, additional partnerships with local providers, and ongoing innovation to meet the evolving needs of our customers in the region. Never Miss an Update We’ll continue to share roundup blog posts as we continue to build and innovate. Be sure to follow along on the Cloudflare Blog for the latest news and updates. What’s new in Cloudflare One: Digital Experience (DEX) monitoring notifications and seamless access to Cloudflare Gateway with China Expresshttps://blog.cloudflare.com/roundup-dex-alerts-cloudflare-gateway-china-express","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"Improving Platform Resilience at Cloudflare Through Automation","path":"/RSSBOX/rss/1f0f88f8.html","content":"Failure is an expected state in production systems, and no predictable failure of either software or hardware components should result in a negative experience for users. The exact failure mode may vary, but certain remediation steps must be taken after detection. A common example is when an error occurs on a server, rendering it unfit for production workloads, and requiring action to recover.When operating at Cloudflare’s scale, it is important to ensure that our platform is able to recover from faults seamlessly. It can be tempting to rely on the expertise of world-class engineers to remediate these faults, but this would be manual, repetitive, unlikely to produce enduring value, and not scaling. In one word: toil; not a viable solution at our scale and rate of growth.In this post we discuss how we built the foundations to enable a more scalable future, and what problems it has immediately allowed us to solve. Growing pains The Cloudflare Site Reliability Engineering (SRE) team builds and manages the platform that helps product teams deliver our extensive suite of offerings to customers. One important component of this platform is the collection of servers that power critical products such as Durable Objects, Workers, and DDoS mitigation. We also build and maintain foundational software services that power our product offerings, such as configuration management, provisioning, and IP address allocation systems.As part of tactical operations work, we are often required to respond to failures in any of these components to minimize impact to users. Impact can vary from lack of access to a specific product feature, to total unavailability. The level of response required is determined by the priority, which is usually a reflection of the severity of impact on users. Lower-priority failures are more common — a server may run too hot, or experience an unrecoverable hardware error. Higher-priority failures are rare and are typically resolved via a well-defined incident response process, requiring collaboration with multiple other teams.The commonality of lower-priority failures makes it obvious when the response required, as defined in runbooks, is “toilsome”. To reduce this toil, we had previously implemented a plethora of solutions to automate runbook actions such as manually-invoked shell scripts, cron jobs, and ad-hoc software services. These had grown organically over time and provided solutions on a case-by-case basis, which led to duplication of work, tight coupling, and lack of context awareness across the solutions.We also care about how long it takes to resolve any potential impact on users. A resolution process which involves the manual invocation of a script relies on human action, increasing the Mean-Time-To-Resolve (MTTR) and leaving room for human error. This risks increasing the amount of errors we serve to users and degrading trust.These problems proved that we needed a way to automatically heal these platform components. This especially applies to our servers, for which failure can cause impact across multiple product offerings. While we have mechanisms to automatically steer traffic away from these degraded servers, in some rare cases the breakage is sudden enough to be visible. Solving the problem To provide a more reliable platform, we needed a new component that provides a common ground for remediation efforts. This would remove duplication of work, provide unified context-awareness and increase development speed, which ultimately saves hours of engineering time and effort.A good solution would not allow only the SRE team to auto-remediate, it would empower the entire company. The key to adding self-healing capability was a generic interface for all teams to self-service and quickly remediate failures at various levels: machine, service, network, or dependencies.A good way to think about auto-remediation is in terms of workflows. A workflow is a sequence of steps to get to a desired outcome. This is not dissimilar to a manual shell script which executes what a human would otherwise do via runbook instructions. Because of this logical fit with workflows, we decided to adopt Temporal. Temporal is a durable execution platform which is useful to gracefully manage infrastructure failures such as network outages and transient failures in external service endpoints. This capability meant we only needed to build a way to schedule “workflow” tasks and have Temporal provide reliability guarantees. This allowed us to focus on building out the orchestration system to support the control and flow of workflow execution in our data centers. Temporal’s documentation provides a good introduction to writing Temporal workflows. Building an Automatic Remediation System Below, we describe how our automatic remediation system works. It is essentially a way to schedule tasks across our global network with built-in reliability guarantees. With this system, teams can serve their customers more reliably. An unexpected failure mode can be recognized and immediately mitigated, while the root cause can be determined later via a more detailed analysis. Step one: we need a coordinator After our initial testing of Temporal, it was now possible to write workflows. But we needed a way to schedule workflow tasks from other internal services. The coordinator was built to serve this purpose, and became the primary mechanism for the authorisation and scheduling of workflows. The most important roles of the coordinator are authorisation, workflow task routing, and safety constraints enforcement. Each consumer is authorized via mTLS authentication, and the coordinator uses an ACL to determine whether to permit the execution of a workflow. An ACL configuration looks like the following example. server_config &#123; enable_tls = true [...] route_rule &#123; name = \"global_get\" method = \"GET\" route_patterns = [\"/*\"] uris = [\"spiffe://example.com/worker-admin\"] &#125; route_rule &#123; name = \"global_post\" method = \"POST\" route_patterns = [\"/*\"] uris = [\"spiffe://example.com/worker-admin\"] allow_public = true &#125; route_rule &#123; name = \"public_access\" method = \"GET\" route_patterns = [\"/metrics\"] uris = [] allow_public = true skip_log_match = true &#125; &#125; Each workflow specifies two key characteristics: where to run the tasks and the safety constraints, using an HCL configuration file. Example constraints could be whether to run on only a specific node type (such as a database), or if multiple parallel executions are allowed: if a task has been triggered too many times, that is a sign of a wider problem that might require human intervention. The coordinator uses the Temporal Visibility API to determine the current state of the executions in the Temporal cluster.An example of a configuration file is shown below: task_queue_target = \"&lt;target&gt;\" The following entries will ensure that1. This workflow is not run at the same time in a 15m window.2. This workflow will not run more than once an hour.3. This workflow will not run more than 3 times in one day.constraint &amp;#123; kind &#x3D; “concurency” value &#x3D; “1” period &#x3D; “15m”&amp;#125; constraint &amp;#123; kind &#x3D; “maxExecution” value &#x3D; “1” period &#x3D; “1h”&amp;#125; constraint &amp;#123; kind &#x3D; “maxExecution” value &#x3D; “3” period &#x3D; “24h” is_global &#x3D; true&amp;#125; Step two: Task Routing is amazing An unforeseen benefit of using a central Temporal cluster was the discovery of Task Routing. This feature allows us to schedule a Workflow/Activity on any server that has a running Temporal Worker, and further segment by the type of server, its location, etc. For this reason, we have three primary task queues — the general queue in which tasks can be executed by any worker in the datacenter, the node type queue in which tasks can only be executed by a specific node type in the datacenter, and the individual node queue where we target a specific node for task execution.We rely on this heavily to ensure the speed and efficiency of automated remediation. Certain tasks can be run in datacenters with known low latency to an external resource, or a node type with better performance than others (due to differences in the underlying hardware). This reduces the amount of failure and latency we see overall in task executions. Sometimes we are also constrained by certain types of tasks that can only run on a certain node type, such as a database.Task Routing also means that we can configure certain task queues to have a higher priority for execution, although this is not a feature we have needed so far. A drawback of task routing is that every Workflow/Activity needs to be registered to the target task queue, which is a common gotcha. Thankfully, it is possible to catch this failure condition with proper testing. Step three: when/how to self-heal? None of this would be relevant if we didn’t put it to good use. A primary design goal for the platform was to ensure we had easy, quick ways to trigger workflows on the most important failure conditions. The next step was to determine what the best sources to trigger the actions were. The answer to this was simple: we could trigger workflows from anywhere as long as they are properly authorized and detect the failure conditions accurately.Example triggers are an alerting system, a log tailer, a health check daemon, or an authorized engineer via a chatbot. Such flexibility allows a high level of reuse, and permits to invest more in workflow quality and reliability.As part of the solution, we built a daemon that is able to poll a signal source for any unwanted condition and trigger a configured workflow. We have initially found Prometheus useful as a source because it contains both service-level and hardware/system-level metrics. We are also exploring more event-based trigger mechanisms, which could eliminate the need to use precious system resources to poll for metrics.We already had internal services that are able to detect widespread failure conditions for our customers, but were only able to page a human. With the adoption of auto-remediation, these systems are now able to react automatically. This ability to create an automatic feedback loop with our customers is the cornerstone of these self-healing capabilities and we continue to work on stronger signals, faster reaction times, and better prevention of future occurrences.The most exciting part, however, is the future possibility. Every customer cares about any negative impact from Cloudflare. With this platform we can onboard several services (especially those that are foundational for the critical path) and ensure we react quickly to any failure conditions, even before there is any visible impact. Step four: packaging and deployment The whole system is written in golang, and a single binary can implement each role. We distribute it as an apt package or a container for maximum ease of deployment.We deploy a Temporal-based worker to every server we intend to run tasks on, and a daemon in datacenters where we intend to automatically trigger workflows based on the local conditions. The coordinator is more nuanced since we rely on task routing and can trigger from a central coordinator, but we have also found value in running coordinators locally in the datacenters. This is especially useful in datacenters with less capacity or degraded performance, removing the need for a round-trip to schedule the workflows. Step five: test, test, test Temporal provides native mechanisms to test an entire workflow, via a comprehensive test suite that supports end-to-end, integration, and unit testing, which we used extensively to prevent regressions while developing. We also ensured proper test coverage for all the critical platform components, especially the coordinator.Despite the ease of written tests, we quickly discovered that they were not enough. After writing workflows, engineers need an environment as close as possible to the target conditions. This is why we configured our staging environments to support quick and efficient testing. These environments receive the latest changes and point to a different (staging) Temporal cluster, which enables experimentation and easy validation of changes.After a workflow is validated in the staging environment, we can then do a full release to production. It seems obvious, but catching simple configuration errors before releasing has saved us many hours in development/change-related-task time. Deploying to production As you can guess from the title of this post, we put this in production to automatically react to server-specific errors and unrecoverable failures. To this end, we have a set of services that are able to detect single-server failure conditions based on analyzed traffic data. After deployment, we have successfully mitigated potential impact by taking any errant single sources of failure out of production.We have also created a set of workflows to reduce internal toil and improve efficiency. These workflows can automatically test pull requests on target machines, wipe and reset servers after experiments are concluded, and take away manual processes that cost many hours in toil.Building a system that is maintained by several SRE teams has allowed us to iterate faster, and rapidly tackle long-standing problems. We have set ambitious goals regarding toil elimination and are on course to achieve them, which will allow us to scale faster by eliminating the human bottleneck. Looking to the future Our immediate plans are to leverage this system to provide a more reliable platform for our customers and drastically reduce operational toil, freeing up engineering resources to tackle larger-scale problems. We also intend to leverage more Temporal features such as Workflow Versioning, which will simplify the process of making changes to workflows by ensuring that triggered workflows run expected versions. We are also interested in how others are solving problems using durable execution platforms such as Temporal, and general strategies to eliminate toil. If you would like to discuss this further, feel free to reach out on the Cloudflare Community and start a conversation!If you’re interested in contributing to projects that help build a better Internet, our engineering teams are hiring. Improving platform resilience at Cloudflare through automationhttps://blog.cloudflare.com/improving-platform-resilience-at-cloudflare","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"Highlights From Git 2.47","path":"/RSSBOX/rss/1666e8e3.html","content":"The open source Git project just released Git 2.47 with features and bug fixes from over 83 contributors, 28 of them new. We last caught up with you on the latest in Git back when 2.46 was released. To celebrate this most recent release, here is GitHub’s look at some of the most interesting features and changes introduced since last time. Incremental multi-pack indexes Returning readers of this series will no doubt remember our coverage of all things related to multi-pack indexes (MIDXs). If you’re new here, or could use a refresher, here’s a brief recap. Git stores objects (the blobs, trees, commits, and tags that make up your repository’s contents) in one of two formats: either loose or packed. Loose objects are the individual files stored in the two-character sub-directories of $GIT_DIR/objects, each representing a shard of the total set of loose objects. For instance, the object 08103b9f2b6e7fbed517a7e268e4e371d84a9a10 would be stored loose at $GIT_DIR/objects/08/103b9f2b6e7fbed517a7e268e4e371d84a9a10. Objects can also be packed together in a single file known as a packfile. Packfiles store multiple objects together in a binary format, which has a couple of advantages over storing objects loose. Packfiles often have better cache locality because similar objects are often packed next to or near each other. Packfiles also have the advantage of being able to represent objects as deltas of one another, enabling a more compact representation of pairs of similar objects. However, repositories can start to experience poor performance when they accumulate many packfiles, since Git has to search through each packfile to perform every object lookup. To improve performance when a repository accumulates too many packs, a repository must repack to generate a single new pack which contains the combined contents of all existing packs. This leaves the repository with only a single pack (resulting in faster lookup times), but the cost of generating that pack can be expensive. In Git 2.21, multi-pack indexes were introduced to mitigate this expense. MIDXs are an index mapping between objects to the pack and location within that pack at which they appear. Because MIDXs can store information about objects across multiple packs, they enable fast object lookups for repositories that have many individual packs, like so: Here the multi-pack index is shown as a series of colored rectangles, each representing an object. The arrows point to those objects’ location within the pack from which they were selected in the MIDX, and encode the information stored in the MIDX itself. But generating and updating the repository’s MIDX takes time, too: each object in the packs which are part of the MIDX need to be examined to record their object ID and offset within their source pack. This time can stretch even further if you are using multi-pack reachability bitmaps, since it adds a potentially large number of traversals covering significant portions of the repository to the runtime. So what is there to do? Repacking your repository to optimize object lookups can be slow, but so can updating your repository’s multi-pack index. Git 2.47 introduces a new experimental feature known as incremental multi-pack indexes, which allow storing more than one multi-pack index together in a chain of MIDX layers. Each layer contains packs and objects which are distinct from earlier layers, so the MIDX can be updated quickly via an append operation that only takes time proportional to the new objects being added, not the size of the overall MIDX. Here’s an example: The first half of the figure is the same as earlier, but the second half shows a new incremental layer in the multi-pack index chain. The objects contained in the MIDX on the second half are unique to the ones on the first half. But note that the source packs which appear in the MIDX on the second half have some overlap with the objects which appear in the MIDX on the first half. In Git 2.47, the incremental multi-pack index feature is still considered experimental, and doesn’t yet support multi-pack reachability bitmaps. But support for incremental multi-pack bitmaps is currently under review and will hopefully appear in a future release. (At GitHub, we plan to use incremental multi-pack bitmaps as part of further scaling efforts to support even larger repositories during repository maintenance. When we do, expect a blog post from us covering the details.) You can experiment with incremental multi-pack indexes by running: $ git multi-pack-index write --incremental to add new packs to your repository’s existing MIDX today. [source] Quickly find base branches with for-each-ref Have you ever been working on a branch, or spelunking through a new codebase and wondered to yourself, “what is this branch based on”? It’s a common question, but the answer can be surprisingly difficult to answer with the previously existing tools. A good approximation for determining what branch was the likely starting point for some commit C is to select the branch which minimizes the first-parent commits which are unique to C. (Here, “first parent commits” are the commits which are reachable by only walking through a merge commit’s first parent instead of traversing through all of its parents). If you’re wondering: “why limit the traversal to the first-parent history?”, the answer is because the first-parent history reflects the main path through history which leads up to a commit. By minimizing the number of unique first-parent commits among a set of candidate base branches, you are essentially searching for the one whose primary development path is closest to commit C. So the branch with the fewest unique first-parent commits is likely where C originated or was branched from. You might think that you could use something like git rev-list --count --first-parent to count the number of first-parent commits between two endpoints. But that’s not quite the case, since rev-list will remove all commits reachable from the base before returning the unique count. Git 2.47 introduces a new tool for figuring out which branch was the likely starting point for some commit via a new atom used in for-each-ref‘s --format specification. For example, let’s say I’m trying to figure out which branch name was picked for a topic I worked on upstream. $ needle=fcb2205b77470c60f996a3206b2d4aebf6e951e3 $ git for-each-ref --contains $needle refs/remotes/origin | wc -l 63 Naively searching for the set of branches which contain the thing I’m looking for can return many results, for example if my commit was merged and is now contained in many other branches. But the new %(is-base:) atom can produce the right answer: $ git for-each-ref --format=\"%(refname) %(is-base:$needle)\" refs/remotes/origin \\ | grep '(' refs/remotes/origin/tb/incremental-midx-part-1 (fcb2205b77470c60f996a3206b2d4aebf6e951e3) [source] Git is famously portable and compatible with a wide variety of systems and architectures, including some fairly exotic ones. But until this most recent release, Git has lacked a formal platform support policy. This release includes a new “Platform Support Policy” document which outlines Git’s official policy on the matter. The exact details can be found in the source link below, but the current gist is that platforms must have C99 or C11, use versions of dependencies which are stable or have long-term support, and must have an active security support system. Discussions about adding additional requirements, including possibly depending upon Rust in a future version, are ongoing. The policy also has suggestions for platform maintainers on which branches to test and how to report and fix compatibility issues. [source] A couple of releases ago, we discussed Git’s preliminary support for a new reference backend known as reftable. If you’re fuzzy on the details, our previous post is chock full of them. This release brings a number of unit tests which were written in the reftable implementation’s custom testing framework to Git’s standard unit test framework. These migrations were done by Chandra Pratap, one of the Git project’s Google Summer of Code (GSoC) contributors. This release also saw reftable gain better support when dealing with concurrent writers, particularly during stack compaction. The reftable backend also gained support for git for-each-ref’s –exclude option which we wrote about when Git 2.42 was released. [source, source, source, source, source, source, source, source, source, source, source, source] While we’re on the topic of unit testing, there were a number of other areas of the project which received more thorough unit test coverage, or migrated over existing test from Git’s Shell-based integration test suite. Git’s hashmap API, OID array, and urlmatch normalization features all were converted from Shell-based tests with custom helpers to unit tests. The unit test framework itself also received significant attention, ultimately resulting in using the Clar framework, which was originally written to replace the unit test framework in libgit2. Many of these unit test conversions were done by Ghanshyam Thakkar, another one of Git’s GSoC contributors. Congratulations, Ghanshyam! [source, source, source, source, source, source, source] While we’re on the topic of Google Summer of Code contributors, we should mention last (but not least!) another student, shejialuo, improved git fsck to check the reference storage backend for integrity in addition to the regular object store. They introduced a new git refs verify sub-command which is run through via git fsck, and catches many reference corruption issues. [source] Since at least 2019, there has been an effort to find and annotate unused parameters in functions across Git’s codebase. Annotating parameters as unused can help identify better APIs, and often the presence of an unused parameter can point out a legitimate bug in that function’s implementation. For many years, the Git project has sought to compile with -Wunused-parameter under its special DEVELOPER=1 mode, making it a compile-time error to have or introduce any unused parameters across the codebase. During that time, there have been many unused parameter cleanups and bug fixes, all done while working around other active development going on in related areas. In this release, that effort came to a close. Now when compiling with DEVELOPER=1, it is now a compile-time error to have unused parameters, making Git’s codebase cleaner and safer going forward. [source, source, source, source, source, source, source, source] Way back when Git 2.34 was released, we covered a burgeoning effort to find and fix memory leaks throughout the Git codebase. Back then, we wrote that since Git typically has a very short runtime, it is much less urgent to free memory than it is in, say, library code, since a process’s memory will be “freed” by the operating system when the process stops. But as Git internals continue to be reshaped with the eventual goal of having them be call-able as a first party library, plugging any memory leaks throughout the codebase is vitally important. That effort has continued in this release, with more leaks throughout the codebase being plugged. For all of the details, check out the source links below: [source, source, source, source] The git mergetool command learned a new tool configuration for Visual Studio Code. While it has always been possible to manually configure Git to run VSCode’s 3-way merge resolution, it required manual configuration. In Git 2.47, you can now easily configure your repository by running: $ git config set merge.tool vscode and subsequent runs of git mergetool will automatically open VSCode in the correct configuration. [source] The rest of the iceberg That’s just a sample of changes from the latest release. For more, check out the release notes for 2.47, or any previous version in the Git repository. The post Highlights from Git 2.47 appeared first on The GitHub Blog. Highlights from Git 2.47https://github.blog/open-source/git/highlights-from-git-2-47/","tags":["Blogs","Github"],"categories":["Blogs","Github"]},{"title":"THAT'S NOT WHAT I MEANT! 🤦","path":"/RSSBOX/rss/57c81727.html","content":"#minecraftshorts #minecraft #shortsvideo #gaming THAT'S NOT WHAT I MEANT! 🤦https://www.youtube.com/watch?v=0n03QPUcC9E","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"1.21.2-Pre1 快照更新","path":"/RSSBOX/rss/35e66e6e.html","content":"1.21.2-pre1 快照更新 1.21.2-pre1 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"Cloudflare Acquires Kivera to Add Simple, Preventive Cloud Security to Cloudflare One","path":"/RSSBOX/rss/72f13894.html","content":"We’re excited to announce that Kivera, a cloud security, data protection, and compliance company, has joined Cloudflare. This acquisition extends our SASE portfolio to incorporate inline cloud app controls, empowering Cloudflare One customers with preventative security controls for all their cloud services.In today’s digital landscape, cloud services and SaaS (software as a service) apps have become indispensable for the daily operation of organizations. At the same time, the amount of data flowing between organizations and their cloud providers has ballooned, increasing the chances of data leakage, compliance issues, and worse, opportunities for attackers. Additionally, many companies — especially at enterprise scale — are working directly with multiple cloud providers for flexibility based on the strengths, resiliency against outages or errors, and cost efficiencies of different clouds. Security teams that rely on Cloud Security Posture Management (CSPM) or similar tools for monitoring cloud configurations and permissions and Infrastructure as code (IaC) scanning are falling short due to detecting issues only after misconfigurations occur with an overwhelming volume of alerts. The combination of Kivera and Cloudflare One puts preventive controls directly into the deployment process, or ‘inline’, blocking errors before they happen. This offers a proactive approach essential to protecting cloud infrastructure from evolving cyber threats, maintaining data security, and accelerating compliance. An early warning system for cloud security risks In a significant leap forward in cloud security, the combination of Kivera’s technology and Cloudflare One adds preventive, inline controls to enforce secure configurations for cloud resources. By inspecting cloud API traffic, these new capabilities equip organizations with enhanced visibility and granular controls, allowing for a proactive approach in mitigating risks, managing cloud security posture, and embracing a streamlined DevOps process when deploying cloud infrastructure.Kivera will add the following capabilities to Cloudflare’s SASE platform:One-click security: Customers benefit from immediate prevention of the most common cloud breaches caused by misconfigurations, such as accidentally allowing public access or policy inconsistencies.Enforced cloud tenant control: Companies can easily draw boundaries around their cloud resources and tenants to ensure that sensitive data stays within their organization. Prevent data exfiltration: Easily set rules to prevent data being sent to unauthorized locations.Reduce ‘shadow’ cloud infrastructure: Ensure that every interaction between a customer and their cloud provider is in line with preset standards. Streamline cloud security compliance: Customers can automatically assess and enforce compliance against the most common regulatory frameworks.Flexible DevOps model: Enforce bespoke controls independent of public cloud setup and deployment tools, minimizing the layers of lock-in between an organization and a cloud provider.Complementing other cloud security tools: Create a first line of defense for cloud deployment errors, reducing the volume of alerts for customers also using CSPM tools or Cloud Native Application Protection Platforms (CNAPPs). An intelligent proxy that uses a policy-based approach to enforce secure configuration of cloud resources. Better together with Cloudflare One As a SASE platform, Cloudflare One ensures safe access and provides data controls for cloud and SaaS apps. This integration broadens the scope of Cloudflare’s SASE platform beyond user-facing applications to incorporate increased cloud security through proactive configuration management of infrastructure services, beyond what CSPM and CASB solutions provide. With the addition of Kivera to Cloudflare One, customers now have a unified platform for all their inline protections, including cloud control, access management, and threat and data protection. All of these features are available with single-pass inspection, which is 50% faster than Secure Web Gateway (SWG) alternatives. With the earlier acquisition of BastionZero, a Zero Trust infrastructure access company, Cloudflare One expanded the scope of its VPN replacement solution to cover infrastructure resources as easily as it does apps and networks. Together Kivera and BastionZero enable centralized security management across hybrid IT environments, and provide a modern DevOps-friendly way to help enterprises connect and protect their hybrid infrastructure with Zero Trust best practices.Beyond its SASE capabilities, Cloudflare One is integral to Cloudflare’s connectivity cloud, enabling organizations to consolidate IT security tools on a single platform. This simplifies secure access to resources, from developer privileged access to technical infrastructure and expanding cloud services. As Forrester echoes, “Cloudflare is a good choice for enterprise prospects seeking a high-performance, low-maintenance, DevOps-oriented solution.” The growing threat of cloud misconfigurations The cloud has become a prime target for cyberattacks. According to the 2023 Cloud Risk Report, CrowdStrike observed a 95% increase in cloud exploitation from 2021 to 2022, with a staggering 288% jump in cases involving threat actors directly targeting the cloud.Misconfigurations in cloud infrastructure settings, such as improperly set security parameters and default access controls, provide adversaries with an easy path to infiltrate the cloud. According to the 2023 Thales Global Cloud Security Study, which surveyed nearly 3,000 IT and security professionals from 18 countries, 44% of respondents reported experiencing a data breach, with misconfigurations and human error identified as the leading cause, accounting for 31% of the incidents.Further, according to GartnerⓇ, “Through 2027, 99% of records compromised in cloud environments will be the result of user misconfigurations and account compromise, not the result of an issue with the cloud provider.”1Several factors contribute to the rise of cloud misconfigurations:Rapid adoption of cloud services: Leaders are often driven by the scalability, cost-efficiency, and ability to support remote work and real-time collaboration that cloud services offer. These factors enable rapid adoption of cloud services which can lead to unintentional misconfigurations as IT teams struggle to keep up with the pace and complexity of these services. Complexity of cloud environments: Cloud infrastructure can be highly complex with multiple services and configurations to manage. For example, AWS alone offers 373 services with 15,617 actions and 140,000+ parameters, making it challenging for IT teams to manage settings accurately. Decentralized management: In large organizations, cloud infrastructure resources are often managed by multiple teams or departments. Without centralized oversight, inconsistent security policies and configurations can arise, increasing the risk of misconfigurations.Continuous Integration and Continuous Deployment (CI/CD): CI/CD pipelines promote the ability to rapidly deploy, change and frequently update infrastructure. With this velocity comes the increased risk of misconfigurations when changes are not properly managed and reviewed.Insufficient training and awareness: Employees may lack the cross-functional skills needed for cloud security, such as understanding networks, identity, and service configurations. This knowledge gap can lead to mistakes and increases the risk of misconfigurations that compromise security. Common exploitation methods Threat actors exploit cloud services through various means, including targeting misconfigurations, abusing privileges, and bypassing encryption. Misconfigurations such as exposed storage buckets or improperly secured APIs offer attackers easy access to sensitive data and resources. Privilege abuse occurs when attackers gain unauthorized access through compromised credentials or poorly managed identity and access management (IAM) policies, allowing them to escalate their access and move laterally within the cloud environment. Additionally, unencrypted data enables attackers to intercept and decrypt data in transit or at rest, further compromising the integrity and confidentiality of sensitive information.Here are some other vulnerabilities that organizations should address: Unrestricted access to cloud tenants: Allowing unrestricted access exposes cloud platforms to data exfiltration by malicious actors. Limiting access to approved tenants with specific IP addresses and service destinations helps prevent unauthorized access.Exposed access keys: Exposed access keys can be exploited by unauthorized parties to steal or delete data. Requiring encryption for the access keys and restricting their usage can mitigate this risk.Excessive account permissions: Granting excessive privileges to cloud accounts increases the potential impact of security breaches. Limiting permissions to necessary operations helps prevent lateral movement and privilege escalation by threat actors.Inadequate network segmentation: Poorly managed network security groups and insufficient segmentation practices can allow attackers to move freely within cloud environments. Drawing boundaries around your cloud resources and tenants ensures that data stays within your organization.Improper public access configuration: Incorrectly exposing critical services or storage resources to the internet increases the likelihood of unauthorized access and data compromise. Preventing public access drastically reduces risk.Shadow cloud infrastructure: Abandoned or neglected cloud instances are often left vulnerable to exploitation, providing attackers with opportunities to access sensitive data left behind. Preventing untagged or unapproved cloud resources to be created can reduce the risk of exposure. Limitations of existing tools Many organizations turn to CSPM tools to give them more visibility into cloud misconfigurations. These tools often alert teams after an issue occurs, putting security teams in a reactive mode. Remediation efforts require collaboration between security teams and developers to implement changes, which can be time-consuming and resource-intensive. This approach not only delays issue resolution but also exposes companies to compliance and legal risks, while failing to train employees on secure cloud practices. On average, it takes 207 days to identify these breaches and an additional 70 days to contain them. Addressing the growing threat of cloud misconfigurations requires proactive security measures and continuous monitoring. Organizations must adopt proactive security solutions that not only detect and alert but also prevent misconfigurations from occuring in the first place and enforce best practices. Creating a first line of defense for cloud deployment errors reduces the volume of alerts for customers, especially those also using CSPM tools or CNAPPs. By implementing these proactive strategies, organizations can safeguard their cloud environments against the evolving landscape of cyber threats, ensuring robust security and compliance while minimizing risks and operational disruptions. What’s next for Kivera The Kivera product will not be a point solution add-on. We’re making it a core part of our Cloudflare One offering because integrating features from products like our Secure Web Gateway give customers a comprehensive solution that works better together.We’re excited to welcome Kivera to the Cloudflare team. Through the end of 2024 and into early 2025, Kivera’s team will focus on integrating their preventive inline cloud app controls directly into Cloudflare One. We are looking for early access testers and teams to provide feedback about what they would like to see. If you’d like early access, please join the waitlist.[1] Source: Outcome-Driven Metrics You Can Use to Evaluate Cloud Security Controls, Gartner, Charlie Winckless, Paul Proctor, Manuel Acosta, 09/28/2023 GARTNER is a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and is used herein with permission. All rights reserved. Cloudflare acquires Kivera to add simple, preventive cloud security to Cloudflare Onehttps://blog.cloudflare.com/cloudflare-acquires-kivera","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"Leveraging Kubernetes Virtual Machines at Cloudflare With KubeVirt","path":"/RSSBOX/rss/835eaaab.html","content":"Cloudflare runs several multi-tenant Kubernetes clusters across our core data centers. These general-purpose clusters run on bare metal and power our control plane, analytics, and various engineering tools such as build infrastructure and continuous integration.Kubernetes is a container orchestration platform. It enables software engineers to deploy containerized applications to a cluster of machines. This enables teams to build highly-available software on a scalable and resilient platform.In this blog post we discuss our Kubernetes architecture, why we needed virtualization, and how we’re using it today. Multi-tenant clusters Multi-tenancy is a concept where one system can share its resources among a wide range of customers. This model allows us to build and manage a small number of general purpose Kubernetes clusters for our internal application teams. Keeping the number of clusters small reduces our operational toil. This model shrinks costs and increases computational efficiency by sharing hardware. Multi-tenancy also allows us to scale more efficiently. Scaling is done at either a cluster or application level. Cluster operators scale the platform by adding more hardware. Teams scale their applications by updating their Kubernetes manifests. They can scale vertically by increasing their resource requests or horizontally by increasing the number of replicas.All of our Kubernetes clusters are multi-tenant with various components enabled for a secure and resilient platform.Pods are secured using the latest standards recommended by the Kubernetes project. We use Pod Security Admission (PSA) and Pod Security Standards to ensure all workloads are following best practices. By default, all namespaces use the most restrictive profile, and only a few Kubernetes control plane namespaces are granted privileged access. For additional policies not covered by PSA, we built custom Validating Webhooks on top of the controller-runtime framework. PSA and our custom policies ensure clusters are secure and workloads are isolated. Our need for virtualization A select number of teams needed tight integration with the Linux kernel. Examples include Docker daemons for build infrastructure and the ability to simulate servers running the software and configuration of our global network. With our pod security requirements, these workloads are not permitted to interface with the host kernel at a deep level (e.g. no iptables or sysctls). Doing so may disrupt other tenants sharing the node and open additional attack vectors if an application was compromised. A virtualization platform would enable these workloads to interact with their own kernel within a secured Kubernetes cluster.We considered various different virtualization solutions. Running a separate virtualization platform outside of Kubernetes would have worked, but would not tightly integrate containerized workloads with virtual machines. It would also be an additional operational burden on our team, as backups, alerting, and fleet management would have to exist for both our Kubernetes and virtual machine clusters.We then looked for solutions that run virtual machines within Kubernetes. Teams could already manually deploy QEMU pods, but this was not an elegant solution. We needed a better way. There were several other options, but KubeVirt was the tool that met the majority of our requirements. Other solutions required a privileged container to run a virtual machine, but KubeVirt did not – this was a crucial requirement in our goal of creating a more secure multi-tenant cluster. KubeVirt also uses a feature of the Kubernetes API called Custom Resource Definitions (CRDs), which extends the Kubernetes API with new objects, increasing the flexibility of Kubernetes beyond its built-in types. For KubeVirt, this includes objects such as VirtualMachine and VirtualMachineInstanceReplicaSet. We felt the use of CRDs would allow KubeVirt to grow as more features were added. What is KubeVirt? KubeVirt is a virtualization platform that enables users to run virtual machines within Kubernetes. With KubeVirt, virtual machines run alongside containerized workloads on the same platform. Kubernetes primitives such as network policies, configmaps, and services all integrate with virtual machines. KubeVirt scales with our needs and is successfully running hundreds of virtual machines across several clusters. We frequently remediate Kubernetes nodes, so virtual machines and pods are always exercising their startup/shutdown processes. How Cloudflare uses KubeVirt There are a number of internal projects leveraging virtual machines at Cloudflare. We’ll touch on a few of our more popular use cases:Kubernetes scalability testingDevelopment environmentsKernel and iPXE testingBuild pipelines Kubernetes scalability testing Setup processOur staging clusters are much smaller than our largest production clusters. They also run on bare metal and mirror the configuration we have for each production cluster. This is extremely useful when rolling out new software, operating systems, or kernel changes; however, they miss bugs that only surface at scale. We use KubeVirt to bridge this gap and virtualize Kubernetes clusters with hundreds of nodes and thousands of pods.The setup process for virtualized clusters differs from our bare metal provisioning steps. For bare metal, we use Salt to provision clusters from start to finish. For our virtualized clusters we use Ansible and kubeadm. Our bare metal staging clusters are responsible for testing and validating our Salt configuration. The virtualized clusters give us a vanilla Kubernetes environment without any Cloudflare customizations. Having a stock environment in addition to our Salt environment helps us isolate bugs down to a Kubernetes change, a kernel change, or a Cloudflare-specific configuration change.Our virtualized clusters consist of a KubeVirt VirtualMachine object per node. We create three control-plane nodes and any number of worker nodes. Each virtual machine starts out as a vanilla Debian generic cloud image. Using KubeVirt’s cloud-init support, the virtual machine downloads an internal Ansible playbook which installs a recent kernel, cri-o (the container runtime we use), and kubeadm. - name: Add the Kubernetes gpg key apt_key: url: https://pkgs.k8s.io/core:/stable:/&#123;&#123; kube_version &#125;&#125;/deb/Release.key keyring: /etc/apt/keyrings/kubernetes-apt-keyring.gpg state: present name: Add the Kubernetes repositoryshell: echo “deb [signed-by&#x3D;&#x2F;etc&#x2F;apt&#x2F;keyrings&#x2F;kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/&#123;&amp;#123; kube_version &amp;#125;&amp;#125;&#x2F;deb&#x2F; &#x2F;“ | tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;kubernetes.list name: Add the CRI-O gpg keyapt_key: url: https://pkgs.k8s.io/addons:/cri-o:/&#123;&amp;#123; crio_version &amp;#125;&amp;#125;&#x2F;deb&#x2F;Release.key keyring: &#x2F;etc&#x2F;apt&#x2F;keyrings&#x2F;cri-o-apt-keyring.gpg state: present name: Add the CRI-O repositoryshell: echo “deb [signed-by&#x3D;&#x2F;etc&#x2F;apt&#x2F;keyrings&#x2F;cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/&#123;&amp;#123; crio_version &amp;#125;&amp;#125;&#x2F;deb&#x2F; &#x2F;“ | tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;cri-o.list name: Install CRI-O and Kubernetes packagesapt: name:- cri-o- kubelet- kubeadm- kubectl update_cache: yes state: present name: Enable and start CRI-O serviceservice: state: started enabled: yes name: crio.service Ansible playbook steps to download and install Kubernetes toolingOnce each node has completed its individual playbook, we can initialize and join nodes to the cluster using another playbook that runs kubeadm. From there the cluster can be accessed by logging into a control plane node using kubectl.Simulating at scaleWhen losing 10s or 100s of nodes at once, Kubernetes needs to act quickly to minimize downtime. The sooner it recognizes node failure, the faster it can reroute traffic to healthy pods.Using Kubernetes in KubeVirt we are able to simulate a large cluster undergoing a network cut and observe how Kubernetes reacts. The KubeVirt Kubernetes cluster allows us to rapidly iterate on configuration changes and code patches.The following Ansible playbook task simulates a network segmentation failure where only the control-plane nodes remain online. - name: Disable network interfaces on all workers command: ifconfig enp1s0 down async: 5 poll: 0 ignore_errors: yes when: inventory_hostname in groups['kube-node'] An Ansible role which disables the network on all worker nodes simultaneously.This framework allows us to exercise the code in controller-manager, Kubernetes’s daemon that reconciles the fundamental state of the system (Nodes, Pods, etc). Our simulation platform helped us drastically shorten full traffic recovery time when a large number of Kubernetes nodes become unreachable. We upstreamed our changes to Kubernetes and more controller-manager speed improvements are coming soon. Development environments Compiling code on your laptop can be slow. Perhaps you’re working on a patch for a large open-source project (e.g. V8 or Clickhouse) or need more bandwidth to upload and download containers. With KubeVirt, we enable our developers to rapidly iterate on software development and testing on powerful server hardware. KubeVirt integrates with Kubernetes Persistent Volumes, which enables teams to persist their development environment across restarts.There are a number of teams at Cloudflare using KubeVirt for a variety of development and testing environments. Most notably is a project called Edge Test Fleet, which emulates a physical server and all the software that runs Cloudflare’s global network. Teams can test their code and configuration changes against the entire software stack without reserving dedicated hardware. Cloudflare uses Salt to provision systems. It can be difficult to iterate and test Salt changes without a complete virtual environment. Edge Test Fleet makes iterating on Salt easier, ensuring states compile and render the right output. With Edge Test Fleet, new developers can better understand how Cloudflare’s global network works without touching staging or production.Additionally, one Cloudflare team developed a framework that allows users to build and test changes to Clickhouse using a VSCode environment. This framework is generally applicable to all teams requiring a development environment. Once a template environment is provisioned, CSI Volume Cloning can duplicate a golden volume, separating persistent environments for each developer. apiVersion: v1 kind: PersistentVolumeClaim name: devspace-jcichra-rootfs namespace: dev-clickhouse-vms spec: accessModes: - ReadWriteOnce storageClassName: rook-ceph-nvme dataSource: kind: PersistentVolumeClaim name: dev-rootfs resources: requests: storage: 500Gi A PersistentVolumeClaim that clones data from another volume using CSI Volume Cloning Kernel and iPXE testing Unlike user space software development, when a kernel crashes, the entire system crashes. The kernel team uses KubeVirt for development. KubeVirt gives all kernel engineers, regardless of laptop OS or architecture, the same x86 environment and hypervisor. Virtual machines on server hardware can be scaled up to more cores and memory than on laptops. The Cloudflare kernel team has also found low-level issues which only surface in environments with many CPUs.To make testing fast and easy, the kernel team serves iPXE images via an nginx Pod and Service adjacent to the virtual machine. A recent kernel and Debian image are copied to the nginx pod via kubectl cp. The iPXE file can then be referenced in the KubeVirt virtual machine definition via the DNS name for the Kubernetes Service. interfaces: name: default masquerade: &#123;&#125; model: e1000e ports: - port: 22 dhcpOptions: bootFileName: http://httpboot.u-$K8S_USER.svc.cluster.local/boot.ipxe When the virtual machine boots, it will get an IP address on the default interface behind NAT due to our masquerade setting. Then it will download boot.ipxe, which describes what additional files should be downloaded to start the system. In this case, the kernel (vmlinuz-amd64), Debian (baseimg-amd64.img) and additional kernel modules (modules-amd64.img) are downloaded. UEFI iPXE boot connecting and downloading files from nginx pod in user’s namespaceOnce the system is booted, a developer can log in to the system for testing: linux login: root Password: Linux linux 6.6.35-cloudflare-2024.6.7 #1 SMP PREEMPT_DYNAMIC Mon Sep 27 00:00:00 UTC 2010 x86_64 The programs included with the Debian GNU&#x2F;Linux system are free software;the exact distribution terms for each program are described in theindividual files in &#x2F;usr&#x2F;share&#x2F;doc&#x2F;*&#x2F;copyright. Debian GNU&#x2F;Linux comes with ABSOLUTELY NO WARRANTY, to the extentpermitted by applicable law.root@linux:~# Custom kernels can be copied to the nginx pod via kubectl cp. Restarting the virtual machine will load that new kernel for testing. When a kernel panic occurs, the virtual machine can quickly be restarted with virtctl restart linux and it will go through the iPXE boot process again. Build pipelines Cloudflare leverages KubeVirt to build a majority of software at Cloudflare. Virtual machines give build system users full control over their pipeline. For example, Debian packages can easily be installed and separate container daemons (such as Docker) can run all within a Kubernetes namespace using the restricted Pod Security Standard. KubeVirt’s VirtualMachineReplicaSet concept allows us to quickly scale up and down the number of build agents to match demand. We can roll out different sets of virtual machines with varying sizes, kernels, and operating systems.To scale efficiently, we leverage container disks to store our agent virtual machine images. Container disks allow us to store the virtual machine image (for example, a qcow image) in our container registry. This strategy works well when the state in virtual machines is ephemeral. Liveness probes detect unhealthy or broken agents, shutting down the virtual machine and replacing them with a fresh instance. Other automation limits virtual machine uptime, capping it to 3–4 hours to keep build agents fresh. Next steps We’re excited to expand our use of KubeVirt and unlock new capabilities for our internal users. KubeVirt’s Linux ARM64 support will allow us to build ARM64 packages in-cluster and simulate ARM64 systems.Projects like KubeVirt CDI (Containerized Data Importer) will streamline our user’s virtual machine experience. Instead of users manually building container disks, we can provide a catalog of virtual machine images. It also allows us to copy virtual machine disks between namespaces. Conclusion KubeVirt has proven to be a great tool for virtualization in our Kubernetes-first environment. We’ve unlocked the ability to support more workloads with our multi-tenant model. The KubeVirt platform allows us to offer a single compute platform supporting containers and virtual machines. Managing it has been simple, and upgrades have been straightforward and non-disruptive. We’re exploring additional features KubeVirt offers to improve the experience for our users.Finally, our team is expanding! We’re looking for more people passionate about Kubernetes to join our team and help us push Kubernetes to the next level. Leveraging Kubernetes virtual machines at Cloudflare with KubeVirthttps://blog.cloudflare.com/leveraging-kubernetes-virtual-machines-with-kubevirt","tags":["Blogs","Cloudflare"],"categories":["Blogs","Cloudflare"]},{"title":"The Second Half of Software Supply Chain Security on GitHub","path":"/RSSBOX/rss/f9621847.html","content":"Software supply chain security has rocketed into the public consciousness after a major cybersecurity attack against the U.S. federal government was made public in late 2020. In this post, we’ll go over the history of the response to that attack, how that’s impacted the U.S. government as well as anyone who produces software, a community-developed framework for how to think about this problem holistically, and how to use GitHub, particularly, to improve the security in the second half of your software supply chain. An abridged history In late 2020, details of a major cyberattack against the U.S. federal government were made public. Like many cyberattacks, it made use of malware, but what was novel about this attack is that the malware was included in a widely-used security product during the build process. That way the malware was not visible to employees accessing the product’s codebase, and the targets were already regularly installing and updating the product. Because of how powerful and effective this attack was, it set off a wave of responses across the U.S. government and private industry, starting with a May 2021 White House Executive Order on Improving the Nation’s Cybersecurity with section 4 detailing (using letters “a” through “x”!), steps to enhance software supply chain security. For the sake of brevity we won’t detail the entire U.S. federal response, but fast forward to 2024 and software supply chain security is an ongoing concern. In May 2024, the White House released v2 of the National Cybersecurity Strategy Implementation Plan with initiative 5.5.4 calling to “promulgate and amplify Cybersecurity Supply Chain Risk Management (CSCRM) key practices across and within critical infrastructure sectors.” In August 2024, the Office of the National Cyber Director summarized feedback it received from the open source community and private sector into 12 activities to address in the coming year, including “(5) Strengthen the software supply chain.” These recommendations aren’t just for U.S. federal agencies. Companies selling cloud services to the U.S. government need to comply with frameworks like FedRAMP, which uses the NIST 800-53 control system and hopefully at this point you aren’t surprised that it includes “PM-30: Supply Chain Risk Management Strategy.”. But even companies who aren’t selling to the U.S. government it’s a good idea to have internal policies around software supply chain security management, which can then provide evidence of compliance for SOC2 or ISO 27001 audits. What does software supply chain mean? Hopefully, I’ve convinced you that having a plan for securing your software supply chain is a good idea. But what does that look like in practice? How do you get started? The Open Source Security Foundation (OpenSSF), which itself is part of the Linux Foundation, brings together people who produce and use open source code to improve security practices. There are several frameworks for thinking about software supply chain security, but the one I’ve found most user-friendly comes from the OpenSSF called Supply-chain Levels for Software Artifacts (SLSA). If you’ve been to a software security conference since 2020, you’ve probably seen this project’s depiction of the software supply chain: As you know, producing software is a complex process, and a lot can go wrong along the way. At GitHub, we’ve previously written about practices for writing secure code and handling dependencies, so here we’re going to focus on the second half of the software supply chain: ensuring the integrity of our build and making sure the software consumers run hasn’t been tampered with. I could hardly say it better than the U.S. Cybersecurity and Infrastructure Security Agency in their Defending Against Software Supply Chain Attacks publication: Providing a mechanism for verifying software release integrity (in particular, the protection of the code signing certificate) to help customers ensure that the software they acquire has not been subjected to tampering. Using GitHub Code signing is hardly new, but many implementations are full of pitfalls and usability hurdles. Securely managing private key material is tricky, and many systems don’t have any way to recover if your private key is accidentally made public. GitHub built artifact attestations to make it deadsimple for you to sign any software you build in GitHub Actions. It uses the workload identity available via the Actions OIDC token to securely obtain a code signing certificate. You don’t need to manage a long-lived private key, and key rotation is built into the service. To use it you just need to add a workflow step to do the signing, and you can use the gh CLI to verify the signatures, including offline verification support. If you’re just looking for an easy way to get started with signing and verifying builds, you can stop here. But there are some more advanced features in artifact attestations that you might also be interested in as you continue your security journey. To explain, let’s go back to the SLSA framework, in particular the build track. Using artifact attestations rockets you from level 0 (not having anything) to level 2 (signed provenance). Provenance is more than just a signature over the bytes of your build. Because the code signing certificate is obtained with the workload identity of the build, there’s a lot of additional useful information in that certificate: the exact repository the build came from, the path to the build instructions, the exact SHA of the source code and build instructions, what branch the build came from, and how the build was triggered. When you’re using the gh CLI to verify the build signature, the least amount of information you can provide is what owner or repository the build came from. But you can additionally enforce policy on any of these fields, like requiring that builds come from branches that are protected by rulesets. In fact, SLSA build track defines a level 3 where the build takes place in an isolated environment with controlled inputs. On the GitHub platform, this maps to having your build take place in a reusable workflow where your organization has vetted the build instructions, and then when verifying artifact attestations specifying which workflows you allow builds to come from. To help, we’ve written up documentation specifically for using artifact attestations with reusable workflows. Conclusion Software supply chain security is an emerging and massive topic. Instead of trying to tackle everything at once, we recommend starting by signing your builds with artifact attestations and verifying those signatures before you run them. Even if your organization doesn’t require it today, it’s a good practice to get into and is something more consumers will be asking about in the near future. We’re here to grow along with you, and as your security needs mature, we’ll be ready to help you tackle more advanced capabilities. The post The second half of software supply chain security on GitHub appeared first on The GitHub Blog. The second half of software supply chain security on GitHubhttps://github.blog/security/supply-chain-security/the-second-half-of-software-supply-chain-security-on-github/","tags":["Blogs","Github"],"categories":["Blogs","Github"]},{"title":"HAVE YOU SEEN the NEW MOB?👀","path":"/RSSBOX/rss/f6d4c3bb.html","content":"#minecraftshorts #minecraftshorts #shortsvideo #gaming #ytshorts #minecraftlive #minecraftlive2024 #shortsvideo #villagernews #elementanimation HAVE YOU SEEN THE NEW MOB?👀https://www.youtube.com/watch?v=MixMxBYniUw","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"VISIT the PALE GARDEN NOW!","path":"/RSSBOX/rss/793e3ab1.html","content":"#minecraft #minecraftshorts #ytshorts #shortvideo #ytshort #shortvideo #palegarden #creaking VISIT THE PALE GARDEN NOW!https://www.youtube.com/watch?v=Zg69LE2YfxU","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"24w40a 快照更新","path":"/RSSBOX/rss/d3800e3f.html","content":"24w40a 快照更新 24w40a 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"Minecraft Live 2024: The Monster in the Woods","path":"/RSSBOX/rss/cf79ae55.html","content":"We interrupt your YouTube feed with a special news report from Element Animation! Apparently, there's been a MONSTER SIGHTING during Minecraft Live 2024! Is it the real deal or a terrible tall tale? Villager #5 and Villager #9 are ready to bring you the latest update on this eerie event! Minecraft Live 2024: The Monster in the Woodshttps://www.youtube.com/watch?v=nDLmBv-2wb4","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"Minecraft Live 2024: Minecraft Experience | Villager Rescue","path":"/RSSBOX/rss/2dd240dd.html","content":"Minecraft Experience: Villager Rescue is an epic interactive experience where the most popular elements of Minecraft are brought to life before your eyes! Let’s grab our spyglass and take a closer look at the real-world adventure coming to Dallas, Texas! Minecraft Live 2024: Minecraft Experience | Villager Rescuehttps://www.youtube.com/watch?v=9pF_VGP7NVA","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"BREAKING NEWS: DANGEROUS NEW MOB!","path":"/RSSBOX/rss/12ca1c92.html","content":"#minecraftshorts #minecraft #shortsvideo #gaming #ytshorts #minecraftlive #minecraftlive2024 #shortsvideo #villagernews #elementanimation BREAKING NEWS: DANGEROUS NEW MOB!https://www.youtube.com/watch?v=J05pfgB2Z1M","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"Minecraft Live 2024: Minecraft’s Past & Future","path":"/RSSBOX/rss/ca256992.html","content":"Minecraft is changing! Join Chief Creative Officer Jens “Jeb” Bergensten as he looks back on its blocky history, its deep-rooted philosophy, and talks about what the future may hold. Minecraft Live 2024: Minecraft’s past & futurehttps://www.youtube.com/watch?v=LI1eL8x1KYM","tags":["Minecraft","官方油管"],"categories":["Minecraft","官方油管"]},{"title":"24w39a 快照更新","path":"/RSSBOX/rss/580be125.html","content":"24w39a 快照更新 24w39a 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"24w38a 快照更新","path":"/RSSBOX/rss/e1f03acd.html","content":"24w38a 快照更新 24w38a 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"24w37a 快照更新","path":"/RSSBOX/rss/b578f950.html","content":"24w37a 快照更新 24w37a 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"24w36a 快照更新","path":"/RSSBOX/rss/c8322b8.html","content":"24w36a 快照更新 24w36a 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"24w35a 快照更新","path":"/RSSBOX/rss/1dfe48c1.html","content":"24w35a 快照更新 24w35a 快照更新https://www.minecraft.net/","tags":["Minecraft","MC更新"],"categories":["Minecraft","MC更新"]},{"title":"Fern Wifi Cracker 报错解决方案","path":"/RSSBOX/rss/b45191e3.html","content":"查看无线网卡1airmon-ng重新设置无线网卡12ip link set wlan0 downip link set wlan0 name wlan0mon fern wifi cracker 报错解决方案http://blog.imc.re/archives/91fb.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"博客真的需要图片点击放大功能吗？","path":"/RSSBOX/rss/f143b031.html","content":"这是一个非常有争议的话题，自 Stellar 诞生之初，就有这个话题相关的讨论，本文将就这个话题展开详细阐述，解答一下 Stellar 为什么与众不同。 博客真的需要图片点击放大功能吗？https://xaoxuu.com/blog/20240208/","tags":["Blogs","XAOXUU"],"categories":["Blogs","XAOXUU"]},{"title":"我为什么开发了一个专栏功能？","path":"/RSSBOX/rss/9e02f6c3.html","content":"在回访用户的过程中，我发现有不少用户用 wiki 当作专栏来使用，可见专栏确实是一个普遍的需求点，而现有的 wiki 系统并不是专门为此场景设计的，虽然能用，但是不够好用。 我为什么开发了一个专栏功能？https://xaoxuu.com/blog/20240203/","tags":["Blogs","XAOXUU"],"categories":["Blogs","XAOXUU"]},{"title":"项目警告对构建速度的巨大影响","path":"/RSSBOX/rss/8ced571.html","content":"近期（指一个月前）终于有时间偿还前辈留下的技术债了，下面分享一下经验和心得。 项目警告对构建速度的巨大影响https://xaoxuu.com/blog/20240111/","tags":["Blogs","XAOXUU"],"categories":["Blogs","XAOXUU"]},{"title":"关于 ObjC 通知的一个神奇崩溃","path":"/RSSBOX/rss/d72c298a.html","content":"近日发现一个用 NSNotificationCenter 发通知时触发的 EXC_BAD_ACCESS 崩溃，表现形式比较奇怪，特此记录一下。 关于 ObjC 通知的一个神奇崩溃https://xaoxuu.com/blog/20240110/","tags":["Blogs","XAOXUU"],"categories":["Blogs","XAOXUU"]},{"title":"Gallery 标签组件的使用方法和 Unsplash 高清壁纸分享","path":"/RSSBOX/rss/1eb3057e.html","content":"水一下文，主要是为了测试一下图库功能，都是高清大图，可以用作壁纸。 Gallery 标签组件的使用方法和 Unsplash 高清壁纸分享https://xaoxuu.com/blog/20231223/","tags":["Blogs","XAOXUU"],"categories":["Blogs","XAOXUU"]},{"title":"Linux查询操作","path":"/RSSBOX/rss/bcd9ad2.html","content":"whereis用来寻找可执行文件的位置12345678[root@master ~]# whereis lsls: /usr/bin/ls /usr/share/man/man1/ls.1.gz[root@master ~]# lsaaa anaconda-ks.cfg localperl nohup.out perl5 ShiSai2022-1.0-SNAPSHOT.jar spark-yarn-logs zookeeper.out[root@master ~]# cd /usr/bin/[root@master bin]# ll ls-rwxr-xr-x. 1 root root 117680 10月 31 2018 lswhatis用来获取命令的简介12[root@master bin]# whatis lsls (1) - list directory contentsfind用来寻找文件的位置从根目录下开始，寻找cd这个文件的位置。一共找到了3处。1234567[root@master ~]# find / -name cd/root/.npm/_cacache/index-v5/cd/root/.npm/_cacache/content-v2/sha512/f7/cd/usr/bin/cd[root@master ~]# cd /usr/bin/[root@master bin]# ls cdcdgrep用来查找文件中，包含指定字符串的行建立./aaa.txt并插入一些内容1234567891011121314151617[root@master ~]# echo \"hello hadoop\" &gt; ./aaa.txt[root@master ~]# echo \"hello hdfs\" &gt;&gt; ./aaa.txt[root@master ~]# echo \"hello hdfs\" &gt;&gt; ./aaa.txt[root@master ~]# echo \"hello yarn\" &gt;&gt; ./aaa.txt[root@master ~]# echo \"hello spark\" &gt;&gt; ./aaa.txt[root@master ~]# echo \"hello hive\" &gt;&gt; ./aaa.txt[root@master ~]# echo \"hello flink\" &gt;&gt; ./aaa.txt[root@master ~]# echo \"hello hbase\" &gt;&gt; ./aaa.txt[root@master ~]# cat ./aaa.txthello hadoophello hdfshello hdfshello yarnhello sparkhello hivehello flinkhello hbase在./aaa.txt里查找含有字符串hadoop的行1234567[root@master ~]# grep hadoop ./aaa.txthello Hadoop[root@master ~]# grep flink ./aaa.txthello flink[root@master ~]# grep hdfs ./aaa.txthello hdfshello hdfs用数字法修改文件的权限./aaa.txt原来的权限：对于文件的所有者而言aaa.txt可读可写不可执行，对于同组用户而言aaa.txt可读不可写不可执行，对于其他用户而言aaa.txt可读不可写不可执行123456[root@master ~]# ll ./aaa.txt-rw-r--r--. 1 root root 93 11月 20 08:21 ./aaa.txt 421421421 6 4 4-rwxr--r-- 7 4 4把./aaa.txt变成：对于文件的所有者而言aaa.txt可读可写可执行，对于同组用户而言aaa.txt可读不可写不可执行，对于其他用户而言aaa.txt可读不可写不可执行123[root@master ~]# chmod 744 ./aaa.txt[root@master ~]# ll ./aaa.txt-rwxr--r--. 1 root root 93 11月 20 08:21 ./aaa.txt把./aaa.txt变成：对于文件的所有者而言aaa.txt可读可写可执行，对于同组用户而言aaa.txt可读可写可执行，对于其他用户而言aaa.txt可读可写可执行123[root@master ~]# chmod 777 ./aaa.txt[root@master ~]# ll ./aaa.txt-rwxrwxrwx. 1 root root 93 11月 20 08:21 ./aaa.txt把./aaa.txt变回到原来状态123[root@master ~]# chmod 644 ./aaa.txt[root@master ~]# ll ./aaa.txt-rw-r--r--. 1 root root 93 11月 20 08:21 ./aaa.txt课堂练习：把hadoop01镜像转到20230912，并启动hadoop01，用Ubuntu进行联机。完成下面的操作，并把命令和结果截图粘贴到ubuntu桌面下的目录Release的“模块A提交的结果.docx”。要求所有命令均使用绝对路径。在/root目录下建立子目录bbb1mkdir /root/bbb在/root/bbb目录下建立文件b01.txt1touch /root/bbb/b01.txt在b01.txt中插入以下内容： Hello Hadoop Hello hdfs Hello spark Hello flink Hello yarn Hello hbase12345678cat &gt;&gt; /root/bbb/b01.txt &lt;&lt; EOFHello HadoopHello hdfsHello sparkHello flinkHello yarnHello hbaseEOF将b01.txt拷贝为b02.txt，并在b02.txt中新增两行内容： Hello Redis Hello clickhouse123cp /root/bbb/b01.txt /root/bbb/b02.txtecho \"Hello Redis\" &gt;&gt; /root/bbb/b02.txtecho \"Hello clickhouse\" &gt;&gt; /root/bbb/b02.txt用diff命令比较b01.txt、b02.txt的不同之处1diff /root/bbb/b01.txt /root/bbb/b02.txt用grep命令查找b01.txt中含有flink的行1grep \"flink\" /root/bbb/b01.txt修改/root/bbb/b01.txt的权限：文件所有者可读可写可执行，同组用户可读可写不可执行，其他用户不可读不可写不可执行，并用ls命令查看结果1chmod 760 /root/bbb/b01.txt &amp;&amp; ls -l /root/bbb/b01.txt修改/root/bbb/b02.txt的权限：文件所有者可读不可写可执行，同组用户可读不可写不可执行，其他用户不可读不可写可执行，并用ls命令查看结果1chmod 441 /root/bbb/b01.txt &amp;&amp; ls -l /root/bbb/b01.txt用find命令查找b01.txt文件的位置1find / -name b01.txt用whereis命令查找grep命令文件的位置1whereis grep用tar命令将到/opt/hadoop-2.7.7.tar.gz解压到/usr/local/src下面。1tar -xvzf /opt/hadoop-2.7.7.tar.gz -C /usr/local/src用find命令查找文件stop-all.sh的位置。1find / -name stop-all.sh Linux查询操作http://blog.imc.re/archives/b59e.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"Linux管道命令","path":"/RSSBOX/rss/f7d3eead.html","content":"管道命令：|管道命令的作用：前一个命令的输出是后一个命令的输入例如：统计目前有几个用户登录1234[root@master ~]# whoroot pts/0 2023-12-18 08:08 (192.168.152.130)[root@master ~]# who | wc -l1查看/etc下面的文件和文件夹及其子文件夹、子文件的详细信息，并以可翻页的形式显示：1[root@master ~]# ls -Rl /etc | less增加一个用户aaa，并查看/etc/passwd的内容，该文件存放了所有的系统用户、root用户、自定义用户。增加一个用户aaa，就可以看到该文件最后增加的相应的一行内容。用管道命令统计该文件的行数、字数、字符数。1234567891011121314[root@master ~]# adduser aaa[root@master ~]# cat /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologin…………nfsnobody:x:65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologinmysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/falseaaa:x:1000:1000::/home/aaa:/bin/bash[root@master ~]# cat /etc/passwd | wc 30 59 1491# （行数） （字数） （字符数）查看是否存在用户aaa12[root@master ~]# cat /etc/passwd | grep aaaaaa:x:1000:1000::/home/aaa:/bin/bash命令rpm -qa可以查看目前安装了哪些应用程序。现在要用管道命令，检查是否安装了mysql数据库：123456[root@master ~]# rpm -qa | grep mysqlmysql-community-server-5.7.18-1.el7.x86_64mysql-community-common-5.7.18-1.el7.x86_64mysql-community-libs-5.7.18-1.el7.x86_64mysql-community-client-5.7.18-1.el7.x86_64mysql-community-devel-5.7.18-1.el7.x86_64查看mysql数据库中有多少个组件：12[root@master ~]# rpm -qa | grep mysql | wc -l5通过这个命令，说明了管道命令是可以级联的，从而实现更加复杂的功能。vi编辑器操作（1）vi编辑器的启动和退出用命令：vi+文件名：进入vi编辑器按i键进入编辑模式，可以输入内容按:wq保存退出，按:q!不保存退出（2）vi编辑器的工作模式编辑模式：进入vi编辑器以后的默认模式插入模式：在编辑模式下，按i键，进入插入模式命令模式：在编辑模式下，按：键，进入命令模式（3）vi编辑器常用命令:q! 不保存退出:wq 保存退出:?单词 查找文件中的某一个单词位置，查找定位后，按n键可以连续定位下一个单词。:s/单词1/单词2/g 把文件中所有的单词1，替换成单词2:d 删除当前行，或者在编辑模式状态下，在某一行上按dd，直接删除这一行课堂练习：在/opt下建立文件夹/opt/aaa1mkdir /opt/aaa用vi编辑器建立文件/opt/aaa/a01.txt,并输入下面的内容12345678vi /opt/aaa/a01.txthello hadoophello sparkhello sparkhello hdfshello hdfshello hdfshello spark spark保存并退出用管道命令统计/opt/aaa/a01.txt，一共有几行1cat /opt/aaa/a01.txt | wc -l用管道命令找到所有的含有hdfs单词的内容1grep \"hdfs\" /opt/aaa/a01.txt用管道命令统计含有spark单词的行，一共有几行1grep \"spark\" /opt/aaa/a01.txt | wc -l用管道命令统计spark单词有几个1grep -o \"spark\" /opt/aaa/a01.txt | wc -l查看系统中是否安装了hbase软件包1rpm -qa | grep hbase新建用户bbb，并查找/etc/passwd中是否存在该用户12useradd bbbgrep bbb /etc/passwd在vi编辑器中，用?命令，查找hello单词在vi编辑器中，用:s/单词1/单词2/g命令，将所有的hello，替换成hi1234567# 进入vi 使用如下命令vi /opt/aaa/a01.txt:%s/hello/hi/g:wq# 方法二sed -i s/hello/hi/g /opt/aaa/a01.txt用管道命令，统计包含hi的内容一共有多少行，并利用重定向命令将结果写入/opt/aaa/a02.txt1grep hi /opt/aaa/a01.txt | wc -l &gt;&gt; /opt/aaa/a02.txt Linux管道命令http://blog.imc.re/archives/1903.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"Linux环境设置","path":"/RSSBOX/rss/a0063800.html","content":"工作环境设置文件（1）系统环境/etc/profile(2) root用户的环境/root/.bash_profile将某一个目录/aaa加入工作环境文件，实现在任意位置均可执行该目录下面的程序（1）建立/aaa文件夹，并加入工作环境文件12[root@master ~]# cd /[root@master /]# mkdir /aaa(2) 在/etc/profile中插入下面的内容：1234567[root@master /]# vi /etc/profile # 安装flume环境export FLUME_HOME=/opt/apache-flume-1.7.0-binexport PATH=$PATH:$FLUME_HOME/bin# 将/aaa文件夹加入export AAA_HOME=/aaaexport PATH=$PATH:$AAA_HOME(3) 在/aaa中建立一个简单计时器脚本1234567891011121314[root@master /]# cd /aaa[root@master aaa]# vi time01.shecho \"start\"sleep 1;echo \"----5----\";sleep 1;echo \"----4----\";sleep 1;echo \"----3----\";sleep 1;echo \"----2----\";sleep 1;echo \"----1----\";sleep 1;echo \"stop\"[root@master aaa]# chmod 777 ./time01.sh[root@master aaa]# ll总用量 4-rwxrwxrwx. 1 root root 164 12月 11 08:28 time01.sh(4) 使修改的工作环境文件生效1[root@master aaa]# source /etc/profile(5)在任意位置均可执行：123456[root@master aaa]# cd /[root@master /]# time01.shstart----5--------4--------3----grep打印出所有符合指定规则的文本行123456[root@master /]# cd /aaa[root@master aaa]# ll总用量 4-rwxrwxrwx. 1 root root 164 12月 11 08:28 time01.sh[root@master aaa]# grep 'start' time01.shecho \"start\"正则表达式所有的正则表达式需要用单引号括起来. 匹配单个任意字符[list] 匹配字符串列表中的一个字符匹配前一个字符0次或多次^ 在行头匹配正则表达式重定向重定向是指不使用标准的输入输出接口，而进行重新指定&lt; 输入重定向或 &gt; 输出重定向2&gt;或2&gt;&gt; 错误重定向&amp;&gt; 同时实现输出重定向和错误重定向123[root@master ~]# ll /aaa总用量 4-rwxrwxrwx. 1 root root 164 12月 11 08:28 time01.sh将/aaa文件夹的ll显示的内容保存到/root的result01.txt中123456789101112131415[root@master ~]# ll /aaa &gt; result01.txt[root@master ~]# ll总用量 72drwxr-xr-x. 4 root root 36 11月 16 2022 aaa-rw-------. 1 root root 1257 11月 18 2021 anaconda-ks.cfgdrwxr-xr-x. 5 root root 39 11月 18 2021 localperl-rw-------. 1 root root 1623 7月 11 21:08 nohup.outdrwxr-xr-x. 5 root root 39 11月 18 2021 perl5-rw-r--r--. 1 root root 65 12月 11 09:09 result01.txt-rw-r--r--. 1 root root 50112 7月 11 21:31 ShiSai2022-1.0-SNAPSHOT.jar-rw-r--r--. 1 root root 0 11月 15 2022 spark-yarn-logs-rw-r--r--. 1 root root 4175 11月 16 2022 zookeeper.out[root@master ~]# cat result01.txt总用量 4-rwxrwxrwx. 1 root root 164 12月 11 08:28 time01.sh把/tmp里面的内容追加到result01.txt中1234567891011[root@master ~]# ll /tmp总用量 0drwx------. 3 root root 17 12月 11 08:07 systemd-private-03f98bcc8dcb4c7b8b38ca9ae50eaf49-chronyd.service-XdsvpJdrwx------. 2 root root 6 12月 11 08:07 vmware-root_8740-2865761114[root@master ~]# ll /tmp &gt;&gt; result01.txt[root@master ~]# cat result01.txt总用量 4-rwxrwxrwx. 1 root root 164 12月 11 08:28 time01.sh总用量 0drwx------. 3 root root 17 12月 11 08:07 systemd-private-03f98bcc8dcb4c7b8b38ca9ae50eaf49-chronyd.service-XdsvpJdrwx------. 2 root root 6 12月 11 08:07 vmware-root_8740-2865761114将文件的输出信息和错误信息保存到result02.txt中1234567891011[root@master ~]# date2023年 12月 11日 星期一 09:15:08 CST[root@master ~]# data-bash: data: 未找到命令[root@master ~]# data &amp;&gt; result02.txt[root@master ~]# cat result02.txt-bash: data: 未找到命令[root@master ~]# date &amp;&gt;&gt; result02.txt[root@master ~]# cat result02.txt-bash: data: 未找到命令2023年 12月 11日 星期一 09:16:48 CST课堂练习：在/opt下建立文件夹/opt/bbb，并将该文件夹写入工作环境文件/etc/profile12mkdir /opt/bbbecho \"export PATH=$PATH:/opt/bbb\" &gt;&gt; /etc/profile在/opt/bbb中建立一个可执行文件hello.sh,可以向屏幕中打印一行hello world字符1echo \"echo \"hello world\"\" &gt;&gt; /opt/bbb/hello.sh生效修改的工作环境文件/etc/profile1source /etc/profile在/root目录中直接输入hello.sh进行验证12cd /rootsh hello.sh用重定向符号将hello.sh的结果保存到/root/result001.txt中1sh hello.sh &gt;&gt; /root/result001.txt打印/root/result001.txt1cat /root/result001.txt用grep命令在/root/result001.txt中查找world字符1grep world /root/result001.txt Linux环境设置http://blog.imc.re/archives/1ba9.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"Linux变量使用","path":"/RSSBOX/rss/d28565de.html","content":"Linux变量使用变量的定义与引用，用=进行定义，用$进行引用12345[root@master ~]# name=zhangsan[root@master ~]# echo $namezhangsan[root@master ~]# echo namename$在单引号中不能被识别，在双引号中可识别1234[root@master ~]# echo \"I am $name\"I am zhangsan[root@master ~]# echo 'I am $name'I am $name环境变量PATH环境变量：寻找可执行文件的搜索路径12[root@master ~]# echo $PATH/root/perl5/bin:/root/perl5/bin:/opt/kafka_2.11-2.0.0/bin:/opt/zookeeper-3.4.12/bin:/opt/flink-1.10.2/bin:/opt/spark-2.1.1-bin-hadoop2.7//bin:/opt/apache-hive-2.3.4-bin/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/hadoop-2.7.7/bin:/opt/hadoop-2.7.7/sbin:/usr/java/jdk1.8.0_162/bin:/usr/java/jdk1.8.0_162/jre/bin:/opt/scala-2.11.0/bin:/opt/apache-flume-1.7.0-bin/bin:/root/binPWD：当前工作目录12[root@master ~]# echo $PWD/rootPS1和PS2：命令行的一级提示符和二级提示符123456789101112131415161718192021222324252627282930313233343536373839404142[root@master ~]# echo $PS1[\\u@\\h \\W]\\$[root@master ~]# echo $PS2&gt;[root@master ~]# echo $PS1[\\u@\\h \\W]\\$[root@master ~]# echo $PS2&gt;[root@master ~]# mysql -u root -pEnter password: # （输入Passwd123！）Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 3Server version: 5.7.18 MySQL Community Server (GPL)Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || bak || hive || mysql || performance_schema || shtd_result || shtd_store || shtd_store01 || sys |+--------------------+9 rows in set (0.02 sec)mysql&gt; exitBye[root@master ~]#OLDPWD：前一个工作目录123456789101112131415[root@master ~]# ll总用量 68drwxr-xr-x. 4 root root 36 11月 16 2022 aaa-rw-------. 1 root root 1257 11月 18 2021 anaconda-ks.cfgdrwxr-xr-x. 5 root root 39 11月 18 2021 localperl-rw-------. 1 root root 1623 7月 11 21:08 nohup.outdrwxr-xr-x. 5 root root 39 11月 18 2021 perl5-rw-r--r--. 1 root root 50112 7月 11 21:31 ShiSai2022-1.0-SNAPSHOT.jar-rw-r--r--. 1 root root 0 11月 15 2022 spark-yarn-logs-rw-r--r--. 1 root root 4175 11月 16 2022 zookeeper.out[root@master ~]# cd aaa[root@master aaa]# echo $PWD/root/aaa[root@master aaa]# echo $OLDPWD/rootHISTFILE：储存历史命令1234[root@master aaa]# echo $HISTFILE/root/.bash_history[root@master aaa]# vi /root/.bash_history （退出----按：，按q，按回车键）[root@master aaa]#HOME: 当前用户的用户目录12[root@master ~]# echo $HOME/root3.PATH环境变量的综合运用在PATH环境变量之前加入新的路径，/tmp123[root@master ~]# export PATH=/tmp:$PATH[root@master ~]# echo $PATH/tmp:/root/perl5/bin:/root/perl5/bin:/opt/kafka_2.11-2.0.0/bin:/opt/zookeeper-3.4.12/bin:/opt/flink-1.10.2/bin:/opt/spark-2.1.1-bin-hadoop2.7//bin:/opt/apache-hive-2.3.4-bin/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/hadoop-2.7.7/bin:/opt/hadoop-2.7.7/sbin:/usr/java/jdk1.8.0_162/bin:/usr/java/jdk1.8.0_162/jre/bin:/opt/scala-2.11.0/bin:/opt/apache-flume-1.7.0-bin/bin:/root/bin转到新的路径/tmp下面制造了一个可执行文件hello123456789[root@master ~]# cd /tmp[root@master tmp]# ll总用量 0drwx------. 3 root root 17 12月 4 08:08 systemd-private-29aad2a4b89b4c6b8901f17655036ac6-chronyd.service-IX3I1adrwx------. 2 root root 6 12月 4 08:08 vmware-root_8736-2857568991[root@master tmp]# vi hello# （按i键，看到屏幕底下有insert字样，此时输入）echo \"hello hadoop!\" # (按Esc键，屏幕底下的insert字样消失，再按：，按wq，按回车键，此时就保存退出了vi编辑器)12345678910111213141516171819202122232425[root@master tmp]# ll hello-rw-r--r--. 1 root root 21 12月 4 09:00 hello[root@master tmp]# chmod 777 hello [root@master tmp]# ll hello-rwxrwxrwx. 1 root root 21 12月 4 09:00 hello[root@master tmp]# cd /root[root@master ~]# echo $PATH/tmp:/root/perl5/bin:/root/perl5/bin:/opt/kafka_2.11-2.0.0/bin:/opt/zookeeper-3.4.12/bin:/opt/flink-1.10.2/bin:/opt/spark-2.1.1-bin-hadoop2.7//bin:/opt/apache-hive-2.3.4-bin/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/hadoop-2.7.7/bin:/opt/hadoop-2.7.7/sbin:/usr/java/jdk1.8.0_162/bin:/usr/java/jdk1.8.0_162/jre/bin:/opt/scala-2.11.0/bin:/opt/apache-flume-1.7.0-bin/bin:/root/bin[root@master ~]# hellohello hadoop![root@master ~]# cd /tmp[root@master tmp]# ll总用量 4-rwxrwxrwx. 1 root root 21 12月 4 09:00 hellodrwx------. 3 root root 17 12月 4 08:08 systemd-private-29aad2a4b89b4c6b8901f17655036ac6-chronyd.service-IX3I1adrwx------. 2 root root 6 12月 4 08:08 vmware-root_8736-2857568991[root@master tmp]# rm hellorm：是否删除普通文件 \"hello\"？y[root@master tmp]# ll总用量 0drwx------. 3 root root 17 12月 4 08:08 systemd-private-29aad2a4b89b4c6b8901f17655036ac6-chronyd.service-IX3I1adrwx------. 2 root root 6 12月 4 08:08 vmware-root_8736-2857568991[root@master tmp]# cd /root[root@master ~]# hello-bash: /tmp/hello: 没有那个文件或目录课堂练习（要求所有的命令均使用全局路径，将结果截图粘贴到桌面Release下面的word文件“模块A提交的结果”）：在根目录下面建立文件夹/bbb1mkdir /bbb在文件夹/bbb中建立文件hi.sh1touch /bbb/hi.sh在文件/bbb/hi.sh中，用vi编辑器输入10秒倒计时程序如下：1234567891011121314vi /bbb/hi.sh# 点击 i 键 进入 insert 编辑模式echo \"----start----\"echo \"----10----\";sleep 1;echo \"----9----\";sleep 1;echo \"----8----\";sleep 1;echo \"----7----\";sleep 1;echo \"----6----\";sleep 1;echo \"----5----\";sleep 1;echo \"----4----\";sleep 1;echo \"----3----\";sleep 1;echo \"----2----\";sleep 1;echo \"----1----\";sleep 1;echo \"----launch!----\"保存并退出。(点击Esc退出insert编辑模式后输入 `:wq` )设置hi.sh文件为可执行文件1chmod +x /bbb/hi.sh把/bbb路径添加在PATH环境变量中，并生效12echo \"PATH=\\$PATH:/bbb\" &gt;&gt; /etc/profilesource /etc/profile在系统任意位置，执行hi.sh命令，将结果粘贴。1234567891011121314[root@MySQL01 ~]# sh hi.sh----start--------10--------9--------8--------7--------6--------5--------4--------3--------2--------1--------launch!----[root@MySQL01 ~]# Linux变量使用http://blog.imc.re/archives/43a5.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"1729818333.1186638","path":"/RSSBOX/rss/ce4440a9.html","content":".fa-secondary&#123;opacity:.4&#125;数据通信网络数据通信网络开始阅读 1729818333.1186638https://blog.mhuig.top/p/notes-datacom/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"Linux综合练习","path":"/RSSBOX/rss/2ac8394e.html","content":"Linux综合课堂练习将命令和结果粘贴在桌面的release目录下，模块A提交的结果文件中。要求命令参数文件路径使用绝对路径。建立文件夹/aaa/a011mkdir -p /aaa/a01转移到/aaa/a01目录下，并显示当前路径1cd /aaa/a01 &amp;&amp; pwd在文件夹/aaa/a01下新建文件myHadoop01.txt，内容为： Hello Hadoop Hello HDFS Hello spark12345cat &gt;&gt; /aaa/a01/myHadoop01.txt &lt;&lt; EOFHello HadoopHello HDFSHello sparkEOF新建文件夹/bbb/b011mkdir -p /bbb/b01将文件/aaa/a01/myHadoop01.txt，复制到/bbb/b01目录下，并改名为myHadoop02.txt。1cp /aaa/a01/myHadoop01.txt /bbb/b01/myHadoop02.txt在/bbb/b01/myHadoop02.txt末尾，新增加一行内容：Hello Yarn1echo \"Hello Yarn\" &gt;&gt; /bbb/b01/myHadoop02.txt比较/aaa/a01/myHadoop01.txt和/bbb/b01/myHadoop02.txt的不同之处。1diff /aaa/a01/myHadoop01.txt /bbb/b01/myHadoop02.txt打印/aaa/a01/myHadoop01.txt的前两行内容。1head -n 2 /aaa/a01/myHadoop01.txt打印/bbb/b01/myHadoop02.txt的最后1行内容。1tail -n 1 /bbb/b01/myHadoop02.txt将文件夹/aaa，拷贝为/ccc1cp -r /aaa /ccc删除/ccc/a01/myHadoop01.txt文件。1rm /ccc/a01/myHadoop01.txt分别删除文件夹/ccc/a01，/ccc12rmdir /ccc/a01rmdir /ccc为文件/aaa/a01/myHadoop01.txt，建立软连接/aaa/a01/myHadoop1ln -s /aaa/a01/myHadoop01.txt /aaa/a01/myHadoop用tar命令将/bbb/b01/myHadoop02.txt压缩为/bbb/b01/myHadoop02.tar.tz12cd /bbb/b01tar cvzf myHadoop02.tar.tz myHadoop02.txt查找电脑中所有以myHadoop字符开头的文件。1find / -name \"myHadoop*\"查找/aaa/a01/myHadoop01.txt中所有包含Hadoop的行。1grep \"Hadoop\" /aaa/a01/myHadoop01.txt显示/aaa/a01/myHadoop01.txt的属性，并用文字说明该文件的文件所有者、同组用户、其他用户分别具有什么权限。123456root@RC:/# ll /aaa/a01/总用量 12drwxr-xr-x 2 root root 4096 11月 28 08:12 ./drwxr-xr-x 3 aaa root 4096 11月 28 08:04 ../lrwxrwxrwx 1 root root 23 11月 28 08:12 myHadoop01 -&gt; /aaa/a01/myHadoop01.txt*-rw-r--r-- 1 root root 36 11月 28 08:07 myHadoop01.txt*文件所有者：读写权限同组用户：读权限其他用户：读权限用数字的方式，将myHadoop01.txt的权限设置为文件所有者可读不可写可执行，同组用户只可读，其他用户只可执行。1chmod 541 /aaa/a01/myHadoop01.txt用字母的方式，将myHadoop01.txt的权限设置为一切用户均可读可写可执行。1chmod a=rwx /aaa/a01/myHadoop01.txt新建用户aaa1useradd aaa将文件夹/aaa的所有者设置为aaa用户，并显示/aaa的属性1chown aaa /aaa Linux综合练习http://blog.imc.re/archives/d4fd.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"Linux 权限操作","path":"/RSSBOX/rss/2b561629.html","content":"Linux 权限操作1.用字母的方式改变权限用u表示文件所有者，g表示同组用户，o表示其他用户用r表示可读，用w表示可写，用x表示可执行，文件的权限就可以表示为字母等式的组合1[root@master ~]# touch a01.txt使文件所有者具有可读可写可执行：1[root@master ~]# chmod u=rwx a01.txt使同组用户和其他用户可读不可写可执行：1[root@master ~]# chmod go=rx a01.txt在原来的基础上，为同组用户增加可写属性：1[root@master ~]# chmod g+w a01.txt在原来的基础上，收回其他用户可执行的属性：1[root@master ~]# chmod o-x a01.txt文件所有者可读不可写可执行，同组用户收回可写属性，其他用户增加可写属性：1[root@master ~]# chmod u=rx,g-w,o+w a01.txt2.修改文件的所有者与所属组chown增加用户aaa：1[root@master ~]# useradd aaa将文件a01.txt的所有者改为aaa：123[root@master ~]# chown aaa a01.txt [root@master ~]# ll a01.txt-r-xr-xrw-. 1 aaa root 0 11月 27 08:15 a01.txtLinux权限数字表达由 智谱清言 生成回答, 可能含有不正确的内容, 如有错误欢迎指正.在Linux中，文件和目录的权限可以用数字来表示，这些数字对应于权限的读（r）、写（w）和执行（x）。以下是各个权限对应的数字：读（r）：4写（w）：2执行（x）：1无权限（-）对应的数字是0。权限的设置是通过组合这些数字来实现的。例如，读和执行权限的组合（r-x）对应的数字是5（4+1），读和写权限的组合（rw-）对应的数字是6（4+2）。在Linux中，权限的设置通常遵循以下模式：第一位数字：文件所有者的权限第二位数字：文件所属组的权限第三位数字：其他用户的权限例如，权限模式755表示：所有者（owner）：7（读、写、执行权限）组（group）：5（读、执行权限，无写权限）其他用户（others）：5（读、执行权限，无写权限）在Linux中，还有一些特殊的权限设置：600（rw———-）：所有者有读写权限，组和其他用户无权限。644（rw-r—r—）：所有者有读写权限，组和其他用户只有读权限。666（rw-rw-rw-）：所有者有读写权限，组和其他用户也有读写权限（但通常不会同时给予）。700（rwx———）：所有者有读写执行权限，组和其他用户无权限。744（rwxr—r—）：所有者有读写执行权限，组和其他用户只有读权限。755（rwxr-xr-x）：所有者有读写执行权限，组和其他用户有读和执行权限，但无写权限。777（rwxrwxrwx）：所有者有读写执行权限，组和其他用户也有读写执行权限（但通常不会同时给予）。在实际操作中，可以使用chmod命令来设置或修改文件和目录的权限。例如：1chmod 755 file.txt这个命令会将文件file.txt的权限设置为所有者可读写执行，组和其他用户可读执行。课堂练习：将命令和结果粘贴在桌面的release目录下，模块A提交的结果文件中。要求命令参数文件路径使用绝对路径。建立文件夹/aaa/a011mkdir -p /aaa/a01转移到/aaa/a01目录下，并显示当前路径1cd /aaa/a01 &amp;&amp; pwd在文件夹/aaa/a01下新建文件myHadoop01.txt，内容为： Hello Hadoop Hello HDFS Hello spark12345cat &gt;&gt; /aaa/a01/myHadoop01.txt &lt;&lt; EOFHello HadoopHello HDFSHello sparkEOF新建文件夹/bbb/b011mkdir -p /bbb/b01将文件/aaa/a01/myHadoop01.txt，复制到/bbb/b01目录下，并改名为myHadoop02.txt。1cp /aaa/a01/myHadoop01.txt /bbb/b01/myHadoop02.txt在/bbb/b01/myHadoop02.txt末尾，新增加一行内容： Hello Yarn1echo \"Hello Yarn\" &gt;&gt; /bbb/b01/myHadoop02.txt比较/aaa/a01/myHadoop01.txt和/bbb/b01/myHadoop02.txt的不同之处。1diff /aaa/a01/myHadoop01.txt /bbb/b01/myHadoop02.txt打印/aaa/a01/myHadoop01.txt的前两行内容。1head -n 2 /aaa/a01/myHadoop01.txt打印/bbb/b01/myHadoop02.txt的最后1行内容。1tail -n 1 /bbb/b01/myHadoop02.txt将文件夹/aaa，拷贝为/ccc1cp -r /aaa /ccc删除/ccc/a01/myHadoop01.txt文件。1rm -rf /ccc/a01/myHadoop01.txt分别删除文件夹/ccc/a01，/ccc12rmdir /ccc/a01rmdir /ccc为文件/aaa/a01/myHadoop01.txt，建立软连接/aaa/a01/myHadoop1ln -s /aaa/a01/myHadoop01.txt /aaa/a01/myHadoop用tar命令将/bbb/b01/myHadoop02.txt压缩为/bbb/b01/myHadoop02.tar.tz1tar -C /bbb/b01/ -czvf /bbb/b01/myHadoop02.tar.tz myHadoop02.txt查找电脑中所有以myHadoop字符开头的文件。1find / -name myHadoop*查找/aaa/a01/myHadoop01.txt中所有包含Hadoop的行。1grep \"Hadoop\" /aaa/a01/myHadoop01.txt显示/aaa/a01/myHadoop01.txt的属性，并用文字说明该文件的文件所有者、同组用户、其他用户分别具有什么权限。12[root@MySQL01 aaa]# ll /aaa/a01/myHadoop01.txt-rw-r--r--. 1 root root 36 1月 1 12:48 /aaa/a01/myHadoop01.txt文件所有者: 读写权限同组用户,其他用户: 读权限用数字的方式，将myHadoop01.txt的权限设置为文件所有者可读不可写可执行，同组用户只可读，其他用户只可执行。1chmod 541 /aaa/a01/myHadoop01.txt用字母的方式，将myHadoop01.txt的权限设置为一切用户均可读可写可执行。1chmod a=rwx /aaa/a01/myHadoop01.txt新建用户aaa1useradd aaa将文件夹/aaa的所有者设置为aaa用户，并显示/aaa的属性1chown aaa /aaa &amp;&amp; stat /aaa Linux 权限操作http://blog.imc.re/archives/e5ff.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"Linux查找作业解析","path":"/RSSBOX/rss/579d1b0.html","content":"Linux查找作业解析Linux 20231121 复习内容课堂练习：把hadoop01镜像转到20230912，并启动hadoop01，用Ubuntu进行联机。完成下面的操作，并把命令和结果截图粘贴到ubuntu桌面下的目录Release的“模块A提交的结果.docx”。要求所有命令均使用绝对路径。在/root目录下建立子目录bbb1mkdir /root/bbb在/root/bbb目录下建立文件b01.txt1touch /root/bbb/b01.txt在b01.txt中插入以下内容： Hello Hadoop Hello hdfs Hello spark Hello flink Hello yarn Hello hbase123456echo \"Hello Hadoop\" &gt;&gt; /root/bbb/b01.txtecho \"Hello hdfs\" &gt;&gt; /root/bbb/b01.txtecho \"Hello spark\" &gt;&gt; /root/bbb/b01.txtecho \"Hello flink\" &gt;&gt; /root/bbb/b01.txtecho \"Hello yarn\" &gt;&gt; /root/bbb/b01.txtecho \"Hello hbase\" &gt;&gt; /root/bbb/b01.txt将b01.txt拷贝为b02.txt，并在b02.txt中新增两行内容： Hello Redis Hello clickhouse123cp /root/bbb/b01.txt /root/bbb/b02.txtecho \"Hello Redis\" &gt;&gt; /root/bbb/b02.txtecho \"Hello clickhouse\" &gt;&gt; /root/bbb/b02.txt用diff命令比较b01.txt、b02.txt的不同之处1diff /root/bbb/b01.txt /root/bbb/b02.txt用grep命令查找b01.txt中含有flink的行1grep flink /root/bbb/b01.txt修改/root/bbb/b01.txt的权限：文件所有者可读可写可执行，同组用户可读可写不可执行，其他用户不可读不可写不可执行，并用ls命令查看结果12chmod 760 /root/bbb/b01.txtls修改/root/bbb/b02.txt的权限：文件所有者可读不可写可执行，同组用户可读不可写不可执行，其他用户不可读不可写可执行，并用ls命令查看结果12chmod 541 /root/bbb/b02.txtls用find命令查找b01.txt文件的位置1find / -name b01.txt用whereis命令查找grep命令文件的位置1whereis grep用tar命令将到/opt/hadoop-2.7.7.tar.gz解压到/usr/local/src下面。1tar -xvzf /opt/hadoop-2.7.7.tar.gz -C /usr/local/src用find命令查找文件stop-all.sh的位置。1find / -name stop-all.sh课后笔记关于上述题目同学们有部分易错点，在此记录：绝对路径和相对路径的概念还是不熟悉：绝对路径是以 / 开头，代表从根目录 / 开始。相对目录为 ./ 或者不加前缀。 例：假设当前目录为/root ， /bbb 就是以根目录开始，与 /root 同级，如果是 aaa 或者 ./bbb 就是在当前文件夹下 /root 创建，绝对路径为 /root/aaa /root/bbbecho的用法掌握不熟练：echo的指令根据格式不同会有不同的功能，echo \"Hello Hadoop &gt; a.txt\" 是将 a.txt 中的文件内容替换覆盖为指定内容，而 echo \"Hello Hadoop &gt;&gt; a.txt\" 是在 a.txt 中的文件末尾添加内容，区别为 &gt; 覆盖，&gt;&gt; 添加，使用错误会让文件内容全部消失，切记！创建文件命令混淆： 创建文件夹的命令为 mkdir ，创建文件的命令为 touch，一个是文件夹，一个是文件，不要搞混。如果搞混后需要删除使用如下指令：rmdir 删除文件夹，rm 删除文件权限设置内容：可以参考：Linux chmod命令扩展内容：如果认为上述操作太过简单，可以参考下方扩展内容增加效率使用 cat 命令插入多行内容（以第4题为例）：12345678cat &gt;&gt; /root/bbb/b01.txt &lt;&lt;EOFHello HadoopHello hdfsHello sparkHello flinkHello yarnHello hbaseEOF使用 history 命令查看历史执行指令（可以用于回滚查找操作截图，但是只有命令，没有输出） Linux查找作业解析http://blog.imc.re/archives/d40e.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"中慧1+X证书真题","path":"/RSSBOX/rss/f04fdde.html","content":"所属院校：厦门南洋职业学院2022年Python 1+X中级考试真题(理论)单选题每小题2分，共60 分11Django模板的变量名在HTML中用（ ）定义。1A：[[ ]]1B：&#123;&#123; &#125;&#125;1C：&#123; &#125;1D：[ ]答案：B问题解析：Na21爬虫代理的可以编写为（ ）。1A：参数是一个序列&#123;'类型'：'代理ip：端口号'&#125;1B：参数是一个字典&#123;'类型'：'代理ip：端口号'&#125;1C：参数是一个字典&#123;'属性'：'代理ip：端口号'&#125;1D：参数是一个序列&#123;'属性'：'代理ip：端口号'&#125;答案：B问题解析：Na31关于MongoDB数据库，下列说法不正确的是（ ）1A：MongoDB是关系型数据库1B：MongoDB是一个基于分布式文件存储的数据库1C：MongoDB数据库中每个文档都有_id字段1D：MongoDB数据库支持海量数据存储答案：A问题解析：Na41下列关于Navicat说法，错误的是（ ）。1A：支持SQL Server数据库可视化管理1B：是一种NoSQL数据库1C：支持MySQL数据库可视化管理1D：支持MongoDB数据库可视化管理答案：B问题解析：Na51itempipeline的一些典型应用不包括（ ）。1A：验证爬取的数据（检查item包含某些分段）1B：将爬取结果保存到数据库中1C：整理HTML、CSS和JS内容1D：查重答案：C问题解析：Na61在MySQL数据库中，以下能够删除一列的是（ ）1A：alter table user delete age1B：alter table user drop column age1C：alter table user delete column age1D：alter table user remove age答案：B问题解析：Na71对于MySQL的一个学生表，其生日用以下哪种数据类型更合适（ ）。1A：DATE1B：DATETIME1C：YEAR1D：TIMESTAMP答案：A问题解析：Na81查询MySQL数据库World中的country表的前10条记录，显示“Name”、“Region”两个字段，下列正确的SQL语句是（ ）。1A：SELECT Name,Region FROM country LIMIT 101B：SELECT * FROM WorldLIMIT 101C：SELECT top 100 * FROM `country`1D：DELETE FROM country WHERE Name&gt;10答案：A问题解析：Na91Django创建名为TestModel应用的语法是（ ）。1A：python manage.py makemigrations TestModel1B：django-admin startproject TestModel1C：python manage.py migrate TestModel1D：python manage.py startapp TestModel答案：D问题解析：Na101Scrapy-redis提供了四种组件，其中不包括（ ）。1A：Scheduler1B：Item Pipeline1C：Base Spider1D：Mutiple Filter答案：D问题解析：Na111MongoDB切换数据库的命令是（ ）1A：find1B：use1C：enter1D：select答案：B问题解析：Na121ORM的Models类对应于数据库的（ ）1A：属性1B：一条记录1C：数据表1D：字段答案：C问题解析：Na131MongoDB数据库集合的find()命令，如果未指定查询参数，则（ ）。1A：执行命令时会报错1B：读取数据库集合的所有记录1C：读取数据库集合的第一条记录1D：返回nil答案：B问题解析：Na141提高Selenium脚本的执行速度描述中错误的是（ ）。1A：优化代码1B：对于firefox浏览器，考虑使用测试专用的profile，因为每次启动浏览器的时候firefox会创建1个新的profile，对于这个新的profile，所有的静态资源都是从服务器直接下载，而不是从缓存里加载，这就导致网络不好的时候用例运行速度特别慢的问题1C：使用更高配置的电脑和选择更快的网络环境1D：可以随便加sleep，使用显式等待答案：D问题解析：Na151Selenium中没有提供原生的方法判断元素是否存在，一般我们可以通过（ ）判断。1A：定位元素+异常捕获1B：定位属性+非空1C：定位元素+非空1D：定位属性+异常捕获答案：A问题解析：Na161一般网站从三个方面反爬虫，其中不包括（ ）。1A：用户行为1B：网站目录和数据加载方式1C：用户点击网页1D：用户请求的Headers答案：C问题解析：Na171SELECT语句的完整语法比较复杂，但至少包含以下哪个部分（ ）1A：SELECT，GROUP1B：SELECT，INTO1C：SELECT，FROM1D：仅SELECT答案：C问题解析：Na181MySQL数据库中，以下聚合函数求数据总和的是（ ）。1A：AVG1B：COUNT1C：MAX1D：SUM答案：D问题解析：Na191MySQL数值型数据类型中有符号TINYINT的范围是（ ）。1A：(0，255)1B：(-128，127)1C：(0，65 535)1D：(-32 768，32 767)答案：B问题解析：Na201当生成一个spider继承redisspider时，会调用setup_redis函数，这个函数回去连接redis数据库，然后会设置（ ）。1A：response1B：singals信号1C：token信令1D：request请求答案：B问题解析：Na211反爬虫应对策略不包含下列哪项？( )。1A：反爬虫自己会好的1B：设置延迟爬虫sleep()1C：更换爬虫IP1D：添加headers信息答案：A问题解析：Na221向一个尚不存在的MongoDB数据库集合执行插入文档操作，下列说法正确的是（ ）。1A：返回false1B：插入成功，自动创建集合1C：返回nil1D：导致报错答案：B问题解析：Na231MongoDB命令dB. CollectionName.remove(&#123;key:value&#125;, &#123;justOne&#125;)的作用是（ ）。1A：删除集合CollectionName下的一条文档1B：查询集合CollectionName下的一条文档，并移动到justOne集合中1C：删除名为CollectionName的集合1D：删除集合CollectionName下的所有文档答案：A问题解析：Na241Django请求对象属性中用于获取当前请求方式的关键字是（ ）1A：path1B：body1C：method1D：POST答案：C问题解析：Na251要启用spider中间件，您可以将其加入到 SPIDER_MIDDLEWARES 设置中。该设置是一个（ ），键为中间件的路径，值为中间件的顺序(order)。1A：堆栈1B：item1C：字典1D：队列答案：C问题解析：Na261元素找不到可能的原因不包括（ ）。1A：动态id定位不到元素1B：在同一个iframe中查找元素1C：xpath描述错误1D：iframe原因定位不到元素答案：B问题解析：Na271MongoDB服务器默认端口是（ ）1A：270171B：80801C：63791D：3306答案：A问题解析：Na281以下哪个不属于Scrapy框架的优点？（ ）。1A：它由Spiders、ItemPipeline、Downloader、Scoop组成1B：它更容易构建大规模的抓取项目1C：它异步处理请求，速度非常快1D：它可以使用自动调节机制自动调整爬行速度答案：A问题解析：Na291使用“CREATE TABLE”语句创建MySQL数据库的表，下列说法正确的是（ ）1A：如果指定创建的表已经存在，则SQL语句执行失败1B：不能在两个不同的数据库里创建相同名称的表1C：创建数据库表时，不能指定主键1D：如果指定创建的表已经存在，则覆盖原有表答案：A问题解析：Na301SQL语句中的条件查询用以下拿一项（ ）1A：THEN1B：WHERE1C：WHILE1D：IF答案：B问题解析：Na多选题每小题2分，共20 分11如果AppModel是一个Django模型类，则修改数据的方法正确的有（ ）。123A：a1 = AppModel.objects.get(id=1)a1.name = ‘zhangsan’a1.save()1B：AppModel.objects.all().modify(name=’abc’)1C：AppModel.objects.all().update(name=’wangwu’)1D：AppModel.objects.filter(id=1).update(name=’lisi’)答案：A,C,D问题解析：Na21以下对Django MVT模式解释正确的是（）1A：V全拼为View，用于模本渲染，生成页面展示的HTML内容。1B：M全拼为Model，与MVC中的M功能相同，负责和数据库交互。1C：V全拼为View，与MVC中的C功能相同，接收请求，返回响应。1D：T全拼为Template，与MVC中的V功能相同，负责封装构造渲染需要返回的HTML页面。答案：B,C,D问题解析：Na31下列关于xpath表达式描述正确的有（）1A：//title[@lang = 'en'] 选取拥有lang属性并且值为‘en’的所有title元素。1B：//bookstore/book[position()&lt;3] 选取最前面的三个属于bookstore元素的子元素的book元素。1C：//bookstore/book[last()-1] 选取属于bookstore子元素的倒数第二个book元素。1D：//bookstore/book[1] 选取属于bookstore子元素的第二个book元素。答案：A,C问题解析：Na41关于Django中的redirect()的参数可以是（ ）。1A：一个绝对的或相对的URL，将原封不动的作为重定向的位置1B：一个视图，可以带有参数：将使用urlresolvers.reverse 来反向解析名称1C：一个元组，元组内是一系列URL1D：一个列表，列表内存放的是一系列的URL答案：A,B问题解析：Na51关于Django路由中的path()和re_path()函数的描述正确的是（ ）。1A：path()函数如果手动添加正则首位限制符号也可用于正则路径1B：path()函数用于普通路径1C：path()函数和Django 1.1.x版本的url()函数用法相同1D：re_path()函数用于正则路径答案：B,D问题解析：Na61Django模板取值的方法正确的是（ ）。1A：在模板中取出列表某个元素可以使用下标1B：视图可以传递列表，在模板中可以直接显示列表1C：视图可以传递字典，在模板中可以直接显示字典1D：在模板中可以通过字典的键得到相应的值答案：A,B,C,D问题解析：Na71关于Django中的HttpRequest对象的属性，错误的是（ ）。1A：POST包含所有HTTP POST参数的列表对象1B：GET包含所有HTTP GET参数的列表对象1C：body表示请求体。1D：method属性返回请求中使用的HTTP方法的字符串表示，全大写表示答案：A,B问题解析：Na81ORM和数据库的对应关系正确的是（ ）。1A：方法对应字段1B：对象实例对应一条记录1C：Models类对应数据表1D：属性对应字段答案：B,C,D问题解析：Na91创建Django项目后，其中的文件的意义描述正确的有（ ）。1A：settings.py 为Django项目的配置文件，里面包含了项目引用的Django组件、项目名等。1B：urls.py为路由系统，主要维护项目的URL路由映射，即定义客户端访问的URL1C：__init__.py告诉Python该目录是一个Python模块，创建后暂无内容1D：wsgi.py定义WSGI的接口信息，用于与其他Web服务器集成，一般无需改动答案：A,B,D问题解析：Na101对requests库描述正确的是（）1A：任何类型的请求都可以通过requests.get()获取数据1B：使用requests.post()发送请求时可以通过data参数传递请求数据1C：可以使用requests.get()发送GET请求1D：使用requests.get()发送请求时可以通过data参数传递请求数据答案：B,C问题解析：Na判断题每小题2分，共20 分11Django的视图函数至少要有一个参数用来接收请求对象（ ）正确错误答案：正确问题解析：Na21MongoDB是一个基于分布式文件存储的数据库。是由C++语言编写（ ）正确错误答案：正确问题解析：Na31我们使用requests库发送post请求的时候，可以通过params关键字传递参数（ ）正确错误答案：错误问题解析：Na41Django是Python的一个Web开发框架，底层是基于C语言实现的。（ ）正确错误答案：错误问题解析：Na51Django在创建应用后需要将应用注册到项目中，才能将应用中的模型类映射为数据表（ ）正确错误答案：正确问题解析：Na61构建Django模型时需要在settings.py文件中修改DATABASE的配置项，里面加上数据库引擎、数据主机、用户、密码等。（ ）正确错误答案：正确问题解析：Na71在Scrapy项目中，我们可以通过yield关键字将item对象传递到管道，然后对数据进行持久化存储（ ）正确错误答案：正确问题解析：Na81我们可以使用Scrapy-Redis框架实现分布式爬虫，大规模的采集数据，但它的缺点是数据只能保存到Redis中，不能保存到MySQL中（ ）正确错误答案：错误问题解析：Na91我们在开发中大量采用MySQL数据库的原因是因为它是基于内存的数据库，效率高，速度快（ ）正确错误答案：错误问题解析：Na101Django MVT架构模式中的V指的是View，它的作用是渲染HTML内容，让用户能够看到一个美观的页面（ ）正确错误答案：错误问题解析：Na 中慧1+X证书真题http://blog.imc.re/archives/7daf.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"中慧1+X证书题库","path":"/RSSBOX/rss/aa1b1a45.html","content":"所属院校：厦门南洋职业学院Python程序开发模拟卷(中级1120)单选题每小题2分，共60 分11提高Selenium脚本的执行速度描述中错误的是（ ）。1A：使用更高配置的电脑和选择更快的网络环境1B：对于firefox浏览器，考虑使用测试专用的profile，因为每次启动浏览器的时候firefox会创建1个新的profile，对于这个新的profile，所有的静态资源都是从服务器直接下载，而不是从缓存里加载，这就导致网络不好的时候用例运行速度特别慢的问题1C：优化代码1D：可以随便加sleep，使用显式等待答案：D问题解析：Na21删除Redis当前数据库的命令为（ ）。1A：FLUSHALL1B：DB. Remove1C：DB. Drop1D：FLUSHDB答案：D问题解析：Na31Django在视图中将数据传递给模板可以通过哪种方式（ ）。1A：return render(request,\"xxx.html\", &#123;key: value&#125;)1B：return render(request,\"xxx.html\", “name”)1C：return render(request,\"xxx.html\", 20)1D：return render(request,\"xxx.html\", [1, 2, 3])答案：A问题解析：Na41Django创建名为TestModel应用的语法是（ ）。1A：python manage.py makemigrations TestModel1B：python manage.py startapp TestModel1C：django-admin startproject TestModel1D：python manage.py migrate TestModel答案：B问题解析：Na51以下操作不可以应对反爬的是( )。1A：发送请求之后加上time.sleep(random.randint(1, 2))1B：requests.get()函数中为headers参数设置合理的User-Agent1C：一台计算机上开启多线程加快发送请求的速度1D：requests.get()函数中为proxies设置合适的代理答案：C问题解析：Na61MySQL字符数据类型不包括( )。1A：TINYBLOB1B：CHAR1C：VARCHAR1D：BIGBLOB答案：D问题解析：Na71ORM是指（ ）。1A：对象、联系、模型1B：关系、对象、模型1C：映射、对象、联系1D：对象、关系、映射答案：D问题解析：Na81以下属于pymongo删除数据方法的是( )。1A：delete_all()1B：drop_one()1C：delete_one()1D：drop_many()答案：C问题解析：Na91查询MySQL数据库World中的country表的前10条记录，显示“Name”、“Region”两个字段，下列正确的SQL语句是（ ）。1A：SELECT Name,Region FROM country LIMIT 101B：DELETE FROM country WHERE Name&gt;101C：SELECT * FROM WorldLIMIT 101D：SELECT top 100 * FROM `country`答案：A问题解析：Na101假如Student是一个Django模型类，利用该模型类删除数据的操作错误的是( )。1A：Student.objects.all().delete()1B：Student.objects.filter(id=1).delete()1C：Student.objects.query_all().delete()12D：stu=Student.objects.get(id=1)stu.delete()答案：C111关于Redis的Hash的说法错误的是( )。1A：HSET可设置字典的一个键值对1B：HDROP可用于删除键1C：HGETALL可获取字典的所有键值对1D：Hash是Redis的一种字典存储数据结构，一个Hash对象可以存储多个键-值对元素，底层由哈希表实现答案：B问题解析：Na121关于Selenium键盘操作，描述错误的是（ ）。1A：send_keys(Keys.BACK_SPACE)表示删除键1B：send_keys(Keys.ENTER)表示回车键1C：send_keys(Keys.CONTROL，'c')表示复制1D：send_keys(Keys.ESCAPE)表示空格键答案：D问题解析：Na131现有一个stuinfo表,其中包含name(VARCHAR)、gender(INT)、birthday(DATE)、address(VARCHAR)、class(INT)几个字段，以下插入语句正确的是( )。1A：NSERT INTO stuinfo VALUES ('chen',1,'1999-5-1','北京市西直门大街123号',5);1B：NSERT INTO stuinfo VALUES ('chen',1,'1999-5-1','北京市西直门大街123号');1C：NSERT INTO stuinfo (name,sex,birthday,address) VALUES ('chen', 1, 1999-5-1, '北京市西直门大街123号', 5);1D：NSERT INTO stuinfo (name,sex,birthday,address,class) VALUES ('chen',1,'1999-5-1',5);答案：A问题解析：Na141Selenium中获取页面相关信息的描述，不正确的是( )。1A：元素的get_attribute()方法可以获取元素的属性值1B：元素的text属性可以获取元素的文本信息1C：元素的title属性可以获取网页标题1D：元素得到size属性可以获取元素的尺寸答案：C问题解析：Na151在Scrapy项目中创建爬虫的命令是（ ）。1A：scrapy genspider 爬虫名 网页域名1B：scrapy genspider 爬虫名 网页的URL1C：scrapy startspider 爬虫名 网页域名1D：scrapy startspider 爬虫名 网页的URL答案：A问题解析：Na161SQL是以下哪三个单词的缩写( )。1A：Selected Quiet Language1B：Structured Query Language1C：Structured Quiet Language1D：Selected Query Language答案：B问题解析：Na171关于Redis的说法，错误的是（ ）。1A：Redis服务端程序为redis-server.exe1B：启动Redis可以通过CMD命令和手动方式启动1C：可发送PING命令测试服务器，正常服务器回复OK1D：Redis客户端程序为redis-cli.exe答案：C问题解析：Na181scrapy-Redis分布式策略中，爬虫继承了RedisSpider，它能够支持分布式的抓取，采用的是basic spider，需要写parse函数。其次就是不再有start_urls了，取而代之的是（ ）。1A：redis_key1B：scheduler1C：key-values1D：setting答案：A问题解析：Na191Redis数据库中一个字符串类型的值能存储最大容量是多少( )。1A：128M1B：64M1C：512M1D：256M答案：C问题解析：Na201Django请求对象属性中用于获取当前请求方式的关键字是（ ）1A：method1B：POST1C：path1D：body答案：A问题解析：Na211关于Redis说法，不正确的是（ ）。1A：速度快1B：单线程操作1C：基于内存存储1D：不支持键-值对数据存储答案：D问题解析：Na221Scrapy框架基于管道的持久化存储流程，描述有误的是哪项( )。1A：爬虫文件抓取到数据后，将数据封装到items对象中1B：settings.py中开启管道1C：使用return将items对象交给pipelines管道进行持久化操作1D：管道文件中使用process_item方法将爬虫文件提交过来的item对象数据进行持久化存储答案：C问题解析：Na231MongoDB数据库操作错误的是( )。1A：show databases 可以显示数据库1B：use 数据库名 可以切换数据库，但不能切换到一个不存在的数据库1C：show dbs 可以显示数据库1D：db.dropDatabase()可以删除数据库答案：B问题解析：Na241SQL语言中数据定义语言DDL中包括哪个语句（ ）。1A：grant1B：select1C：DROP1D：DELETE答案：C问题解析：Na251Django模板中单行注释语法为( )。1A：//1B：/* */1C：#1D：&#123;# ... #&#125;答案：D问题解析：Na261关于Django路由反向解析的说法错误的是（ ）。1A：在模板 templates 中的 HTML 文件中，利用 &#123;% \"路由别名\" %&#125; 反向解析，如：&lt;form action=\"&#123;% 'login' %&#125;\" method=\"post\"&gt;1B：在 views.py 中，从 django.urls 中引入 reverse，利用 reverse(\"路由别名\") 反向解析，如：return redirect(reverse(\"login\"))1C：在urls.py中给路由起别名，name=”路由别名”，如：path(\"login1/\", views.login, name=\"login\")1D：反向解析，开发者可以增加代码的可维护性，使用URL反向解析使得开发者在许多需要写URL绝对路径的地方用映射名来代替答案：A问题解析：Na271MySQLdb连接数据库的语法为bObj=MySQLdB. connect(ServerName,User,PWD,DbName,charset=CharsetName)，以下关于它的描述错误的是（ ）。1A：DbName表示表名1B：ServerName表示服务器名1C：PWD表示密码1D：User表示用户名答案：A问题解析：Na281进入Django项目的交互式环境的命令是( )。1A：python manage.py runserver1B：python manage.py shell1C：python manage.py makemigrations1D：python manage.py migrate答案：B问题解析：Na291向一个尚不存在的MongoDB数据库集合执行插入文档操作，下列说法正确的是（ ）。1A：返回false1B：导致报错1C：返回nil1D：插入成功，自动创建集合答案：D问题解析：Na301Scrapy中Downloader Middleware的核心方法不包括（ ）。1A：process_response(request，response，spider)1B：process_spider(request，spider)1C：process_request(request，spider)1D：process_exception(request，exception，spider)答案：B问题解析：Na多选题每小题2分，共20 分11关于Django路由中的path()和re_path()函数的描述正确的是（ ）。1A：path()函数和Django 1.1.x版本的url()函数用法相同1B：path()函数用于普通路径1C：re_path()函数用于正则路径1D：path()函数如果手动添加正则首位限制符号也可用于正则路径答案：B,C问题解析：Na21如果AppModel是一个Django模型类，则删除数据的方法正确的有（ ）。1A：AppModel.objects.filter(id=1).delete()12B：a1 = AppModel.objects.get(id=1)a1.delete()1C：AppModel.objects.delete()1D：AppModel.objects.all().delete()答案：A,B,D31下列哪些列类型是MySQL数值型的数据( )。1A：DOUBLE1B：FLOAT1C：INT1D：SET答案：A,B,C问题解析：Na41以下Django框架的描述正确的有（ ）。1A：免费开源1B：集成了众多功能强大的模块1C：Django诞生于2003年，是目前Python语言影响力最高和最成熟的网络框架1D：开发效率高答案：A,B,C,D问题解析：Na51关于MongoDB的说法，正确的是（ ）。1A：由C++编写1B：提供可扩展的高性能数据存储解决方法1C：是一种基于分布式文件的开源数据库系统1D：可添加节点保证服务器性能答案：A,B,C,D问题解析：Na61关于Scrapy项目中的文件的描述正确的是（ ）。1A：pipelines.py：数据处理文件，对爬取到的数据进行处理保存等1B：settings.py：项目配置文件。定义了项目设置文件路径、部署信息等内容1C：items.py：定义Item数据结构的文件。在此可以编写所有的Item数据定义1D：scrapy.cfg：项目设置文件，可以定义项目的全局设置，比如USER_AGENT，ROBOTSTXT_OBEY等答案：A,C问题解析：Na71MySQL中关于模糊查询的说法正确的是（ ）。1A：由于模糊查询具有极大的便利性，因此在SQL中应推荐使用1B：‘张%’表示以‘张’开头的记录1C：可以使用LIKE关键字1D：‘_德_’表示三个字且中间为‘德’的记录答案：B,C,D问题解析：Na81Navicat支持哪些数据库的可视化管理（ ）。1A：Oracle1B：MySQL1C：MongoDB1D：SQL Server答案：A,B,C,D问题解析：Na91关于Django视图的描述正确的是（ ）。1A：视图函数可以查询数据库1B：一个视图函数（类），简称视图，就是一个Python函数1C：视图函数render方法不能向页面发送响应数据1D：视图函数接受Web请求并返回Web响应答案：A,B,D问题解析：Na101关于Redis命令，正确的是（ ）。1A：DBCOUNT 命令用于查看当前数据库的记录数1B：SELECT命令用于选择数据库1C：GET KeyName，其中KeyName为键名，返回当前参数KeyName的值，如果指定KeyName不存在，则返加nil1D：SET KeyName Value命令表示将值存储在当前数据库中答案：B,C,D问题解析：Na判断题每小题2分，共20 分11通过CMD命令行启动MySQL服务的命令为start xxx, xxx表示安装过程中配置的MySQL服务名。（ ）正确错误答案：错误问题解析：Na21由于Redis是内存数据库，因此它不支持数据的持久化存储。（ ）正确错误答案：错误问题解析：Na31Scrapy项目中运行爬虫的命令为scrapy start 爬虫名字。（ ）正确错误答案：错误问题解析：Na41Selenium是一个爬虫框架，让浏览器自动加载页面，获取需要的数据，甚至页面提交，后来又应用于Web自动化测试。（ ）正确错误答案：错误问题解析：Na51mongo.exe是MongoDB的客户端程序，在命令行执行mongo命令即可启动MongoDB客户端。（ ）正确错误答案：正确问题解析：Na61在使用多线程的过程中，为了解决全局变量冲突问题，可以通过Lock加锁。（ ）正确错误答案：正确问题解析：Na71Django中path()函数可以用于正则路径。（ ）正确错误答案：错误问题解析：Na81HttpRequest 对象的path属性可以获取URL中的路径部分，数据类型是字符串。（ ）正确错误答案：正确问题解析：Na91MySQL数据库在集群技术、高可用性、安全性、系统管理等方面都有较好性能表现，支持跨平台运行，是目前大型高性能商务数据库的首选。（ ）正确错误答案：错误问题解析：Na101Django中的模板变量名必须有字母、数字、下划线和点组成，可以以字母、数字和下划线开头。（ ）正确错误答案：错误问题解析：Na 中慧1+X证书题库http://blog.imc.re/archives/260d.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"Kafka安装文档","path":"/RSSBOX/rss/6b180cba.html","content":"关闭防火墙1systemctl stop firewalld需安装Zookeeper组件具体要求同Zookeeper任务要求，并与Kafka环境适配，启动Zookeeper并截图保存结果：在master，slave1，slave2分别启动zookeeper1zkServer.sh start查询3台机器的zookeeper启动状态1zkServer.sh statuszookeeper 配置文件 /opt/software/zookeeper/conf/zoo.cfg ，数据文件夹设置为 /opt/software/zookeeper/zkdata，zoo.cfg 需要和 kafka 的 zookeeper.properties 适配1vi /opt/module/zookeeper/conf/zoo.cfg需要查看的配置内容:12345dataDir=/opt/module/zookeeper/zkdataclientPort=2181server.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888解压kafka安装包到 /opt/module 路径,并修改解压后文件夹名为 kafka ,截图并保存结果:12tar -xvzf /opt/software/kafka1.0.0.tar.gz -C /opt/modulemv /opt/module/kafka_2.11-1.0.0 /opt/module/kafka设置kafka环境变量, 并使环境变量只对当前root用户生效，截图并保存结果:1vi /root/.bash_profile添加以下内容:12export KAFKA_HOME=/opt/module/kafkaexport PATH=$PATH:$KAFKA_HOME/bin刷新生效:1source /root/.bash_profile修改kafka相应文件, 截图并保存结果:在 master 上修改文件zookeeper.properties1vi /opt/module/kafka/config/zookeeper.properties配置文件内容:12345clientPort=2181dataDir=/opt/module/kafka/dataserver.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888在kafka安装文件夹中建立 data 文件夹,与 dataDir=/opt/module/kafka/data 对应12mkdir /opt/module/kafka/datavi /opt/module/kafka/data/myid在 master 上修改 server.properties1vi /opt/module/kafka/config/server.properties修改以下内容:12345broker.id=0listeners=PLAINTEXT://192.168.152.240:9092advertised.listeners=PLAINTEXT://192.168.152.240:9092log.dirs=/usr/local/src/kafka/datazookeeper.connect=192.168.152.240:2181,192.168.152.241:2181,192.168.152.242:2181文件分发: 1234scp -r /opt/module/kafka slave1:/opt/modulescp -r /opt/module/kafka slave2:/opt/modulescp /root/.bash_profile slave1:/rootscp /root/.bash_profile slave2:/root在其他节点刷新变量文件生效:1source /root/.bash_profile在 slave1 上修改 myid 文件为 2123cat&gt;/opt/module/kafka/data/myid&lt;&lt;EOF2EOF修改 server.properties 文件为以下内容:1234broker.id=1listeners=PLAINTEXT://192.168.152.241:9092advertised.listeners=PLAINTEXT://192.168.152.241:9092log.dirs=/usr/local/src/kafka/data zookeeper.connect=192.168.152.240:2181,192.168.152.241:2181,192.168.152.242:2181在 slave2 上修改 myid 文件为 3123cat&gt;/opt/module/kafka/data/myid&lt;&lt;EOF3EOF修改 server.properties 文件为以下内容:1234broker.id=1listeners=PLAINTEXT://192.168.152.242:9092advertised.listeners=PLAINTEXT://192.168.152.242:9092log.dirs=/usr/local/src/kafka/data zookeeper.connect=192.168.152.240:2181,192.168.152.241:2181,192.168.152.242:2181启动 kafka 并保存命令输出结果, 截图并保存结果:分别在master, slave1, slave2 上执行123kafka-server-start.sh -daemon /opt/module/kafka/config/server.propertiesjps输出以下内容为正常启动:1232198 QuorumPeerMain6732 Jps3758 Kafka创建指定topic, 截图并保存结果:1kafka-topics.sh --zookeeper master01:2181,slave01:2181,slave02:2181 --create --partitions 3 --replication-factor 3 --topic test输出以下内容为正常启动:1Created topic \"test-topic\".查看所有topic信息, 并截图保存结果:1kafka-topics.sh --list --zookeeper master01:2181输出:1test1kafka-topics.sh -zookeeper master01:2181,slave01:2181,slave02:2181 --describe --topic test输出:1234Topic:test-topic PartitionCount:3 ReplicationFactor:3 Configs: Topic: test-topic Partition: 0 Leader: 1 Replicas: 1,2,0 Isr: 1,0 Topic: test-topic Partition: 1 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1 Topic: test-topic Partition: 2 Leader: 0 Replicas: 0,1,2 Isr: 0,1启动指定生产者(producer), 并截图保存结果:注意: 控制台发送者的broker-list的ip需要与server.properties中listeners处的一致, 控制台消费者就能接收到消息1kafka-console-producer.sh --broker-list 192.168.152.240:9092 --topic test启动消费者(sonsumer), 并截图保存结果: 再开启一个master连接, 输入:1kafka-console-consumer.sh --bootstrap-server 192.168.152.240:9092 --topic test注: kafka旧版本连接服务器的参数为 --zookeeper , 新版为 --bootstrap-server输出:1Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]测试生产者(producer), 并截图保存结果:1kafka-console-producer.sh --broker-list 192.168.152.240:9092 --topic test123456789[root@master01 ~]# kafka-console-producer.sh --broker-list 192.168.152.240:9092 --topic test&gt;111&gt;222&gt;333&gt;444&gt;555&gt;666&gt;777&gt;888测试消费者(consumer), 并截图保存结果消费者上显示: 123456789Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].111222333444555666777888基于命令行方式使用kafka三台机器上启动zookeeperzkServer.sh start 三台机器上启动kafka服务1kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties 查看进程1jps创建主题12345kafka-topics.sh --create \\ --topic itcasttopic \\ --partitions 3 \\ --replication-factor 2 \\ --zookeeper master01:2181,slave01:2181,slave02:2181 创建生产者，来生产消息。在master01上输入123kafka-console-producer.sh \\ --broker-list master01:9092,slave01:9092,slave02:9092 \\ --topic itcasttopic 创建消费者，来消费消息。在slave01上输入 123kafka-console-consumer.sh \\ --from-beginning \\ --topic itcasttopic 测试消费者（consumer）（kafka2.11-2.0.0命令有所不同）1234kafka-console-consumer.sh \\ --from-beginning \\ --topic test \\ --bootstrap-server 192.168.152.245:9092 kafka安装文档http://blog.imc.re/archives/kafka.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"Zookeeper安装文档","path":"/RSSBOX/rss/e32c0aa8.html","content":"关闭防火墙12345678# 禁用防火墙systemctl disable firewalld# 关闭防火墙systemctl stop firewalld# 再次查看防火墙状态systemctl status firewalld查看主机名称1hostname解压zookeeper123tar -xvzf /opt/software/zookeeper-3.4.5.tar.gz -C /opt/modulemv /opt/module/zookeeper-3.4.5 /opt/module/zookeeper修改zookeeper配置文件123cp /opt/module/zookeeper/conf/zoo_simple.cfg /opt/module/zookeeper/conf/zoo.cfgvi /opt/module/zookeeper/conf/zoo.cfg修改以下内容:1234567# 修改 dataDir 路径dataDir=/opt/module/zookeeper/zkdata# 文件末尾添加服务器server.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888创建 dataDir 设置的目录和 myid 文件12345mkdir /opt/module/zookeeper/zkdatacat &gt; /opt/module/zookeeper/zkdata/myid &lt;&lt; EOF1EOF配置环境变量1vi /root/.bash_profile文件结尾插入以下内容:12export ZK_HOME=/opt/module/zookeeperexport PATH=$PATH:$ZK_HOME/bin刷新文件生效:1source /root/.bash_profile分发 zookeeper 相关文件和环境变量到slave1 slave21234scp -r /opt/module/zookeeper slave1:/opt/modulescp -r /opt/module/zookeeper slave2:/opt/modulescp -r /root/.bash_profile slave1:/rootscp -r /root/.bash_profile slave2:/root在其他节点shell终端上刷新环境变量以生效1source /.bash_profile更改 slave1 和 slave2 的 myid 文件内容为 2 和 3123cat &gt; /opt/module/zookeeper/zkdata/myid &lt;&lt; EOF2EOF123cat &gt; /opt/module/zookeeper/zkdata/myid &lt;&lt; EOF3EOF分别在三台机子上启动 zookeeper1zkServer.sh start查看启动状态1zkServer.sh status输出类似下方内容即为启动正常123JMX enabled by defaultUsing config: /usr/local/src/zookeeper/bin/../conf/zoo.cfgMode: follower进入 zkshell1zkCli.sh -server localhost:2181退出1quit停止 zk 服务1zkServer.sh stop zookeeper安装文档http://blog.imc.re/archives/zookeeper.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"Hadoop安装竞赛文档","path":"/RSSBOX/rss/1cfc267e.html","content":"Hadoop安装教程仅供厦门南洋学院的同学参考使用，基于厦门南洋学院实训楼413的环境进行配置，无法保证能够应用到其他场景。编纂时间：2023.10.11编纂作者：夏五郎Linux入门教程在linux下安装hadoop必须要会的几个基本操作：cd 目录 # 切换目录的命令使用 cd 命令可以切换目录，比如 cd $MP 就是打开$MP 目录。其中在目录前添加 ./module或不加任何标记 module 表示当前目录，添加 ../module 表示上级目录 ，/module 表示从根目录 / 开始的绝对路径目录。vi 文件目录 # 在命令行中使用编辑器的命令使用 vi 命令可以在shell中编辑文件，比如 vi /etc/hosts 就是编辑/etc/hosts文件。文件目录的标注方法和cd同样，不存在的文件会直接新建，请注意！使用方向键移动光标，点击 i 键，最下方显示 INSERT 后进入编辑模式点击 Esc 键，然后输入 :wq可以保存文件并退出，其中q表示退出，w表示保存tar # 解压文件的命令使用tar命令可以压缩/解压文件，hadoop配置中只用到了解压文件，这里给出解压文件的示例命令，其他参数请自行tar --help 或者bing查看。123tar -xvzf 压缩目录文件 -C 解压到的目录# 例子：tar -xvzf /opt/jdk-8u162-linux-x64.tar.gz -C $MP其中 /opt/jdk-8u162-linux-x64.tar.gz 是压缩文件所在文件路径，$MP 是要解压到的文件目录ip #命令使用 ip a 命令可以查看服务器的IP地址，输出一般为这样：12345678910111213 [root@master ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:56:e9:74 brd ff:ff:ff:ff:ff:ff inet 192.168.152.81/24 brd 192.168.152.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::6a9a:2e8:734b:def2/64 scope link noprefixroute valid_lft forever preferred_lft forever 其中 192.168.152.81就是该服务器的ip地址，要学会去看其他节点服务器的ip地址，以便连接操作。ssh #连接服务器命令使用 ssh 命令可以连接其他节点服务器，连接命令为：123ssh root@192.168.152.81 #root是连接用户名，192.168.152.81是要连接的服务器ip地址。root@192.168.152.81's password: #显示这个后输入密码[root@master ~]# 显示这个代表成功连接小贴士在敲命令的过程中可以使用 TAB 键补全命令哦~可以使用方向键的 ↑ ↓ 键切换到上次执行的命令使用 ssh root@192.168.152.81 \"command\" 指令即可远程执行 command 指令，前提是配置了密钥连接哦。Hadoop安装流程一，配置服务器基本环境需要在本板块将服务器所需模块以及其他配置文件弄好，以便后续的配置在ubuntu中连接hadoop01，将hadoop01作为master节点进行配置：1234ssh root@192.168.152.81 #打开终端，输入上述命令进行连接，root是连接用户名，192.168.152.81是要连接的服务器ip地址。root@192.168.152.81's password: #显示这个后输入密码[root@server-1 ~]# 显示这个代表成功连接如果无法连接说明IP地址更改了，请自行查看！设置master主机名称和host文件，便于连接123456789# 设置主机名hostnamectl set-hostname master# 在/etc/hosts文件里插入master和其他节点的ip和域名idcat&gt;&gt;\"/etc/hosts\"&lt;&lt;EOF192.168.152.81 master192.168.152.101 slave1192.168.152.121 slave2EOF或者使用 vi /etc/hosts 命令在文件末尾插入如下代码123192.168.152.81 master192.168.152.101 slave1192.168.152.121 slave2生成密钥配置一键免密登录1234567# 生成密钥，回车三次即可ssh-keygen -t rsa# 将密钥复制给其他的节点便于一键连接，先输入 yes ，再输入节点密码 123456ssh-copy-id masterssh-copy-id slave1ssh-copy-id slave2解压配置jdk以及hadoop1234567891011121314# 新建目录 /opt/module 用于放置所有需要的模组文件mkdir /opt/module# 解压jdk文件，文件路径为/opt/jdk-8u162-linux-x64.tar.gz，如有变更自行更改tar -xvzf /opt/jdk-8u162-linux-x64.tar.gz -C /opt/module# 将jdk目录重命名，方便后续配置mv /opt/module/jdk1.8.0_162 /opt/module/jdk# 解压hadoop文件，文件路径为tar -xvzf /opt/hadoop-3.3.0.tar.gz，学校自带镜像没有该版本，该版本为2023参赛所用版本，需要手动上传。tar -xvzf /opt/hadoop-3.3.0.tar.gz -C /opt/module# 将hadoop目录重命名，方便后续配置mv /opt/module/hadoop-3.3.0 /opt/module/hadoop设置jdk以及hadoop的环境变量使用 vi /root/.bash_profile 在文件末尾添加如下内容：1234export JAVA_HOME=/opt/module/jdkexport PATH=$JAVA_HOME/bin:$PATHexport HADOOP_HOME=/opt/module/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin配置其他文件节点，执行以下命令：12345# 传输hosts以及.root_profile 配置文件到其他的节点上scp -r /etc/hosts slave1:/etcscp -r /etc/hosts slave2:/etcscp -r /root/.root_profile slave1:/etcscp -r /root/.bash_profile slave2:/root打开新的终端页面执行以下指令，配置slave1。12345678910111213141516# 连接服务器sshssh root@slave1# 设置主机名hostnamectl set-hostname slave1# 创建文件夹mkdir /opt/module# 生成密钥，回车三次即可ssh-keygen -t rsa# 将密钥复制给其他的节点便于一键连接，先输入 yes ，再输入节点密码 123456ssh-copy-id masterssh-copy-id slave1ssh-copy-id slave2配置slave2，打开新的终端页面执行以下指令。12345678910111213141516# 连接服务器sshssh root@slave2# 设置主机名hostnamectl set-hostname slave2# 创建文件夹mkdir /opt/module# 生成密钥，回车三次即可ssh-keygen -t rsa# 将密钥复制给其他的节点便于一键连接，先输入 yes ，再输入节点密码 123456ssh-copy-id masterssh-copy-id slave1ssh-copy-id slave2查看安装情况在12345678# 刷新文件使变量生效source /root/.bash_profile# 查看Java安装情况和版本java -version# 查看hadoop安装情况和版本hadoop version如果都正常输出证明安装成功，如果提示未找到命令则需要排查问题。二，更改hadoop配置文件一共有6个配置文件需要更改，hadoop-3.3.0版本需要在4个启动/关闭脚本中添加环境参数，更改文件命令为 vi ./文件名更改 hadoop-env.sh文件切换目录到hadoop配置文件目录下1cd /opt/module/hadoop/etc/hadoop使用如下命令更改：1vi ./hadoop-env.sh查找文件中 JAVA_HOME 关键词并替换为下方路径提示：可以使用 ?JAVA_HOME 搜索关键词， N 键是查找下一个，在文件内标注export JAVA_HOME= 的地方进行修改，如果有注释需要删除注释进行修改，修改成下方内容1export JAVA_HOME=/opt/module/jdk更改 core-site.xml 文件配置文件如下：12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;更改 hdfs-site.xml 文件配置文件如下：12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;slave1:50090&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;更改 mapred-site.xml 文件配置文件如下：123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;注：hadoop-2.7.7中需要先复制配置文件模板再进行配置1cp ./mapred-site.xml.template ./mapred-site.xml更改 yarn-site.xml 文件配置文件如下：12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;更改 workers 文件在hadoop-2.7.7中该文件名称为 slaves ，hadoop-3.3.0中该文件为workers 记得根据实际情况更改文件名！配置文件如下：123masterslave1slave2更改 start-dfs.sh 以及 stop-dfs.sh 中的启动变量使用 vi /opt/module/hadoop/sbin/start-dfs.sh 以及 vi /opt/module/hadoop/sbin/stop-dfs.sh 指令分别更改，在 文件开头第二行插入如下代码！（#!/usr/bin/env的下面）1234HDFS_DATANODE_USER=rootHADOOP_SECURE_DN_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root更改 start-yarn.sh 以及 stop-yarn.sh 中的启动变量使用 vi /opt/module/hadoop/sbin/start-yarn.sh 以及 vi /opt/module/hadoop/sbin/stop-yarn.sh 指令分别更改，在 文件开头第二行插入如下代码！（#!/usr/bin/env的下面）123YARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=root将master节点下的模组文件传输给其他的节点12scp -r /opt/module slave1:/optscp -r /opt/module slave2:/opt三，启动hadoop12345678910111213141516# hdfs初始化hdfs namenode -format# 启动dfsstart-dfs.sh# 启动yarnstart-yarn.sh# 检查启动情况jps# 检查其他节点启动情况ssh root@slave1 \"source /root/.bash_profile &amp;&amp; hostnamectl &amp;&amp; jps\"ssh root@slave2 \"source /root/.bash_profile &amp;&amp; hostnamectl &amp;&amp; jps\"如果启动失败记得先关闭yarn以及dfs，然后再排查问题12stop-dfs.shstop-yarn.sh问题更改之后记得使用下方命令同步文件！12scp -r /opt/module slave1:/optscp -r /opt/module slave2:/opt四，配置文件记忆注意：比赛时所有的配置文件都需要背过！！！ 为了方便背诵，我将所有需要背的配置文件内容拆解开来，助于理解记忆。理解记忆：xml文件中配置框架英文含义：configuartion：配置property：属性name：名称value：值hadoop-env.sh 文件中需要重点记忆的内容：export JAVA_HOME Java配置路径core-site.xml 文件中需要重点记忆的内容：property hadoop默认主节点的名称name fs.defaultFSvalue hdfs://master:9000property hadoop缓存目录路径值name hadoop.tmp.dirvalue /opt/module/hadoop/tmphdfs-site.xml 文件中需要重点记忆的内容：property hadoop分布式节点数量名称name dfs.replicationvalue 3property hadoop namenode第二节点http地址name dfs.namenode.secondary.http-addressvalue slave1:50090mapred-site.xml 文件中需要重点记忆的内容：property mapreduce framework 名称name mapreduce.framework.namevalue yarnyarn-site.xml 文件中需要重点记忆的内容：property yarn资源管理主机名称name yarn.resourcemanager.hostnamevalue masterproperty yarn节点管理 aux-servicesname yarn.nodemanager.aux-servicesvalue mapreduce_shuffleworkers 文件中需要重点记忆的内容：所有节点的主机名称*-dfs.sh 文件中需要重点记忆的内容：HDFS_DATANODE_USER=root # HDFS数据节点用户=rootHADOOP_SECURE_DN_USER=hdfs # HADOOP安全_DN用户=hdfsHDFS_NAMENODE_USER=root # HDFS名称节点用户=rootHDFS_SECONDARYNAMENODE_USER=root # HDFS第二名称节点用户=root*-yarn.sh 文件中需要重点记忆的内容：YARN_RESOURCEMANAGER_USER=root # YARN资源管理用户=rootHADOOP_SECURE_DN_USER=yarn # HADOOP安全_DN用户=yarnYARN_NODEMANAGER_USER=root # YARN节点管理用户=root一键安装脚本针对南洋学院内网2022参赛环境编写的一键安装脚本，在master节点执行本脚本可以一键安装hadoop。仅供参考请不要在学习hadoop搭建的时候使用脚本偷懒！脚本参考是为了辅助理解计算机配置hadoop的流程！学习hadoop搭建的过程无需查看，或仅供参考。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194#! /bin/bash### FIRST 第一板块# echo \"set PATH\" 设置脚本执行环境变量，文件位置，模组路径等JF=/opt/jdk-8u162-linux-x64.tar.gzHF=/opt/hadoop-3.3.0.tar.gzMP=/opt/module# echo \"set host 设置主机名以及配置host文件\" hostnamectl set-hostname mastercat&gt;&gt;\"/etc/hosts\"&lt;&lt;EOF192.168.152.81 master192.168.152.101 slave1192.168.152.121 slave2EOF# echo \"set ssh key 配置ssh key进行自动连接\" ssh-keygen -t rsa # -n '' -f ~/.ssh/id_rsassh-copy-id masterssh-copy-id slave1ssh-copy-id slave2# echo \"set java &amp;&amp; hadoop\" 解压配置jdk以及hadoopmkdir $MPmkdir $MP/jdktar -xvzf /opt/jdk-8u162-linux-x64.tar.gz -C $MP/jdk --strip-components 1mkdir $MP/hadooptar -xvzf /opt/hadoop-3.3.0.tar.gz -C $MP/hadoop --strip-components 1# echo \"set PATH 设置jdk以及hadoop的环境变量\" cat&gt;&gt;\"/root/.bash_profile\"&lt;&lt;EOFexport JAVA_HOME=$MP/jdkexport PATH=\\$JAVA_HOME/bin:\\$PATHexport HADOOP_HOME=$MP/hadoopexport PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbinEOF# echo \"check install 查看安装情况\"source /root/.bash_profilejava -versionhadoop version# echo \"set slave 设置其他slave节点的主机配置文件\" scp -r /root/.bash_profile slave1:/root/scp -r /root/.bash_profile slave2:/root/scp -r /etc/hosts slave1:/etc/scp -r /etc/hosts slave2:/etc/# echo \"set ssh slave ssh连接设置其他节点\" ssh root@slave1 &lt;&lt;EOFhostnamectl set-hostname slave1mkdir $MPssh-keygen -t rsa -n '' -f ~/.ssh/id_rsassh-copy-id masterssh-copy-id slave1ssh-copy-id slave2EOFssh root@slave2 &lt;&lt;EOFhostnamectl set-hostname slave2mkdir $MPssh-keygen -t rsa -n '' -f ~/.ssh/id_rsassh-copy-id masterssh-copy-id slave1ssh-copy-id slave2EOF### SECOND 第二板块# hadoop settings file 设置hadoop配置文件# 替换 hadoop-env.sh 中的JAVA_HOME PATH文件sed -i '/export JAVA_HOME=/cexport JAVA_HOME='$MP'/jdk' $MP/hadoop/etc/hadoop/hadoop-env.sh# 配置 core-site.xml 文件cat &gt; $MP/hadoop/etc/hadoop/core-site.xml &lt;&lt;EOF&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;EOF# 配置 hdfs-site.xml 文件cat &gt; $MP/hadoop/etc/hadoop/hdfs-site.xml &lt;&lt;EOF&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;slave1:50090&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;EOF# 配置 mapred-site.xml 文件cp $MP/hadoop/etc/hadoop/mapred-site.xml.template $MP/hadoop/etc/hadoop/mapred-site.xmlcat &gt; $MP/hadoop/etc/hadoop/mapred-site.xml&lt;&lt;EOF&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;EOF# 配置 yarn-site.xml 文件cat &gt; $MP/hadoop/etc/hadoop/yarn-site.xml&lt;&lt;EOF&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;EOF# 配置 slaves 文件 hadoop-2.7.7cat &gt; $MP/hadoop/etc/hadoop/slaves&lt;&lt;EOFmasterslave1slave2EOF# 配置workers wenjian hadoop-3.3.0cat &gt; $MP/hadoop/etc/hadoop/workers&lt;&lt;EOFmasterslave1slave2EOF# hadoop-3.3.0 need to add PATH for start&amp;stop shell hadoop-3.3.0需要添加PATH变量到启动/关闭脚本中sed -i \"2i HDFS_DATANODE_USER=root\" $MP/hadoop/sbin/start-dfs.shsed -i \"2i HADOOP_SECURE_DN_USER=hdfs\" $MP/hadoop/sbin/start-dfs.shsed -i \"2i HDFS_NAMENODE_USER=root\" $MP/hadoop/sbin/start-dfs.shsed -i \"2i HDFS_SECONDARYNAMENODE_USER=root\" $MP/hadoop/sbin/start-dfs.shsed -i \"2i HDFS_DATANODE_USER=root\" $MP/hadoop/sbin/stop-dfs.shsed -i \"2i HADOOP_SECURE_DN_USER=hdfs\" $MP/hadoop/sbin/stop-dfs.shsed -i \"2i HDFS_NAMENODE_USER=root\" $MP/hadoop/sbin/stop-dfs.shsed -i \"2i HDFS_SECONDARYNAMENODE_USER=root\" $MP/hadoop/sbin/stop-dfs.shsed -i \"2i YARN_RESOURCEMANAGER_USER=root\" $MP/hadoop/sbin/start-yarn.shsed -i \"2i HADOOP_SECURE_DN_USER=yarn\" $MP/hadoop/sbin/start-yarn.shsed -i \"2i YARN_NODEMANAGER_USER=root\" $MP/hadoop/sbin/start-yarn.shsed -i \"2i YARN_RESOURCEMANAGER_USER=root\" $MP/hadoop/sbin/stop-yarn.shsed -i \"2i HADOOP_SECURE_DN_USER=yarn\" $MP/hadoop/sbin/stop-yarn.shsed -i \"2i YARN_NODEMANAGER_USER=root\" $MP/hadoop/sbin/stop-yarn.sh### THIRD 第三板块# trasnfer module 将配置好的模组路径下的所有文件传输到其他节点scp -r $MP slave1:/optscp -r $MP slave2:/opt# check hadoop start 启动hadoophdfs namenode -format$MP/hadoop/sbin/stop-dfs.sh$MP/hadoop/sbin/stop-yarn.sh$MP/hadoop/sbin/start-dfs.sh$MP/hadoop/sbin/start-yarn.sh# 检查启动情况hostnamectljpsssh root@slave1 \"source /root/.bash_profile &amp;&amp; hostnamectl &amp;&amp; jps\"ssh root@slave2 \"source /root/.bash_profile &amp;&amp; hostnamectl &amp;&amp; jps\"其他操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 覆盖文件cat&gt;\"/etc/hosts\"&lt;&lt;EOF EOF# 在文件末尾添加cat&gt;&gt;\"/etc/hosts\"&lt;&lt;EOFEOF# 自动ssh配置sed -i '/StrictHostKeyChecking/cStrictHostKeyChecking no' /etc/ssh/ssh_configrm -rf ~/.ssh/&#123;known_hosts,id_rsa*&#125;# tar末尾添加参数--strip-components 1 可以去掉最外层的目录# 目录更改重命名mv $MP/hadoop-2.7.7 $MP/hadoopmv $MP/jdk1.8.0_162 $MP/jdkssh root@slave1 \"source /root/.bash_profile\"ssh root@slave2 &gt; /dev/null 2&gt;&amp;1 &lt;&lt;EOF# JAVA_HOME路径直接添加echo \"export JAVA_HOME=$MP/jdk\" &gt;&gt; $MP/hadoop/etc/hadoop/hadoop-env.sh# 传输hadoop的配置文件所在目录scp -r $MP/hadoop/etc/hadoop slave1:$MP/hadoop/etcscp -r $MP/hadoop/etc/hadoop slave2:$MP/hadoop/etc# dfs启动脚本添加参数，不能直接使用，需要添加到文件头cat &gt;&gt; $MP/hadoop/sbin/start-dfs.sh&lt;&lt;EOFHDFS_DATANODE_USER=rootHADOOP_SECURE_DN_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=rootEOFcat &gt;&gt; $MP/hadoop/sbin/stop-dfs.sh&lt;&lt;EOFHDFS_DATANODE_USER=rootHADOOP_SECURE_DN_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=rootEOFcat &gt;&gt; $MP/hadoop/sbin/start-yarn.sh&lt;&lt;EOFYARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=rootEOFcat &gt;&gt; $MP/hadoop/sbin/stop-yarn.sh&lt;&lt;EOFYARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=rootEOF# 传输hadoop启动脚本文件scp -r $MP/hadoop/sbin slave1:$MP/hadoopscp -r $MP/hadoop/sbin slave2:$MP/hadoop hadoop安装竞赛文档http://blog.imc.re/archives/1385.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"MCBE 88音平均律","path":"/RSSBOX/rss/824ee0f8.html","content":"前言今天修IMC Geyser的时候看到冲田在烷MC，随便试了试Xbox联机没想到进去了）然后冲田似乎在研究playsound的调音，我才知道原来MCBE的playsound命令还有微分音的用法（红石音乐人狂喜x）通过微分音的调教，理论上突破原版材质包里自带的音符盒音域，实现88音的完整钢琴音域就可能实现了，如果再搭配上midi文件的转化就可以实现在MC游戏里不依靠任何材质包，mod等就能实现音乐的播放！虽然已经有相关的项目了，比如 Open Note Block Studio 就已经实现了nbs音乐相关的编辑，以及midi文件的转换。不过这个工具目前只支持MCJE，并不支持MCBE，扩展音域似乎也是通过扩展音符包来实现的，相对繁琐。本文主要记录一下MCBE的playsound调音对应88键平均律的对应数值以及方法，虽然不一定在MCJE适用，但是基本方法应该是差不多的，可以作为参考留档。基本知识让我们先来复习一下初中基本物理知识——声音的基本三要素：音色：波形决定了声音的音调。由于不同对象材料的特点，声音具有不同的特性，音色本身就是抽象的东西，但波形就是把这种抽象和直观的性能。波形因音调而异，不同的音调可以通过波形来区分。音调：声音的高低由频率决定，频率越高音调越高，人耳听觉范围20—20000Hz；20Hz以下称为次声波，20000Hz以上称为超声波。响度：指人耳感觉到的声音强弱，即声音响亮的程度，根据它可以把声音排成由轻到响的序列。响度的大小主要依赖于声强和频率，振幅越大响度越大，人和声源的距离越小，响度越大。而这三要素在Minecraft的 /playsound 指令里基本算是都齐活了，声音的基本三要素和MC指令的信息对应还是需要说明一下（毕竟wiki没提）：MCBE的 /playsound 指令：1/playsound &lt;sound: string&gt; [player: target] [position: x y z] [volume: float] [pitch: float] [minimumVolume: float]示例指令：音色为 note.pling ，音调为 中央C（C4） ，播放给全局玩家。1/playsound note.pling @a ~~~ 9999 0.7简单说明：音色：对应的 &lt;sound: string&gt; ，只能调用 sounds.json 里的项目，具体的可以去Minecraft Wiki或者自行拆包查看，有预览表，也可以使用自制的材质包实现自定义音色。值得一提的是MC中所有的音效都可以使用。这里列几个常用的Note音色：note.harp 音符盒音色，算是MC红石音乐比较标志性的音色。类似音色： note.pling ，声音比较洪亮，音长相对较长。note.guitar 音符盒吉他音色，拨弦乐器声音采样比较短促，类似音色： note.banjo 班卓琴音色note.bass 音符盒贝斯音色，低音必备。类似音色： note.bassattack note.hat 音符盒打击音色，可以当鼓点使用。类似音色： note.bd note.snarenote.flute 音符盒笛声音色，使用高频播放的话可以达到延音的效果（吹奏类乐器音色的好处，采样出来的音色首尾相接基本不会有违和感，《光遇》里的乐器演奏也能用类似的方法实现延音。）类似音色： note.didgeridoo 有点低音号的感觉，但是高频播放不是很连贯。note.bit bit音乐采样音色，算是电子音乐比较经典的音色。类似音色： note.bell 八音盒感觉的音色， note.cow_bell 相对低沉的八音盒感觉的音色音调：对应的 [pitch: float] ，以数值的形式表现，默认是 1 ，在 note.pling 中对应十二平均律中的 F# 。这种对应关系也是本文的重点，大概是通过程序将音色移调的参数，通过这种方式即便不把所有音符采样塞进游戏里理论上也能实现十二平均律，以及对应的钢琴八十八键的复原。响度：响度有些特殊，决定响度的是 [player: target] [position: x y z] [volume: float] [minimumVolume: float] 这三个参数决定，依次是玩家目标，声音发生的位置，能够听到声音的范围，和补偿范围。没有直接影响响度的变量，不过可以通过这些参数间歇性的实现响度的控制，甚至环绕音的混响实现也是有可能的。详细参数说明： sound: string：basic_string指定要播放的声音。必须为在sounds.json中被定义的一个声音项目（例如，entity.pig.ambient）。一个声音项目可被多个不同的声音关联，而实际产生的声音为从中随机挑选的结果，被选中的概率由其权重决定，与正常游戏中相同。例如，声音项目entity.pig.ambient会随机播放多种猪音效之一，因为有多个声音关联于该项目。资源包可能会向sounds.json中添加额外的声音项目；此命令可以正常播放这些项目。该命令使用的声音名称不是文件名；而是严格使用sounds.json内定义的项目（项目名称可能与实际音频文件的文件名和目录相差甚远），因此资源包在添加新声音时必须为这些音频文件定义声音事件项目（但当资源包替换原有的已被定义的音频文件时，不必为其重新定义）。player: target：CommandSelector Player指定播放声音的目标。必须为玩家名、目标选择器或UUID。且目标选择器只允许玩家。position: x y z：CommandPositionFloat指定声音发出的方位。必须为三维坐标，元素为浮点数。允许波浪号与脱字号标记。volume: float：float指定声音能被听见的距离。必须至少为0.0。对小于1.0的值，声音会相对减轻，球状可闻范围会相对小。对大于1.0的值，声音不会实际上增大，但其可闻范围（1.0时半径为16米）会与音量相乘。声音总会基于与球体中心的距离逐渐衰减至无声。默认为1.0。必须为单精度浮点数。在Java版中，必须大于等于0.0。pitch: float：float指定声音的音调。若未指定，默认为1.0。在Java版中，该数值必须在0.0至2.0间（含），而小于0.5的值与0.5等价。小于1.0的值降低音调而提升持续时间；大于1.0的值提升音调而降低持续时间。音调值是乘在声音频率上的一个倍率，因此若将0.5-1.0（含）区间内的音调值乘以2，所得的新音调便会高一个八度。在基岩版中，该数字没有特别限制，但是必须要在0.0至256.0之间才有对应效果。高于256.0的值与默认值的效果相同。小于等于0.0的值会导致听不到该声音。对于音符盒的声音，如果你希望将其他音符转换为音调数值，请参阅相关内容。但请注意1.0的音调值对于每种声音并不一定都是F♯。minimumVolume: float：float指定在声音可闻范围外的目标能听到的音量。若目标在可闻范围外，作为补偿，声源会被放置在距离目标较近的位置（距离小于4格），而_最小音量_会决定补偿声源的音量。如果此数值等于0，则正常可闻范围外的目标听不到声音。如果未指定，则默认为0.0。必须为单精度浮点数。在Java版中，必须在0.0和1.0（含）之间。——《命令/playsound - Minecraft Wiki》 有这三要素基本就满足了声音构成的条件，但是把范围缩小到 音乐 当中，光有这基本三要素是不够的，关于 音调 的使用范围至关重要，而在乐理中基本的音调构成范围就不得不提到 十二平均律 了：十二平均律（12-equal temperament）又称十二等程律，是世界上通用的一组音（八度）分成十二个半音音程的律制，各相邻两律之间的波长之比完全相等。 十二平均律是由中国明朝皇族世子朱载堉发现。十二平均律是指八度的音程按波长比例平均分成十二等份，每一等份称为一个半音（小二度）。一个大二度则是两等份，称为全音。将一个八度分成12等份有着惊人的一些巧合。这是因为它的纯五度音程的两个音的波长比（即1/2的7/12次方）约为0.6674，与2/3，约为0.6667，非常接近。十二平均律在交响乐队和键盘乐器中得到广泛使用，钢琴即是根据十二平均律来定音的。——十二平均律_百度百科 (baidu.com)可能有人会问为什么要拘泥于十二平均律？使用微分音创作音乐不是也很新吗？其实这么想也不是不可以，不过截至目前，人类的基本音乐创作理论体系 （俗称乐理） 还“局限”于基于十二平均律中，因此，想要作出“给人听”的音乐十二平均律的存在是不可或缺的。当然科技发展至今，使用微分音创作的电子音乐也不算少数，这个就因人而异咯xd 借用《海上钢琴师》里我印象深刻的经典台词来说：Take a piano. The keys begin, the keys end. You know there are eightyeight of them, nobody can tell you any different. They are not infinite. You are infinite. And on these keys the music that you can make is infinite. I like that. That I can live by.译：拿一架钢琴来说，从琴键开始，又结束。你知道钢琴只有88个键，随便什么琴都没差。它们不是无限的。你才是无限的，在琴键上制作出的音乐是无限的。我喜欢这样，我活的惯。MC中的平均律理论成立，实践开始！Minecraft原版中的音符盒的音域只有F#3到F#5的25个键，想要还原钢琴八十八键的音域一般来说只有扩展材质包，mod之类的办法。但是通过 /playsound 指令中的 [pitch: float] 调音参数就能实现原版MC不使用任何外部资源加持的前提下实现88键钢琴音域的还原！ （甚至超出88键都有可能xd）那么在这其中需要实现各个键位的音调还原，Minecraft中 pitch参数和十二平均律的对应关系是必不可少的。本来以为是很简单的正比线性对应关系，但是试了几个数之后感觉有点不对。。比如 p=1 的时候音色 note.pling 对应的是 F#4， p=2 的时候对应的是 F#5，但是到 F#3的时候对应的 p=0.5 ，并不是一般的一次函数线性关系。通过冲田分享的文章 【我的世界】playsound钢琴全键音调值 - 哔哩哔哩 (bilibili.com) 知道了基本的对应关系式（不知道是怎么来的，不过随机抽样了几个数试了试大概是对的，就按这个公式来看吧x）：p=2^&#123;\\frac&#123;\\left(n-12\\right)&#125;&#123;12&#125;&#125;其中 $p$ 代表的是 [pitch: float] 中的数值，而 $n$ 则是十二平均律中的数值关系， $n$ 的整数之间都是半音关系，比如在音色 note.pling 中， $n=0$ 的时候对应 $p=0.5$ ，对应的 F#3 , $n=12$ 的时候对应 $p=1$ 对应的 F#4，$n=24$ 的时候对应 $p=2$ …… 以此类推，便能得到钢琴八十八键对应的音符音调了。值得一提的是，MC中所有可以使用的音效对应的基准因，也就是 p=1 对应的音，并不一定都是 F# ，但是就目前来看，所有 Note 类型的音色对应的基准音都是 F#，所以以 F# 为基准音的基础上去测试以及记录对应的88音平均律的基本关系，如果有基准因不是 F# 的，也可以通过参考这个基准音对应关系来进行平移得到相对应的平均律谱表。88音平均律谱表以 note.pling 音色为基准，通过 p=1 F#3 的基本音关系延伸，根据 p=2^((n-12)/12) 的关系式，测试计算出来的平均律谱表如下（p取值为小数点后四位数）：p=2^&#123;\\frac&#123;\\left(n-12\\right)&#125;&#123;12&#125;&#125;测试指令：1/playsound note.harp @a ~~~ 1 pnp音符音域-330.0743A0-320.0787A#-310.0834B-300.0884C1-290.0936C#-280.0992D-270.1051D#-260.1114E-250.1180F-240.1250F#-230.1324G-220.1403G#-210.1487A-200.1575A#-190.1669B-180.1768C2-170.1873C#-160.1984D-150.2102D#-140.2227E-130.2360F-120.2500F#-110.2649G-100.2806G#-90.2973A-80.3150A#-70.3337B-60.3536C3-50.3746C#-40.3969D-30.4204D#-20.4454E-10.4719F00.5000F#10.5297G20.5612G#30.5946A40.6300A#50.6674B60.7071C470.7492C#80.7937D90.8409D#100.8909E110.9439F121.0000F#131.0595G141.1225G#151.1892A161.2599A#171.3348B181.4142C5191.4983C#201.5874D211.6818D#221.7818E231.8877F242.0000F#252.1189G262.2449G#272.3784A282.5198A#292.6697B302.8284C6312.9966C#323.1748D333.3636D#343.5636E353.7755F364.0000F#374.2379G384.4898G#394.7568A405.0397A#415.3394B425.6569C7435.9932C#446.3496D456.7272D#467.1272E477.5510F488.0000F#498.4757G508.9797G#519.5137A5210.0794A#5310.6787B5411.3137C8以上表格使用Excel计算 原文件 （懒人一个xd） ：【腾讯文档】MC音律表结语从未设想过的道路）如果能加上midi转换的话，那么直接在MC里制作音乐也是完全有可能的了（确信。虽然有了nbs音乐，但是nbs本身并不支持在原版MC中直接播放，虽然可以用NBS Studio进行转换，但是目前NBS Studio也不支持MC基岩版，以及原版适配中目前似乎并没有支持到用命令扩展音域的地步（不过高版本mcfunction似乎可行。？）能在原生的MC里实现88音律表，也方便以后创作红石音乐的人们。简单的一次尝试，也学到了不少东西，xd MCBE 88音平均律http://blog.imc.re/archives/7d13.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"Nostr浅尝记录","path":"/RSSBOX/rss/ab2923a2.html","content":"前言Nostr是什么？一句话总结nostr：基本成型的去中心化的社交网络。 其实相关的概念已经见到过不少了，“去中心化”也是最近网上的热点 (嘛，多半都是被比特币炒上去的热度就是了，实际上关注技术和应用本身的人并不算很多x) 而Nostr就是“去中心化社交网络”的具体体现之一（似乎有别的，不过我没接触过x）我本身对“去中心化”其实也有一定的兴趣和倾向，不过个人技术力属实有限，能做到的事情太少，充其量也就是看看别人做的现成品x。不过经过大半天的琢磨体验下来，稍稍总结了一些心得。尽管在自己的nostr账户上也发布了，不过由于无法修改/删除，故不公开，只写在博客上作为记录（毕竟博客的数据是自己的x）nostr相关链接为了研究nostr专门把edge开了个分类，感觉浏览器快被我用成图书馆了xNostr Client项目：vitorpamplona/amethyst: Nostr client for Android (github.com) Android开源客户端，功能还是比较齐全的，自动翻译，画廊功能算是特色吧，对于node节点的检测也比较人性化，以及具有群组聊天功能，如果有个国内能直连的node节点的话体验还是很不错的。damus-io/damus: iOS nostr client (github.com) IOS开源客户端，因为没有苹果设备所以没体验过，不过挺出名的倒是xiris 网页客户端，同时具有iOS/Android的客户端。基本操作还是比较友好的，也有多语言实现，就是一开始从哪看其他信息有点摸不着头脑x，另外功能比较简单，只实现了基本的功能，没太多特色亮点，值得一提的是有类似Twitter那样的自定义ID的功能，比如 https://iris.to/smgoro 这样的。似乎还会注册NIP-05 Address，不过这是什么我不太清楚，有待研究。irislib/iris-messenger: Decentralized messenger (github.com) 开源客户端仓库。lovvtide/satellite-web: Satellite nostr client (github.com) 在线体验网址 https://satellite.earth/ 一个非常风格化的网页客户端，似乎有不少奇奇怪怪的功能（待开发），没有i18n支持，也没有中文，不过还是值得一看的。digi-monkey/flycat-web：马上在Nostr上写博客 (github.com) 国人开发的网页客户端，理念是在Nostr上写博客，总的来说还是挺不错的，不过目前来说写博客还是自己的博客更方便些x，而且美中不足的就是UI方面还差了点火候）要是能完善一下就更好了。在线体验网址： flycat.club styppo/hamstr: A twitter-style Nostr web client (github.com) 模仿Twitter风格的网页客户端，UI很好看（不如说大多数Nostr客户端UI都是模仿Twitter做的就是了），但是功能上似乎有些缺陷，我用他的在线体验网址 hamstr.to 试了试，登录账号之后刷新就什么都做不了了，很怪。如果能和flycat结合一下那大概是蕨杀。Nostr node项目：参考 自建Nostr中继器 - Nostr协议 (nostrtips.com) 这里留档几个自己比较感兴趣的项目。（留坑为后续自建node做准备x）hoytech/strfry: a nostr relay (github.com) 基于C++实现的无数据库中继器nostr-rs-relay: Nostr relay in rust (sr.ht) 基于Rust使用SQLite存储数据的中继器nostr relay: nostr relay (pobblelabs.org) 基于Python使用SQLite存储的中继器其他Nostr相关网址：nostr-protocol (github.com)：应该算是“nostr官方”吧，理论提出的文章记录仓库，以及nostr相关的基本介绍，实现思路应该都有，不过基本只有md文件，过于简陋，只能算是理论，不是什么实际项目，有兴趣可以看看）nostr.build media uploader 为Nostr网络提供图像存储上传的网站，可以使用直链查看，不过可惜国内屏蔽了，不然可以当图床用x 不过关键的数据存储是否使用Nostr网络这点打个问号，目前来看大概率不是，还是自托管的，毕竟用的php实现的，还能查看具体的存储用量）估计和一般的图床差不多，只是挂了个nostr的名号。nostr.watch 查看Nostr节点的网站，目前来看已经有500多个节点了，不过中国节点屈指可数）这么来看的话Nostr网络集群也算是发展起来了，虽然不如区块链那般壮大。Repository search results · GitHub Github上nostr相关的仓库，目前来看数量也有2.6k了，也算是比较成熟的项目了吧。aljazceru/awesome-nostr: A curated list of nostr projects and resources (github.com) Nostr相关的项目大集合，有不少，有时间可以看看。去中心化的“web3”概念提出到现在，Nostr是第一个我看到，并且亲自尝试的实际项目。 概念的变革都是由理想家提出，由工程师落实，最后让用户进行巩固充盈，这三者缺一不可，但资本并不是绝对必须的存在。 对于Nostr网络的未来，我还是很期待的。使用体验&amp;区别一般的使用体验基本和Twitter差不多，发送文章，评论，点赞什么的（尽管不能撤销x）主要说下技术上的区别带来的直观体验吧。1. 账户体验：应该是最直观的体验差异，账户不像中心化管理的社交网络那样使用常规的邮箱/手机号注册，密码验证，而是使用公钥私钥进行创建账户以及认证信息。这样做和中心化管理的最大差异就是你可以使用公钥看到对方的信息，包括发的文章，关住，粉丝，甚至私信对象（大部分nostr客户端的设计把公钥和私钥都设置成可以登录的认证方式了） 但是并不是所有信息都能看到，毕竟那样就没有设置账户的意义了）私信内容会被加密，只有有私钥的人才能看到具体内容，公钥看就是一团乱码。此外没有私钥的话也不能进行信息交互，发文章点赞关注之类的。大体来说其实和传统的账户使用无异。但是公钥和私钥都是不能更改的，所以说一旦泄露这个账户基本就相当于废了，正儿八经用的话可以使用密码管理器一类的东西来保存。 2. 客户端体验：客户端体验算是一个比较直观的体验差距，和传统中心化管理的社交网络不同，没有“官方客户端”这一说，所有客户端都是第三方的。这样做有利有弊吧，相比起传统的中心化社交网络的第三方客户端，去中心化的社交网络的客户端开发显然地位要比前者高，可操性也更强，事实上半天的琢磨已经发现了不少开源的客户端，软件网页应有尽有，各有各的好处和优势，不过展开讲就长了，留着挖个坑，回头再说x。 基本内容都是一样的，可以看到别人的文章，基本的信息，发布自己的文章等等。要说有什么不同的话，那就是相比起中心化的社交网络多了个“node节点选择”的功能，不同的节点获取到的信息可能是不一样的，你发送的信息实际上也是悲存储在了node节点中。不过倒是不用太担心数据安全的问题（大概 该加密的数据就算存储在别人的node上也会以加密的形式存储，解密需要用到私钥，这个是客户端的活。 3. 服务器体验：至少直到目前为止，去中心化的优势并没有在中国有所体现，因为基本上绝大多数的节点都被gfw噶掉了）而且去中心化的社交网络不利于监管，对于中国这种林子大了什么鸟都有的国家还是不太适用）不过可以自建node节点，这点倒是和传统的中心化社交网络差异性比较大。目前为止从 nostr.watch 的网站来看，已经有了500多个节点，不过大半集中在欧美地区，亚洲地区的node节点还比较少，目前来看日本那边可能用的人多一些（大概是twitter的难民）国内节点目前来看的话，台湾有一个节点，大陆有三四个，其中一半不是失效就是乱提交的，实质上能用的似乎只有一个（开node的大哥哥加油啊x），看别的人说国内节点迟早也会被制裁，这点我也比较同意，基本暂时不对国内节点保留希望了）说实话，目前为止的nostr网络虽然说实现了去中心化管理，但是和我想象中的“完全去中心化社交网络”，或者说是“web3”还是有些差距，理想状态下我是希望能做到“用户及节点”的效果，这样用户体量越大社交网络也就越稳定，但是现阶段是“节点越多网络越稳定”，节点数量和用户数量不一定是正相关的，所以有崩溃的风险。另外虽然说从技术角度来看和区块链关系不大，但是目前主要受众群体其实还是吃到区块链红利的那波人，有没有可能下沉到一般用户群体（特别是在中国）也有待商榷。 另外有机会的话我也想试试自建node节点，不过需要考虑的事情还是有很多，比如选择什么样的方式搭建（node节点的实现方式似乎有不少），使用什么服务对外公开，以及通过什么途径来进行使用以及宣传，以及监管措施导致的服务商层面的惩罚等，要考虑清楚在弄，不能随随便便就去弄了，毕竟已经吃到过教训了） 4. 数据体验：数据的存储，交互可以说是去中心化网络最大的特色和看点，相比起中心化网络，数据的直接所有人是维护中心化网络的主体（一般来说往往是公司）， 去中心化的nostr社交网络的数据从结论上来看则是“不属于任何人”。 尽管数据直接存储在node节点中，但是用户是可以主动选择node节点的，而且用户数据似乎也会被分发到不同的节点中进行分布式存储，以及node搭建的所有人就算能查看到node里的数据，对于隐私数据一般来说是无法查看的，加密信息，密钥等。 说了“不属于任何人”，当然也不属于你自己。你能做到的只有两件事：发布和交互。你想要删除或者修改都是没有办法的，发布出去的信息相当于“泼出去的水”，而且还不会干，会一直保留在去中心化的网络中。账户也是同理，无法注销，无法修改密码。从好的方面来想一定程度上保证了数据的安全性，以及相比起传统的中心化社交网络来说隐秘性更高，因为你的隐私数据只有你自己（私钥所有人）可以查看，连服务提供者都没法看到； 从不好的方面来考虑，那就是现阶段的去中心化社交网络“太危险了” ：因为数据“不属于任何人”，而认证账户的唯一手段是一个无法更改的密钥，你的账户也是并不是绝对安全的，也没有法律措施能够保护账户上的财产安全，上升到“国家舆论保护”的层面更是如此，所以短时间内这种模式的去中心化社交网络很难在中国普及开来。下放到商业化操作，这种去中心化的社交网络所承载的账号也绝对不是什么好选择，不管是作为公司企业的官方账号，还是偶像演员明星的活动账号，放在这种去中心化的社交网络风险都是相当大的，一方面是资本引来的大量流量，作为媒介的账号本身就具有价值，肯定会有人图谋不轨试图获取账号，万一账号密钥泄露，相关的公司或者艺人几乎没有任何办法再把这个账号挽回，放弃账号的话就相当于把一个常年积累的流量入口拱手让人，甚至会产生负面影响。这点其实下放到个人，上升到国家都是一样的，只是影响规模的程度问题。 综上所述，当前的去中心化社交网络几乎不可能吃到资本带来的流量红利，更多的其实还是向往自由的个体户在进行“尝鲜”。其实这种网络更加符合我心目中的“互联网精神”，其技术架构模式也类似bt那样的文件上传存储下载技术，但是如果失去了“资本的利益”，这种网络究竟能走多远，就不得而知了。虽然个人来说看好这种技术，但是就目前来说，他是一个“自由的混乱都市”，无法承担“世界级”的任务。对了，补充一点。因为账户创建的低成本以及数据所有的特性，nostr其实有被大量bot用户灌溉“垃圾信息”的风险。但是目前来看nostr网络实际存储的信息只有文字和账户基本设定等基本信息，图片等其他资源还是引用的外部资源，包括头像，横幅，以及各种插入图也是。虽然有 nostr.build 这样的资源上传站点，但是实际上是否是用的nostr网络进行存储不得而知，似乎也并不是开源的。Amethyst的上传是可以自选图片托管方的，其他客户端目前不得而知，大概是随便选了个，或者是直接自建了吧。 Nostr浅尝记录http://blog.imc.re/archives/99d4.html","tags":["Blogs","SMGoroBlog"],"categories":["Blogs","SMGoroBlog"]},{"title":"博客入门：每个人的独立博客","path":"/RSSBOX/rss/f066eaa2.html","content":"本文讲了如何零基础创建一个属于自己的博客网站。面向从未用过独立博客，也不了解 markdown 和 git 的用户，旨在普及独立博客技术，顺序经过调整以降低首次上手难度。有相关经验的读者可以自行跳过某些步骤。 博客入门：每个人的独立博客https://xaoxuu.com/blog/20221217/","tags":["Blogs","XAOXUU"],"categories":["Blogs","XAOXUU"]},{"title":"博客进阶：自动化部署","path":"/RSSBOX/rss/e3866d96.html","content":"本文讲了如何利用脚本和 GitHub Actions 简化博客搭建和部署流程，提高效率。 博客进阶：自动化部署https://xaoxuu.com/blog/20221126/","tags":["Blogs","XAOXUU"],"categories":["Blogs","XAOXUU"]},{"title":"GitHub Codespaces 快速体验","path":"/RSSBOX/rss/d8dbb789.html","content":"GitHub 最近上线了 Codespaces 功能，免费账号可以使用 120 core-hours compute • 15GB storage 额度，感觉挺不错的，赶快来体验吧～ GitHub Codespaces 快速体验https://xaoxuu.com/blog/20221121/","tags":["Blogs","XAOXUU"],"categories":["Blogs","XAOXUU"]},{"title":"探索 Stellar 时间线标签的 N 种用法","path":"/RSSBOX/rss/112b630b.html","content":"时间线标签是 Stellar 最强大的特性之一，它可以以侧边栏组件身份出现在左侧边栏，可以以标签插件形式出现在文章任意位置，以下是笔者能想到的几种常见用法。 探索 Stellar 时间线标签的 N 种用法https://xaoxuu.com/blog/20221029/","tags":["Blogs","XAOXUU"],"categories":["Blogs","XAOXUU"]},{"title":"辛普森悖论 | 你还相信数据吗","path":"/RSSBOX/rss/91c832c4.html","content":"数据是一个有力的武器，它既能被用来澄清现实，也能被用来混淆是非你知不知道，数据也会说谎？一个栗子假设您患有肾结石并去看医生。医生告诉你有两种治疗方法，治疗 A（开放手术 open surgery）和治疗 B（体外冲击波碎石术 ESWL）。你问哪种治疗效果更好，医生说：“一项研究发现治疗 A 的成功概率高于治疗 B。”你说：“我会接受治疗 A，谢谢！”这时医生打断你，“但同样的研究还研究了哪种治疗效果更好，这取决于患者是大肾结石还是小肾结石。”你说：“好吧，我有大肾结石还是小肾结石？”你说话的时候，医生又打断了你，说：“其实没关系。你看，他们发现治疗 B 比治疗 A 成功的概率更高，不管你的肾结石是大还是小。”你可能想知道你是否没看错。听起来不可能。但这是真的：在一项实际研究中，发现治疗 B 比治疗 A 对大肾结石和小肾结石起作用的概率更高，尽管事实上治疗 A 的总体 概率高于治疗 B。这是研究数据：治疗 A 有帮助治疗 B 有帮助大肾结石 69% (55 / 80)73% (192 / 263) 小肾结石 87% (234 / 270)93% (81 / 87) 所有患者 83% (289 / 350)78% (273 / 350)表中的第一项显示，80 名大肾结石患者接受了 A 治疗，治疗帮助了其中 55 人，成功率为 69%。这不如治疗 B 好，它帮助了 263 名大肾结石患者中的 192 人，成功率为 73%。以类似的方式，第二行显示治疗 B 比治疗 A 对患有小肾结石的人更有效。但是当你把每一列的数字加起来时，你会发现治疗 A 确实比治疗 B 整体效果更好。值得花时间检查所有数字加起来检验一下，并说服自己我没有欺骗你.刚刚展示的这种现象被称为辛普森悖论。如果你和包括我在内的大多数人一样，那么辛普森悖论在你第一次见到它时就会令人震惊。因为它违反了我们对世界推理的本能方式。而且，正如我们看到的那样，辛普森悖论不仅是一种怪异的现象，而且它经常在具有重要决策后果的地方出现。表达式我们抽离出符号表达：统计对象 1 统计对象 2 分项指标 1 分项指标 2 总计如果那么，推不出以上就是辛普森悖论比较通俗易懂的表达式了。我们忽略了什么从数据生成过程（因果模型）来看分析.事实证明，小肾结石被认为是不严重的病例，治疗 B（体外冲击波碎石术 ESWL）比治疗 A（开放手术 open surgery）更加激进。对于小肾结石，医生更有可能推荐保守疗法 A，因为病情不太严重，患者最有可能首先成功恢复。对于严重的大肾结石，医生往往选择更激进的疗法 B。即使疗法 B 在这些病例中表现更好，由于是更严重的病例，疗法 B 的总体恢复率低于疗法 A.在这个现实世界的例子中，肾结石的大小（病例的严重性）是一个混合变量，它会同时影响自变量（疗法）和因变量（恢复率）.疗法病例的严重性恢复率疗法病例的严重性为了确定哪种治疗方法确实更好，我们需要通过对两组数据进行分离并比较组内的恢复率而不是按组聚合来控制混合变量。小肾结石恢复率疗法大肾结石恢复率疗法这样看来激进的治疗 B（体外冲击波碎石术 ESWL）效果更好.如果有潜在变量（特别是混合变量）存在，牢记：整体数据未必可靠，要通过科学合理的分组来查看具体细致的数据。启示数据从来都不是完全客观的。我们必须对这些数字持怀疑态度. 辛普森悖论的出现是因为人们忽略了研究的因果关系，一旦我们理解了数据生成的机制，我们就可以寻找影响结果的其他因素，而图表不会告诉你这些. 充分考察事件的潜在影响因素和维度，系数消除分组数据基数差异造成的影响. 要求我们具备科学辩证思维，客观看待关联现象。很多时候，我们选择相信直觉，因为我们的直觉往往很准。但是，在信息不全或者信息非对称的情况下，直觉常常是是值得怀疑的。 辛普森悖论 | 你还相信数据吗https://blog.mhuig.top/p/2f345bb/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"尝试在博客中添加简易文章推荐功能","path":"/RSSBOX/rss/175a1706.html","content":"把文档转换成 “向量”，并且尝试用线性代数等数学工具来解决信息检索这类问题，至少可以追溯到 20 世纪 70 年代。想到在 Hexo 这种静态博客系统，特别是 GitHub Page 这种 Serverless 服务中使用 Word2Vec 等深度学习方法似乎并不现实，所以这里使用的是一个非常简单经典的 TF-IDF 算法. TF-IDF 算法原来是搜索引擎中的核心部分，谷歌百度已经使用 TF-IDF 作为内容排名因素很长一段时间.现在的搜索引擎一般用如下的算法计算网站页面得分：score (页面得分) = TFIDF 分 * x + 链接分 * y + 用户体验分 * z（其中 x+y+z=100%），TFIDF 分值百度大约占 40% 左右的权重，谷歌更是达到了 50%.这是百度的计算方法： score (页面得分) = 40% 的内容质量相关性（TFIDF）+ 40% 的用户体验分 + 20% 的链接分（域名 + 外链）.搜索引擎使用 TF-IDF 来计算内容质量相关性，我们这边也可以用它来计算文章内容相关性，然后实现简易文章推荐功能。当然也可以使用文章的 tags 标签最为推荐依据，但是这样过于依赖人工标注数据，顺便最近发现在 Hexo 中分类和标签只能在 source/_posts 文件夹中使用才能渲染出来。构建向量空间模型将文档转化为向量来表示，这样文档与文档之间就可以定量的去度量他们之间的关系，挖掘文档之间的联系。上面提到的 Word2Vec 是把单词转化为向量来表示，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系.比如将词的维度降维到 2 维，用下图的词向量表示词:似乎发现了什么不得了的事情。对于自然语言书写的文本这种非结构化信息，从文本中抽取出的特征来量化来表示文本信息，构建向量空间模型，并基于数学模型的处理，将文本转换为机器可以理解的语言的方式是很重要的。举个栗子要实现文章推荐，就需要从一堆文章中找出一些内容相似的文章。基本思路就是，如果文章的关键词越相似，那么他们的内容就会越相似。我们先从几个简单的句子着手。句子A：我这里有苹果和西瓜。句子B：我喜欢吃西瓜，不喜欢吃苹果。句子C：我喜欢吃蔬菜。第一步：数据清洗简单去除标点符号等干扰信息。句子A: 我这里有苹果和西瓜句子B: 我喜欢吃西瓜 不喜欢吃苹果句子C: 我喜欢吃蔬菜第二步：分词使用分词工具进行分词。句子A:[ '我', '这里', '有', '苹果', '和', '西瓜' ]句子B:[ '我', '喜欢', '吃', '西瓜', '不', '喜欢', '吃', '苹果']句子C:[ '我', '喜欢', '吃', '蔬菜' ]第三步：去重收集所有句子中所有的词[ '我', '这里', '有', '苹果', '和', '西瓜', '喜欢', '吃', '不', '蔬菜']第四步：计算词频词频背后的假设是文章的重要程度即文章的相关度与单词在文档中出现的次数成正比。文章的关键词应当比文章中的其他词出现的次数多。词频某个词在文章中出现的次数句子A:&#123; '我': 1, '这里': 1, '有': 1, '苹果': 1, '和': 1, '西瓜': 1, '喜欢': 0, '吃': 0, '不': 0, '蔬菜': 0&#125;句子B:&#123; '我': 1, '这里': 0, '有': 0, '苹果': 1, '和': 0, '西瓜': 1, '喜欢': 2, '吃': 2, '不': 1, '蔬菜': 0&#125;句子C:&#123; '我': 1, '这里': 0, '有': 0, '苹果': 0, '和': 0, '西瓜': 0, '喜欢': 1, '吃': 1, '不': 0, '蔬菜': 1&#125;第四步：词频标准化文章的篇幅有长有短，为了便于不同文章的比较，进行” 词频” 标准化。词频标准化的目的是把所有的词频在同一维度上分析。词频标准化有两种方案：方案一：词频某个词在文章中出现的次数文章的总词数方案二：词频某个词在文章中出现的次数该文出现最多的词出现的次数一般情况下，第二个标准化方案更适用，因为能够使词频的值相对大点，便于分析。有时候常常用 这个值来代替原来的 TF 取值。这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长。这里使用比较简单的方案一作为栗子：句子A:&#123; '我': 0.16666666666666666, '这里': 0.16666666666666666, '有': 0.16666666666666666, '苹果': 0.16666666666666666, '和': 0.16666666666666666, '西瓜': 0.16666666666666666, '喜欢': 0, '吃': 0, '不': 0, '蔬菜': 0&#125;句子B:&#123; '我': 0.125, '这里': 0, '有': 0, '苹果': 0.125, '和': 0, '西瓜': 0.125, '喜欢': 0.25, '吃': 0.25, '不': 0.125, '蔬菜': 0&#125;句子C:&#123; '我': 0.25, '这里': 0, '有': 0, '苹果': 0, '和': 0, '西瓜': 0, '喜欢': 0.25, '吃': 0.25, '不': 0, '蔬菜': 0.25&#125;第五步：计算逆文档频率仅有词频 (TF) 不能比较完整地描述文档的相关度。因为语言的因素，有一些单词可能会比较自然地在很多文档中反复出现，比如这里中的 “我”、“和” 等等。这些词大多起到了链接语句的作用，是保持语言连贯不可或缺的部分。很明显，如果有太多文档都涵盖了某个单词，这个单词也就越不重要，或者说是这个单词就越没有信息量。因此，我们需要对词频 (TF) 的值进行修正，而 逆文档频率 (IDF) 的想法是用文档频率 (DF) 的倒数来进行修正。逆文档频率语料库的文档总数包含该词的文档数如果一个词越常见，那么分母就越大，逆文档频率就越小越接近 0。分母之所以要加 1，是为了避免分母为 0（即所有文档都不包含该词）。log 表示对得到的值取对数。为什么要用 log 函数？log 函数是单调递增，求 log 是为了归一化，保证逆文档频率不会过大。用 Log 进行变换，也是一个非线性增长的技巧，这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长。句子A:&#123; '我': -0.2876820724517809, '这里': 0.4054651081081644, '有': 0.4054651081081644, '苹果': 0, '和': 0.4054651081081644, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0.4054651081081644, '蔬菜': 0.4054651081081644&#125;句子B:&#123; '我': -0.2876820724517809, '这里': 0.4054651081081644, '有': 0.4054651081081644, '苹果': 0, '和': 0.4054651081081644, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0.4054651081081644, '蔬菜': 0.4054651081081644&#125;句子C:&#123; '我': -0.2876820724517809, '这里': 0.4054651081081644, '有': 0.4054651081081644, '苹果': 0, '和': 0.4054651081081644, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0.4054651081081644, '蔬菜': 0.4054651081081644&#125;第六步：计算 TF-IDF词频逆文档频率可以看到，TF-IDF 与一个词在文档中的出现次数成正比，与该词在整个文档库中的出现次数成反比。句子A:&#123; '我': -0.047947012075296815, '这里': 0.06757751801802739, '有': 0.06757751801802739, '苹果': 0, '和': 0.06757751801802739, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0, '蔬菜': 0&#125;句子B:&#123; '我': -0.03596025905647261, '这里': 0, '有': 0, '苹果': 0, '和': 0, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0.05068313851352055, '蔬菜': 0&#125;句子C:&#123; '我': -0.07192051811294523, '这里': 0, '有': 0, '苹果': 0, '和': 0, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0, '蔬菜': 0.1013662770270411&#125;第七步：列出文档向量这里准确来说应该是 TF-IDF 特征向量，这是一个稀疏矩阵。下面把它作为描述文章特征的向量。说明一点，这里的文章的特征不是由某几个关键词的 TF-IDF 值决定的，而是由所有的词的 TF-IDF 值共同决定的。句子A:[ -0.047947012075296815, 0.06757751801802739, 0.06757751801802739, 0, 0.06757751801802739, 0, 0, 0, 0, 0]句子B:[ -0.03596025905647261, 0, 0, 0, 0, 0, 0, 0, 0.05068313851352055, 0]句子C:[ -0.07192051811294523, 0, 0, 0, 0, 0, 0, 0, 0, 0.1013662770270411]可以对文档向量进行标准化，使得这些向量能够不受向量里有效元素多少的影响，也就是不同的文档可能有不同的长度。把向量都标准化为一个单位向量的长度。这个时候再进行点积运算，就相当于在原来的向量上进行余弦相似度的运算。第八步：计算余弦相似度到这里，如何寻找两篇相似文章的问题转变成了如何计算这两个向量的相似程度的问题。什么叫做向量相似？一般地，如果两个向量平行或重合，我们认为这两个向量相似度为 。如果这两个向量垂直，或者说正交，我们认为这两个向量的相似度为 。以二维空间为例， 和 是两个向量，我们要计算它们的夹角 。余弦定理告诉我们，可以用下面的公式求得：假定和是两个维向量，我们把这些向量放到维欧几里得空间中讨论，向量之间的距离使用欧几里得距离。假定是 ，是 ，则与的夹角的余弦等于：&#123; '句子A': &#123; '句子A': 1.0000000000000002, '句子B': 0.21934876427664535, '句子C': 0.21934876427664535 &#125;, '句子B': &#123; '句子A': 0.21934876427664535, '句子B': 1, '句子C': 0.33484380220099325 &#125;, '句子C': &#123; '句子A': 0.21934876427664535, '句子B': 0.33484380220099325, '句子C': 1 &#125;&#125;第九步：余弦相似度按照降序排序两两计算向量夹角的余弦值，然后按降序排列，取排在最前面的几个文章，就是相似度较高的文章，作为推荐结果。&#123; '句子A': &#123; '句子A': 1.0000000000000002, '句子B': 0.21934876427664535, '句子C': 0.21934876427664535 &#125;, '句子B': &#123; '句子B': 1, '句子C': 0.33484380220099325, '句子A': 0.21934876427664535 &#125;, '句子C': &#123; '句子C': 1, '句子B': 0.33484380220099325, '句子A': 0.21934876427664535, &#125;&#125;相同句子的余弦值是 . 余弦值越接近，就表明夹角越接近度，也就是两个向量越相似.第十步：收集推荐结果&#123; '句子A': [ '句子B', '句子C' ], '句子B': [ '句子C', '句子A' ], '句子C': [ '句子B', '句子A' ]&#125;即：A-BB-CC-BA 和 C 之间没什么关系。源码实现Volantis 主题实现文章推荐https://github.com/volantis-x/hexo-theme-volantis/blob/684c40333b16b5d1df1a9e4a7f1da90216e5a17a/scripts/helpers/ 文章推荐.jsUI 界面是随手写的，又不是不能用（代码变量是中文写的，方便易读。其实是中间出 Bug 了，然后调试 Debug 时顺便整的活 尝试在博客中添加简易文章推荐功能https://blog.mhuig.top/p/175a1706/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"记一次重装系统 (Win11)","path":"/RSSBOX/rss/7de86e6a.html","content":"电脑开机后要先卡顿 30 min，然后才能正常工作，还有不明程序一直在读写磁盘，噪声太大了，没救了重装系统吧。重装系统之后操作瞬间就流畅很多～意料之外：重装系统之后，win11 自带的杀软开始报毒，之前在虚拟机看过镜像中是没有这个病毒文件的。使用老毛桃制作启动盘，事先格式了系统盘，推测可能是老毛桃启动盘释放的病毒文件，或者是上一个系统的隐藏分区释放的病毒文件，google: 老毛桃启动盘携带木马病毒，这里建议使用微软官方的镜像和安装介质。重装系统前的备份列表 以下内容适用于 win10 与 win11PE官方镜像和安装介质老毛桃 UEFI 版操作系统windows 镜像库Win11 安装跳过 tpm 2.0 检测的方法TPM 对笔者来说毫无用处打开 Windows 11 的 ISO 镜像安装包，打开后将目录 Source 中的 install.wim 文件复制到桌面上。然后解压 Windows 10 的 ISO 安装包，然后将 Win11 的 install.win 文件拷到解压后的 win10 安装包的 sources 文件夹，双击 Windows 10 安装包里的 setup.exe 文件开始安装，借用 Windows10 安装程序去安装 Windows11 系统W10 Digital License Activation Script笔者发现装完系统以后 win11 已经激活了，是因为之前使用了 W10 Digital License Activation Script。OfficeOffice 镜像库Office Tool PlusOffice Tool Plus 入门教程kms listkms.loli.best 1688浏览器ChromeFirefoxTor Browser安全管理火绒安全腾讯电脑管家Kaspersky暂时只装火绒。Windows 应用商店7-zipUbuntu On windowsKali LinuxIDE 文本编辑器Visual Studio CodeVisual Studiovs2022 永久激活密钥：Visual Studio 2022 Enterprise：VHF9H-NXBBB-638P6-6JHCY-88JWHVisual Studio 2022 Professional：TD244-P4NB7-YQ6XK-Y8MMM-YWV2JCodeBlocksJetBrains PycharmJetBrains Ideahttps://www.exception.site/essay/how-to-free-use-intellij-idea-2019-3Notepad++EclipseMyEclipseTyporaMatlab代理工具Clash for WindowsV2rayNShadowsocketsRDOH阿里：https://dns.alidns.com/dns-query 腾讯：https://doh.pub/dns-query驱动NVIDIAATK Packagecudacudnn开发环境TDM-GCC x64pythonpip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simpleWriting to %appdata%\\pip\\pip.inianacondaJava SE Development Kitnvm找到 nvm 文件位置，点开 settings.txtroot: C:\\software vmpath: C:\\software odejsnode_mirror: https://npm.taobao.org/mirrors/node/npm_mirror: https://npm.taobao.org/mirrors/npm/使用nvm ls // 看安装的所有node.js的版本nvm list available // 查显示可以安装的所有node.js的版本nvm install 版本号 // 例如：nvm install 16.16.0nvm use 版本号 // 切换到使用指定的nodejs版本版本控制GitGitHub DeskTop配置 .gitconfig.gitconfig[user] name = MHuiG email = xxxxxx@qq.com signingkey = BA16368BD4C4169C[gui] encoding = utf-8[filter \"lfs\"] smudge = git-lfs smudge -- %f process = git-lfs filter-process required = true clean = git-lfs clean -- %f[http] schannelCheckRevoke = false[commit] gpgsign = true[gpg] program = C:\\\\software\\\\GnuPG\\\\bin\\\\gpg.exe[init] defaultBranch = main[core] editor = \\\"C:\\\\software\\\\Microsoft VS Code\\\\bin\\\\code\\\" --wait聊天工具QQ微信企业微信Telegram Desktop远程连接XshellXftpSecureCRTVNC数据库MySQLNavicatmongoDBRobo 3TredisRedisDesktopManager截屏工具ScreenToGifGreenshotGPGkeybaseGpg4win下载工具迅雷百度网盘 Aria2杂项搜狗输入法VMware16: ZF3R0-FHED2-M80TY-8QYGC-NPKYFXMindPostMan网易有道词典CAJViewerBestTraceStarUMLHDFViewAdobe Photo Shop 网易云音乐 MathTypePotPlayerPortableDism++UltraISOMyDiskTestDiskGeniuschrome 扩展AdBlock — 最佳广告拦截工具 Enable CopyMetaMaskTampermonkey BETAWappalyzer - Technology profiler 迅雷下载支持windows 导出备份 WiFi 密码Wi-Fi-code.bat@echo off for /f \"skip=9 tokens=1,2 delims=:\" %%i in ('netsh wlan show profiles') do @echo %%j | findstr -i -v echo | netsh wlan show profiles %%j key=clear &gt;&gt;%USERPROFILE%\\desktop\\Wi-Fi-code.txtstart %USERPROFILE%\\desktop\\Wi-Fi-code.txtwindows 提权装完系统先搞权限。夺回 Windows 系统权限碰到过这样的提示：“无法使用内置管理员账户打开 XX 程序，请使用其他账户登录”其实已经使用管理员账户登录了，为什么还会出现这样的提示呢？这是因为使用的内置账户没有对应用程序的操作权限，可以使用注册表来夺回 Windows10 系统权限。账户详解：使用的管理员账户，就是使用用户名或微软账号登录到 Windows 的账号，管理权限无限近似于 Administrator，但在某些文件 / 文件夹的操作上，还是不如真正的 Administrator 好用。在 Windows 内置的账户中，还有两个比 Administrator 权限更高的账户，一个是肉眼可见的 System 权限，最终的 Boss 则是 TrustedInstaller，其实所有账户中它也不是最高的那个，但剩下的无论通过什么方式，都挖掘不出来了，也许超级账户就是 Microsoft 自己了。Windows 夺回系统权限的操作方法：1、Win+R 组合键之后，输入 regedit ，打开注册表；2、定位到：HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System ，在右侧栏找到 FilterAdministratorToken ，双击后将数值数据修改为 1 之后点击 确定。（如果没有的话，就使用鼠标右键新建个 DWORD（32位） 值，将其命名为 FilterAdministratorToken 。）3、之后再定位到：HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\UIPI ，将右侧的默认项目的数值数据修改为 1。4、完成上述操作后，重启电脑或注销当前 Windows 账户后，再进入 “控制面板 - 系统与安全 - 用户账户控制设置”，将 “通知选项” 设置为默认就 OK 了。开启 administrator 账户net user administrator /active:yes右键菜单权限选项管理员取得所有权.regWindows Registry Editor Version 5.00[-HKEY_CLASSES_ROOT\\*\\shell\\runas][HKEY_CLASSES_ROOT\\*\\shell\\runas]@=\"获取超级管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,-78\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\*\\shell\\runas\\command]@=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"[-HKEY_CLASSES_ROOT\\Directory\\shell\\runas][HKEY_CLASSES_ROOT\\Directory\\shell\\runas]@=\"获取超级管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,-78\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\Directory\\shell\\runas\\command]@=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\"[-HKEY_CLASSES_ROOT\\dllfile\\shell][HKEY_CLASSES_ROOT\\dllfile\\shell\\runas]@=\"获取超级管理员权限\"\"HasLUAShield\"=\"\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\dllfile\\shell\\runas\\command]@=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"[-HKEY_CLASSES_ROOT\\Drive\\shell\\runas][HKEY_CLASSES_ROOT\\Drive\\shell\\runas]@=\"获取超级管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,-78\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\Drive\\shell\\runas\\command]@=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\"恢复原始权限.regWindows Registry Editor Version 5.00;恢复原始权限 [HKEY_CLASSES_ROOT\\*\\shell\\runas-] @=\"恢复原始权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,101\"\"NoWorkingDirectory\"=\"\"; &amp;&amp; takeown /f \\\"%1\\\"[HKEY_CLASSES_ROOT\\*\\shell\\runas-\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"[HKEY_CLASSES_ROOT\\exefile\\shell\\runas2-] @=\"恢复原始权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,101\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\exefile\\shell\\runas2-\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"[HKEY_CLASSES_ROOT\\Directory\\shell\\runas-] @=\"恢复原始权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,101\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\Directory\\shell\\runas-\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"取得文件修改权限.regWindows Registry Editor Version 5.00;取得文件修改权限 [HKEY_CLASSES_ROOT\\*\\shell\\runas] @=\"获取管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,102\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\*\\shell\\runas\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"[HKEY_CLASSES_ROOT\\exefile\\shell\\runas2] @=\"获取管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,102\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\exefile\\shell\\runas2\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"[HKEY_CLASSES_ROOT\\Directory\\shell\\runas] @=\"获取管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,102\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\Directory\\shell\\runas\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\"移除权限选项.regWindows Registry Editor Version 5.00[-HKEY_CLASSES_ROOT\\*\\shell\\runas][-HKEY_CLASSES_ROOT\\exefile\\shell\\runas2][-HKEY_CLASSES_ROOT\\Directory\\shell\\runas][-HKEY_CLASSES_ROOT\\*\\shell\\runas-][-HKEY_CLASSES_ROOT\\exefile\\shell\\runas2-][-HKEY_CLASSES_ROOT\\Directory\\shell\\runas-]获取 TrustedInstaller 超级权限TrustedInstaller 超级权限是凌驾于管理员权限和系统权限之上的存在TrustedInstaller 超级权限的添加方法：下载注册表权限修改工具 SetACL: https://helgeklein.com/download/#download-setaclSetACL 文档： https://helgeklein.com/setacl/https://helgeklein.com/setacl/documentation/command-line-version-setacl-exe/按照如下格式设置执行获取权限命令：SetACL -on 对象名称 -ot 对象类型 -actn 操作对象名称 (-on)：这是 SetACL 应操作的对象的路径。对象类型（-ot）：对象名称指的是什么类型的对象：文件或目录（file）、注册表项（reg）、服务（srv）、打印机（prn）、网络共享（shr）操作 (-actn)：SetACL 应该如何处理指定的对象注册表值权限获取就可以用如下命令：SetACL.exe -on \"HKEY_CLASSES_ROOT\\XXX\\&#123;xxxxxxx-666-5555-EEEE-yyyyyy&#125;\" -ot reg -actn setowner -ownr \"n:Administrators\";;注释：将上述注册表项所有者更新到管理员 Administrator（默认为不可更改的 TrustedInstaller ）。SetACL.exe -on \"HKEY_CLASSES_ROOT\\XXX\\&#123;xxxxxxx-666-5555-EEEE-yyyyyy&#125;\" -ot reg -actn ace -ace \"n:Administrators;p:full\";;注释：让上述注册表项所有者 Administrator 获取全部权限。注意，上述命令缺一不可，而且要按照先后顺序执行。友情提醒：命令 SetACL -on 对象名称 -ot 对象类型 -actn 操作 适合文件、文件夹（需要写详细路径）和注册表项目超级权限获取，各位可按需提权，出于安全考虑，切勿滥用。如果修改完毕，还可以考虑把命令中的管理员 Administrator 替换为 TrustedInstaller 再次执行命令，恢复系统默认，确保安全。windows 取消登录界面的名字1、打开 regedit。2、定位到以下位置：\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\，找到名为 dontdisplaylastusername 的键值，双击它，然后将它的数值数据改为 1。3、将鼠标光标放在左侧树状列表的 System 项上，单击右键，选择新建 - DWORD（32 位）值，并将该值命名为：DontDisplayLockedUserID。4、双击我们刚刚新建的名为 DontDisplayLockedUserID 的 DWORD（32 位）值，你将看到一个编辑 DWORD（32 位）值的窗口。在这个窗口中，将 DontDisplayLockedUserID 的数值数据改为 3。5、修改完成后，关闭注册表编辑器，在开始菜单中点击你的头像，再点击锁定，你将看到锁屏界面。6、按回车键，或者使用鼠标点击屏幕，你将看到 Win 登录界面。这时我们可以看到，我们的名字已经显示为 “解锁电脑”。7、要登录 Win，你需要手动输入用户名和密码或者 PIN，需要注意的是，这个用户名以 C:\\Users 中的为准。如果你不清楚你当前的用户名，那么可以打开命令提示符后，里面显示的是 C:\\Users\\XXX &gt;，那么当前的用户名就是 XXX ，在登录 Win 时，将 XXX 填入用户名的输入框即可。要将 Win10 登陆界面恢复为显示姓名也非常简单，只需再次来到注册表，然后双击名为 dontdisplaylastusername 的键值，将它的数值数据改为 0 即可。windows 取消登录界面电源按钮1、按下 Win+r 打开运行，输入 regedit 回车，打开注册表编辑器；2、定位至：HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System，然后你就会看到 shutdownwithoutlogon 这个 32 位的 DWORD 值；3、双击 shutdownwithoutlogon，打开编辑界面，将数据设置为０；原值为 1.4、操作完成后，你就会发现，登陆界面的电源按钮没了。上帝模式即 God Mode 完全控制面板开启方法:新建文件夹重命名为GodMode.&#123;ED7BA470-8E54-465E-825C-99712043E01C&#125;时间显示到秒实验中发现 win11 删除了注册表 ShowSecondsInSystemClock，需要先下载安装 startallback，然后笔者放弃了时间显示到秒。win11 需要先下载安装 startallback, win10 直接修改注册表修改注册表 HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Advanced新建 DWORD (32 位) 值 改为 1: “ShowSecondsInSystemClock”=dword:00000001 重启使用注册表隐藏磁盘盘符具体操作如下：注：在开始之前，请使用 Administrator 帐户登录 Windows10。1、首先，将鼠标光标放在 “此电脑” 图标上，点击右键，再点击管理。2、在窗口左侧的树状列表中展开本地用户和组，点击用户文件夹。3、双击帐户列表中的 Administrator 项，将账户已禁用项取消勾选，然后点击确定按钮。4、这时，点击开始按钮，再点击你的头像，你可以看到 Administrator 项，只要点击它，我们就可以 以 Administrator 帐户登录 Windows 了。5、以 Administrator 账户登录 Windows 后，打开 regedit。6、定位到以下位置：\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\7、在窗口右侧的空白处点击鼠标右键，选择新建 - 新建 DWORD（32 位）值，并将该值命名为：NoDrives8、以下是一份对照表，这个表我们接下来有用。 A：1 B：2 C：4 D：8 E：16 F：32 G：64 H：128 I：256 J：512 K：1024 L：2048 M：4096 N：8192 O：16384 P：32768 Q：65536 R：131072 S：262144 T：524288 U：1048576 V：2097152 W：4194304 X：8388608 Y：16777216 Z：33554432 所有：67108863想要隐藏 E 盘，查表可知，该盘对应的值为 16。9、双击我们刚刚新建的名为 NoDrives 的 DWOED（32 位）值，将该值基数改为 10 进制，再将该值的数值数据设置为 16。10、重启电脑后，打开文件资源管理器（此电脑），可见，E 盘已经消失了。11、要进入 E 盘非常简单，我们只需在文件资源管理器的地址栏输入：E:\\，然后回车即可。如何恢复？如果你不想再隐藏 E 盘，那么你可以通过以下方式恢复：以 Administrator 帐户登录 Windows。打开 regedit，定位到以下位置：\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policie，找到名为 NoDrives 的 DWORD（32 位）值，双击该值，将其数值数据改为 0，然后重启即可。重置组策略在 Windows 系统中，通过组策略我们可以设置系统的各种软件、计算机和用户策略等。不小心将组策略中的设置修改错了，导致系统中的很多组件都无法使用了，这该怎么办呢？这时候重置组策略编辑器是最好的解决办法还原本地安全策略用于管理 Windows 安全选项的安全策略与组策略使用了不同的管理控制台 ——secpol.msc（本地安全策略），该安全设置管理单元对组策略进行了扩展，可方便个人用户或域管理员手动配置和定义计算机安全策略。如果你对 Windows 的安全管理策略不太了解，又自己手动更改了一些乱七八糟的设置，可以通过如下步骤对本地安全策略进行还原：1、使用 Windows + X 快捷键打开「命令提示符（管理员）」；2、执行如下命令：secedit /configure /cfg %windir%\\inf\\defltbase.inf /db defltbase.sdb /verbose3、命令执行完成之后需要重启计算机才能生效，如果某些组件仍出现奇怪的问题，可以通过下面介绍的步骤来重置组策略对象。使用命令行重置组策略对象此种方法比较特殊，我们可以直接从安装 Windows 的分区中直接删除组策略配置文件夹，以达到全部重置目的：1、使用 Windows + X 快捷键打开「命令提示符（管理员）」；2、执行如下命令：RD /S /Q \"%WinDir%\\System32\\GroupPolicyUsers\"RD /S /Q \"%WinDir%\\System32\\GroupPolicy\"gpupdate /force3、命令执行完成后重启计算机即可。禁用设置和控制面板该方法适用于：Win10 + 专业版 / 企业版 / 教育版1、运行 gpedit；2、定位到：用户配置 - 管理模板 - 控制面板，在窗口的右侧找到并双击禁止访问 “控制面板” 和 PC 设置；3、在弹出的窗口中选择已启用，点击确定按钮；4、当你完成这些步骤后，用户将无法打开设置。该方法适用于：Win10 + 家庭版重要提醒：编辑注册表是有风险的，如果你不小心误操作，这则可能对你电脑的系统造成不可逆转的损害，在继续之前，我们建议你备份注册表或者创建系统还原点。1、运行 regedit；2、定位到：\\HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\，在窗口右侧的空白处单击鼠标右键，选择新建 - DWORD（32 位）值，并将此值命名为：NoControlPanel；3、双击刚刚新建的名为 NoControlPanel 的 DWORD（32 位）值，将数值数据由 0 改为 1 ，点击确定按钮；4、完成这些步骤后，用户将无法打开设置。如何恢复？1、如果你是使用组策略编辑器来禁用设置和控制面板的，要恢复原样，你需要进入组策略编辑器，来到以下目录：用户配置 - 管理模板 - 控制面板，在窗口的右侧重新找到禁止访问 “控制面板” 和 PC 设置，双击它，然后在弹出的窗口中将已启用改回未配置，点击确定按钮。2、如果你是使用注册表编辑器来禁用设置和控制面板的，要恢复原样，你需要进入注册表编辑器，来到以下目录：\\HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\，在窗口的右侧找到名为 NoControlPanel 的 DWORD（32 位）值，双击它，将数值数据由 1 改成 0 ，点击确定按钮。禁用注册表没事不要乱动注册表 / 邪恶禁用注册表.regWindows Registry Editor Version 5.00[HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System]\"DisableRegistryTools\"=dword:00000001解锁注册表.inf[Version]Signature=“$CHICAGO$”[DefaultInstall]DelReg=del[del]HKCU,Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System,Disableregistrytools,1,00,00,00,00右键选择安装解锁修复注册表.cmdreg add \"HKEY_LOCAL_MACHINESOFTWAREMicrosoftWindowsSelfHostApplicability\" /v \"BranchName\" /d \"fbl_release\" /t REG_SZ /freg add \"HKEY_LOCAL_MACHINESOFTWAREMicrosoftWindowsSelfHostApplicability\" /v \"ThresholdRiskLevel\" /d \"low\" /t REG_SZ /freg deldte \"HKEY_LOCAL_MACHINESOFTWAREMicrosoftWindowsSelfHostApplicability\" /v \"ThresholdInternal\" /freg deldte \"HKEY_LOCAL_MACHINESOFTWAREMicrosoftWindowsSelfHostApplicability\" /v \"ThresholdOptedIn\" /f 记一次重装系统 (Win11)https://blog.mhuig.top/p/de4b9576/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"1729818333.4104185","path":"/RSSBOX/rss/9e0f5fb7.html","content":".fa-secondary&#123;opacity:.4&#125;CoPoKoCoPoKo开始阅读 1729818333.4104185https://blog.mhuig.top/p/wiki-copoko/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"部署第一个智能合约","path":"/RSSBOX/rss/6a03238d.html","content":"注意访问本文的引用链接可能需要魔法对区块链最好的描述是将其描述为一个公共数据库，它由网络中的许多计算机更新和共享。小试身手我猜您和我们一样会很兴奋在以太坊区块链上部署智能合约并与之交互。别担心，作为我们的第一个智能合约，我们会将其部署在本地测试网络上，因此您不需要任何开销就可以随意部署和运行它。编写合约第一步访问 Remix 并创建一个新文件。 在 Remix 界面的左上角添加一个新文件，并输入所需的文件名。在这个新文件中，我们将粘贴如下代码：// SPDX-License-Identifier: MITpragma solidity &gt;=0.5.17;contract Counter &#123; // Public variable of type unsigned int to keep the number of counts uint256 public count = 0; // Function that increments our counter function increment() public &#123; count += 1; &#125; // Not necessary getter to get the count value function getCount() public view returns (uint256) &#123; return count; &#125;&#125;如果您曾经写过程序，应该可以轻松猜到这个程序是做什么的。 下面按行解释：第 3 行：定义了一个名为 Counter 的合约。第 6 行：我们的合约存储了一个无符号整型 count，从 0 开始。第 9 行：第一个函数将修改合约的状态并且 increment() 变量 count。第 14 行，第二个函数是一个 getter 函数，能够从智能合约外部读取 count 变量的值。 请注意，因为我们将 count 变量定义为公共变量，所以这个函数是不必要的，但它可以作为一个例子展示。第一个简单的智能合约到此结束。 正如您所知，它看上去像是 Java、C++ 这样的面向对象编程语言中的一个类。 现在可以运行我们的合约了。部署合约当我们写了第一个智能合约后，我们现在可以将它部署在区块链中并运行它。在区块链上部署智能合约实际上只是发送了一个包含已编译智能合约代码的交易，并且没有指定任何收件人。我们首先点击左侧的编译图标来编译合约：然后点击编译按钮：您可以选择 “自动编译” 选项，这样当您在文本编辑器中保存内容时，合约始终会自动编译。然后切换到部署和运行交易屏幕：在 “部署和运行交易” 屏幕上，仔细检查显示的合约名称并点击 “部署”。 在页面顶部可以看到，当前环境为 “Javascript VM”，这意味着当前我们在本地测试区块链上部署智能合约并交互，这样测试可以更快，也不需要任何费用。点击 “部署” 按钮后，您可以看到合约在底部显示出来。 点击左侧的箭头展开，可以看到合约的内容。 这里有我们的变量 counter、函数 increment() 和 getter getCounter()。如果您点击 count 或 getCount 按钮，它将实际检索合约的 count 变量的内容，并显示出来。 因为我们尚未调用 increment 函数，它应该显示 0。现在点击按钮来调用 increment 函数。 您可以在窗口底部看到交易产生的日志。 当按下检索数据按钮而非 increment 按钮时，您看到的日志有所不同。 这是因为读取区块链的数据不需要任何交易（写入）或费用。 因为只有修改区块链的状态需要进行交易。在按下 increment 按钮后，将产生一个交易来调用我们的 increment() 函数，如果我们点击 count 或 getCount 按钮，将读取我们的智能合约的最新状态，count 变量大于 0。使用事件记录智能合约中的数据在 solidity 中，事件是智能合约可触发的调度信号。 去中心化应用或其他任何连接到以太坊 JSON-PRC API 的程序，都可以监听这些事件，并执行相应操作。 可以建立事件的索引，以便稍后可以搜索到事件历史记录。在撰写这篇文章之时，以太坊区块链上最常见的事件是由 ERC20 代币转账时触发的 Transfer 事件。event Transfer(address indexed from, address indexed to, uint256 value);事件签名在合约代码内声明，并且可以使用 emit 关键字来触发。 例如，transfer 事件记录了谁发起了转账 (from)，转账给谁 (to)，以及转账的代币数转账 (value)。我们再次回到 Counter 智能合约，决定在每次值发生变化时进行记录。 由于这个合约不是为了部署，而是作为基础，通过扩展来构建另一个合约：因此它被称为抽象合约。 在我们 counter 示例中，它将类似于如下：pragma solidity 0.5.17;contract Counter &#123; event ValueChanged(uint oldValue, uint256 newValue); // Private variable of type unsigned int to keep the number of counts uint256 private count = 0; // Function that increments our counter function increment() public &#123; count += 1; emit ValueChanged(count - 1, count); &#125; // Getter to get the count value function getCount() public view returns (uint256) &#123; return count; &#125;&#125;注意：第 5 行：我们声明了事件及其包含的内容、旧值以及新值。第 13 行：当我们增加 count 变量的值时，我们会触发事件。如果我们现在部署合约并调用 increment 函数，如果您在名为 logs 的数组内单击新交易，我们将看到 Remix 会自动显示它。日志在调试智能合约时非常有用，另一方面，如果您构建一个不同人使用的应用，并且使分析更容易跟踪和了解您的智能合约的使用情况，那么日志也是非常重要的手段。 交易生成的日志会显示常见的区块浏览器中，并且，举例来说，您也可以使用它们来创建链外脚本，用于侦听特定的事件，并且这些事件发生时采取相应操作。注册钱包安装 Chrome MetaMask 插件https://chrome.google.com/webstore/detail/metamask/nkbihfbeogaeaoehlefnkodbefgpgknn注册绑定你的账号。添加 BSC Testnet （测试网络）添加自定义网络网络名称：BSC TestnetRPC URL: https://data-seed-prebsc-1-s1.binance.org:8545/ 链 ID: 0x61区块浏览器：https://testnet.bscscan.com/ 货币符号: BNB每日领取测试 BNB进入 https://testnet.binance.org/faucet-smart , 填入账户 Address 领取测试链的 BNB。BNB 用于支付测试网络的燃料费用.在 metamask 钱包中也可以看到这笔 BNB 到账了。发行 MHGC (MHuiGCoin) 代币进入 Remix 环境 中FILE EXPLORERS依次创建三个文件: EIP20.sol、EIP20Factory.sol、EIP20Interface.sol。EIP20.sol/*Implements EIP20 token standard: https://github.com/ethereum/EIPs/blob/master/EIPS/eip-20.md.*/pragma solidity ^0.4.21;import \"./EIP20Interface.sol\";contract EIP20 is EIP20Interface &#123; uint256 constant private MAX_UINT256 = 2**256 - 1; mapping (address =&gt; uint256) public balances; mapping (address =&gt; mapping (address =&gt; uint256)) public allowed; /* NOTE: The following variables are OPTIONAL vanities. One does not have to include them. They allow one to customise the token contract &amp; in no way influences the core functionality. Some wallets/interfaces might not even bother to look at this information. */ string public name; //fancy name: eg Simon Bucks uint8 public decimals; //How many decimals to show. string public symbol; //An identifier: eg SBX function EIP20( uint256 _initialAmount, string _tokenName, uint8 _decimalUnits, string _tokenSymbol ) public &#123; balances[msg.sender] = _initialAmount; // Give the creator all initial tokens totalSupply = _initialAmount; // Update total supply name = _tokenName; // Set the name for display purposes decimals = _decimalUnits; // Amount of decimals for display purposes symbol = _tokenSymbol; // Set the symbol for display purposes &#125; function transfer(address _to, uint256 _value) public returns (bool success) &#123; require(balances[msg.sender] &gt;= _value); balances[msg.sender] -= _value; balances[_to] += _value; emit Transfer(msg.sender, _to, _value); //solhint-disable-line indent, no-unused-vars return true; &#125; function transferFrom(address _from, address _to, uint256 _value) public returns (bool success) &#123; uint256 allowance = allowed[_from][msg.sender]; require(balances[_from] &gt;= _value &amp;&amp; allowance &gt;= _value); balances[_to] += _value; balances[_from] -= _value; if (allowance &lt; MAX_UINT256) &#123; allowed[_from][msg.sender] -= _value; &#125; emit Transfer(_from, _to, _value); //solhint-disable-line indent, no-unused-vars return true; &#125; function balanceOf(address _owner) public view returns (uint256 balance) &#123; return balances[_owner]; &#125; function approve(address _spender, uint256 _value) public returns (bool success) &#123; allowed[msg.sender][_spender] = _value; emit Approval(msg.sender, _spender, _value); //solhint-disable-line indent, no-unused-vars return true; &#125; function allowance(address _owner, address _spender) public view returns (uint256 remaining) &#123; return allowed[_owner][_spender]; &#125;&#125;EIP20Factory.solimport \"./EIP20.sol\";pragma solidity ^0.4.21;contract EIP20Factory &#123; mapping(address =&gt; address[]) public created; mapping(address =&gt; bool) public isEIP20; //verify without having to do a bytecode check. bytes public EIP20ByteCode; // solhint-disable-line var-name-mixedcase function EIP20Factory() public &#123; //upon creation of the factory, deploy a EIP20 (parameters are meaningless) and store the bytecode provably. address verifiedToken = createEIP20(10000, \"Verify Token\", 3, \"VTX\"); EIP20ByteCode = codeAt(verifiedToken); &#125; //verifies if a contract that has been deployed is a Human Standard Token. //NOTE: This is a very expensive function, and should only be used in an eth_call. ~800k gas function verifyEIP20(address _tokenContract) public view returns (bool) &#123; bytes memory fetchedTokenByteCode = codeAt(_tokenContract); if (fetchedTokenByteCode.length != EIP20ByteCode.length) &#123; return false; //clear mismatch &#125; //starting iterating through it if lengths match for (uint i = 0; i &lt; fetchedTokenByteCode.length; i++) &#123; if (fetchedTokenByteCode[i] != EIP20ByteCode[i]) &#123; return false; &#125; &#125; return true; &#125; function createEIP20(uint256 _initialAmount, string _name, uint8 _decimals, string _symbol) public returns (address) &#123; EIP20 newToken = (new EIP20(_initialAmount, _name, _decimals, _symbol)); created[msg.sender].push(address(newToken)); isEIP20[address(newToken)] = true; //the factory will own the created tokens. You must transfer them. newToken.transfer(msg.sender, _initialAmount); return address(newToken); &#125; //for now, keeping this internal. Ideally there should also be a live version of this that // any contract can use, lib-style. //retrieves the bytecode at a specific address. function codeAt(address _addr) internal view returns (bytes outputCode) &#123; assembly &#123; // solhint-disable-line no-inline-assembly // retrieve the size of the code, this needs assembly let size := extcodesize(_addr) // allocate output byte array - this could also be done without assembly // by using outputCode = new bytes(size) outputCode := mload(0x40) // new \"memory end\" including padding mstore(0x40, add(outputCode, and(add(add(size, 0x20), 0x1f), not(0x1f)))) // store length in memory mstore(outputCode, size) // actually retrieve the code, this needs assembly extcodecopy(_addr, add(outputCode, 0x20), 0, size) &#125; &#125;&#125;EIP20Interface.sol// Abstract contract for the full ERC 20 Token standard// https://github.com/ethereum/EIPs/blob/master/EIPS/eip-20.mdpragma solidity ^0.4.21;contract EIP20Interface &#123; /* This is a slight change to the ERC20 base standard. function totalSupply() constant returns (uint256 supply); is replaced with: uint256 public totalSupply; This automatically creates a getter function for the totalSupply. This is moved to the base contract since public getter functions are not currently recognised as an implementation of the matching abstract function by the compiler. */ /// total amount of tokens uint256 public totalSupply; /// @param _owner The address from which the balance will be retrieved /// @return The balance function balanceOf(address _owner) public view returns (uint256 balance); /// @notice send `_value` token to `_to` from `msg.sender` /// @param _to The address of the recipient /// @param _value The amount of token to be transferred /// @return Whether the transfer was successful or not function transfer(address _to, uint256 _value) public returns (bool success); /// @notice send `_value` token to `_to` from `_from` on the condition it is approved by `_from` /// @param _from The address of the sender /// @param _to The address of the recipient /// @param _value The amount of token to be transferred /// @return Whether the transfer was successful or not function transferFrom(address _from, address _to, uint256 _value) public returns (bool success); /// @notice `msg.sender` approves `_spender` to spend `_value` tokens /// @param _spender The address of the account able to transfer the tokens /// @param _value The amount of tokens to be approved for transfer /// @return Whether the approval was successful or not function approve(address _spender, uint256 _value) public returns (bool success); /// @param _owner The address of the account owning tokens /// @param _spender The address of the account able to transfer the tokens /// @return Amount of remaining tokens allowed to spent function allowance(address _owner, address _spender) public view returns (uint256 remaining); // solhint-disable-next-line no-simple-event-func-name event Transfer(address indexed _from, address indexed _to, uint256 _value); event Approval(address indexed _owner, address indexed _spender, uint256 _value);&#125;SOLIDITY COMPILER我们选择 0.4.21 版本的编译环境.点击编译.在这里你可以将源码发布到 IPFS.Metadata of \"eip20\" was published successfully.token/EIP20.sol : dweb:/ipfs/QmZemboWkhVyhYMRyCJmjwhw4caenvXS5Mw7Uc5tdE77jPtoken/EIP20Interface.sol : dweb:/ipfs/QmWQEUAdy5QnnCTGsEtRfedZrVG2xZ77YfKiPE5MVvQiSSmetadata.json : dweb:/ipfs/QmWFrJHPJLEWJGAoXpq8MD1pGJsvdcfxhPiW5SDXiwEvm8DEPLOY &amp; RUN TRANSACTIONS运行环境选择 Injected Web3因为我们用的是 metamask 钱包；Account 账户填写 metamask 钱包账户；此时浏览器插件会弹出，我们选择连接账户。点击下一步，点击连接.在 DEPLOY 中输入部署信息，合约构造函数的输入参数._INITIALAMOUNT: \"21000000000000000000000000\"_TOKENNAME: \"MHuiGCoin\"_DECIMALUNITS: \"18\"_TOKENSYMBOL: \"MHGC\"发币数量 21000000, 和比特币一样，向中本聪致敬。货币名称 MHuiGCoin，最小货币单位 18（decimaUnits），货币简称 MHGC。点击 transact.点击确认支付燃料费用.从 Deployed Contracts 复制部署的合约地址: 0xeb86A66E2d4A51F8de3d4d0F509f18A15Cde68F6.导入代币合约进入部署合约的 MetaMask 账户，点击导入代币.输入代币合约信息代币合约地址：0xeb86A66E2d4A51F8de3d4d0F509f18A15Cde68F6代币符号：MHGC小数精度：18可以看到我的账户中有 21000000 MHGC (MHuiGCoin)代币转账测试我们创建一个新的账户，并导入上面的代币合约.我们向目标账户发送 1 MHGC 测试需要支付测试网络的燃料费用.转账发送成功.测试网络资源管理器在资源管理器中查看 资产: https://testnet.bscscan.com/token/0xeb86A66E2d4A51F8de3d4d0F509f18A15Cde68F6可以看到资源信息和公开的转账记录. 如你所见这些在区块链中已经发生的历史信息是无法更改的.安全风险提示任何人都可以创建代币，包括创建现有代币的假版本。了解更多关于 欺诈和安全风险.2016 年 6 月，The DAOEther 的漏洞造成损失 5000 万美元，而开发者试图达成共识的解决方案。DAO 的程序在黑客删除资金之前有一段时间的延迟。以太坊软件的一个硬分叉在时限到期之前完成了攻击者的资金回收工作。2021 年 9 月 24 日，中国人民银行发布进一步防范和处置虚拟货币交易炒作风险的通知。通知指出，虚拟货币不具有与法定货币等同的法律地位。比特币、以太币、泰达币等虚拟货币具有非货币当局发行、使用加密技术及分布式账户或类似技术、以数字化形式存在等主要特点，不具有法偿性，不应且不能作为货币在市场上流通使用 。最后MHGC 获取方式测试网络网络名称：BSC TestnetRPC URL: https://data-seed-prebsc-1-s1.binance.org:8545/ 链 ID: 0x61区块浏览器：https://testnet.bscscan.com/ 货币符号: BNB代币合约代币合约地址：0xeb86A66E2d4A51F8de3d4d0F509f18A15Cde68F6代币符号：MHGC小数精度：18尾巴联系我，发我地址，在 BSC Testnet 下可领取 100 MHGC.共发行 21000000 MHGC, 集齐 22000000 MHGC 可领取精美礼品一份. Just For Fun.参考资料Welcome to Ethereum 部署第一个智能合约https://blog.mhuig.top/p/6a03238d/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"一组粗略的类比","path":"/RSSBOX/rss/6dc844b7.html","content":"这是一个关于生物学与深度学习的粗略类比的笔记，这些类比中的大多数都是非常推测性和探索性的，但我发现它们很有趣并且值得思考.生物学深度学习外界环境输入数据认知过程特征提取基因神经网络权重结构 (特征) 基因突变神经网络权重结构改变性状神经网络功能细胞神经元组织 (细胞分化) 神经网络在后面的层器官神经网络专门处理特定任务的组件个体模型繁殖迭代反馈调节优化器 (eg: 梯度下降) 更新网络的机制自然选择损失函数定向衡量性能进化学习进化 = 基因突变 + 自然选择 (定向) 学习 = 神经网络权重结构改变 + 损失函数定向衡量性能基因决定性状神经网络权重结构可以决定神经网络功能个体繁殖模型迭代 一组粗略的类比https://blog.mhuig.top/p/6dc844b7/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"唯物辩证法","path":"/RSSBOX/rss/6ace289f.html","content":"唯物辩证法是关于自然界、人类社会以及人类思维领域发展最一般规律的科学，它在坚持唯物论观点的基础上，研究世界的运行状况、形态和发展规律，进一步回答客观世界究竟 “怎么样” 的问题。唯物辩证法的总特征唯物辩证法的基本观点是：世界是普遍联系的有机整体，同时又是变化发展的。联系和发展的观点是唯物辩证法的总特征。事物的普遍联系联系指事物内部各要素之间和事物之间相互影响、相互制约、相互作用的关系。联系的特性及其具体内容如下表所示：特性具体内容客观性事物的联系是事物本身所固有的，不是主观臆想的普遍性任何事物内部的不同部分和要素之间都是相互联系的；任何事物都不能孤立存在，都同其他事物处于一定的联系之中，整个世界是相互联系的统一整体。多样性世界上的事物是多样的，事物之间的联系也是多样的条件性条件是对事物存在和发展发生作用的诸要素的总和。条件对事物发展和人的活动具有支持或制约作用；条件是可以改变的；改变和创造条件不是任意的，必须尊重事物发展的客观规律。事物的变化发展事物的相互联系包含事物的相互作用，而相互作用必然导致事物的运动、变化和发展。发展是前进的、上升的运动，发展的实质是新事物的产生和旧事物的灭亡。联系与发展的基本环节内容与形式、本质与现象、原因与结果、必然与偶然、现实与可能构成了联系和发展的基本环节。唯物辩证法的三大规律对立统一规律唯物辩证法的实质和核心对立统一规律是唯物辩证法的实质和核心，其原因如下：对立统一规律揭示了事物普遍联系的根本内容和变化发展的内在动力，从根本上回答了事物为什么会发展的问题。对立统一规律是贯穿量变质变规律、否定之否定规律以及唯物辩证法基本范畴的中心线索，也是理解这些规律和范畴的 “钥匙”。对立统一规律为人们提供了认识世界和改变造世界的根本方法 —— 矛盾分析析方法。对立统一规律又称矛盾规律，矛盾是辩证法的核心概念，是否承认矛盾，是否承认矛盾是事物发展的动力和源泉，是辩证法和形而上学的根本分歧。矛盾的两种基本属性是同一性和斗争性，矛盾的同一性和斗争性相互联结，相辅相成。内因和外因内因指事物发展变化的内部原因，是事物自身的矛盾。外因指事物之间的相互联系、相互影响，是事物变化的条件。二者的关系：内因是事物变化的依据，外因是事物变化的条件，外因必须通过内因起作用，内因与外因共同推动事物的发展。矛盾的普遍性和特殊性矛盾的普遍性指矛盾存在于一切事物中，存在于一切事物发展过程的始终，即” 矛盾无处不在，矛盾无时不有”。矛盾的特殊性指各个具体事物的矛盾、每一个矛盾的各个方面在发展的不同阶段上各有其特点。矛盾的特殊性决定了事物的不同性质。二者关系：矛盾的普遍性和特殊性是辩证统一的。矛盾的普遍性即矛盾的共性，矛盾的特殊性，即矛盾的个性。矛盾的共性是无条件的、绝对的。矛盾的个性是有条件的、相对的。任何现实存在的事物的矛盾都是共性和个性的有机统一，共性寓于个性之中，没有离开个性的共性，也没有离开共性的个性。主次矛盾和矛盾的主次方面主要矛盾：处于支配地位，对事物的发展起决定作用次要矛盾：处于从属地位，对事物的发展起次要作用主要矛盾和次要矛盾的联系：相互依赖，相互影响两者在一定条件下相互转化矛盾的主要方面：处于支配地位，起主导作用的方面矛盾的次要方面：处于被支配地位，不起主导作用的方面矛盾的主要方面和矛盾的次要方面的联系：相互依赖，相互影响两者在一定条件下相互转化方法论：要坚持 “两点论” 和 “重点论” 的统一，“两点论” 和 “重点论” 的统一要求我们看问题既要全面地看，又要看主流、大势、发展趋势。量变质变规律量、质、度量是事物的规模、程度、速度等可以用数量关系表示的规定性。质是一事物区别于其他事物的内在规定性。度是保持物质的稳定性的数量界限，即事物的限度、幅度和范围，这启示我们在认识和处理问题时，要掌握适度度原则。量变和质变量变和质变的区别如下表所示：区别量变质变性质事物数量的增减和组成要素次序的变动事物根本性质的变化，是事物由一种质态向另一种质态的飞跃特点渐进的、不显著的变化根本的、显著的变化呈现状态统一、相持、平衡和静止统一物的分解、平衡和静止的破坏结果事物还是其自身，没有变成另一事物事物不再是其自身，而变成了另一事物量变和质变的联系，辩证关系：量变是质变的必要准备；质变是量变的必然结果；量变和质变是相互渗透的；量变和质变是相互依存，相互贯通的，量变引起质变，在新质的基础上，事物又开始新的量变，如此交替循环，构成了事物的发展过程。否定之否定规律肯定因素和否定因素肯定因素指维持现存事物存在的因素。否定因素指促使事物灭亡的因素。辩证否定观辩证否定观的具体内容如下：否定是事物的自我否定，是事物内部矛盾运动的结果。否定是事物发展的环节，是旧事物向新事物的转变，是旧质到新质的飞跃。- 否定是新旧事物联系的环节，新事物孕育产生于旧事物，新旧事物是通过否定环节联系起来的。辩证否定的实质是 “扬弃”，即新事物对旧事物既批判又继承，既克服其消极因素又保留其积极因素。否定之否定事物的辩证发展过程经过 “肯定 —— 否定 —— 否定之否定” 三个阶段。否定之否定规律揭示了事物发展的前进性与曲折性的统一。这表明，事物的发展不是直线式前进，而是螺旋式上升的。按照否定之否定规律办事，要求我们树立辩证的否定观，正确看待事物发展的过程，既要看到道路的曲折，又要看到前途的光明。 唯物辩证法https://blog.mhuig.top/p/6ace289f/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"辩证唯物论","path":"/RSSBOX/rss/171a81d0.html","content":"辩证唯物论是关于世界的的物质性学说、关于物质和意识的辩证关系学说，它采用辩证法的观点研究世界的本质，所要说明的是世界的本质 “是什么” 的问题。辩证唯物论的基本观点是：世界的本原是物质，主张物质决定意识，意识是对物质的反映；同时，意识对物质有能动的反作用，承认世界是物质的，物质具有客观实在性。物质物质的定义物质是不依赖于人类的意识而存在，并能为人类的意识所反应的客观存在。物质的唯一特性是客观实在性。物质和运动物质和运动是不可分割的，二者关系及其具体内容如下表所示：关系理解误区物质是运动的物质，运动是物质固有的根本属性任何具体的物质形态只有在运动中才能保持自己的存在，世界上不存在脱离运动的物质离开运动谈物质会导致形而上学运动是物质的运动，物质是运动的承担者任何运动都有自己的承担者或者载体，离开物质载体的运动是不存在的离开物质谈运动导致唯心主义运动和静止物质的运动是绝对的，而物质在运动过程中又有某种相对的静止。运动和静止的区别如下表所示：运动静止含义宇宙间一切事物、现象的变化和过程两种情形：一是指空间的相对位置暂时不变，二是指事物的根本性质暂时不变性质无条件的、永恒的和绝对的有条件的、暂时的和相对的运动和静止的联系：静止是一种不显著的运动，是运动的特殊状态；动中有静，静中有动，世界上一切事物的存在和发展，都是绝对运动和相对静止的统一。只承认静止而否认运动是形而上学的不变论，只承认绝对运动而否认相对静止则导致相对主义和诡辩论。时间和空间时间和空间是物质运动的存在形式。时间是指物质运动的持续性、顺序性，特点是一维性，即时间的流逝一去不复返。空间是指物质运动空间的广延性、伸张性，特点是三维性，即空间具有长、宽、高三方面的规定性。物质运动总是在一定的时间和空间中进行的，没有离开物质运动的 “纯粹” 时间和空间，也没有离开时间和空间的物质运动。意识意识的含义意识是物质世界长期发展的产物，是人脑的机能和属性，是客观世界的主观映像。意识是社会的人所特有的精神活动及其成果的总和。从内容上看，人的意识是知、情、意三者的统一。意识的本质意识是人脑这种特殊物质器官的机能。意识是对客观存在的反映，其在内容上是客观的，在形式上是主观的。物质和意识的辩证关系物质和意识的辩证关系体现在：物质决定意识，意识对物质具有反作用（意识的能动作用）。意识的能动作用主要表现在：意识活动具有目的性和计划性；意识活动具有创造性；意识具有指导实践改造客观世界的作用（最重要的表现）；意识具有调控人的行为和生理活动的作用。正确认识和把握物质和意识的辩证关系，还需要处理好主观能动性和客观规律性的关系：尊重客观规律是正确发挥主观能动性的前提；只有充分发挥主观能动性，才能正确认识和利用客观规律。 辩证唯物论https://blog.mhuig.top/p/171a81d0/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"我们从哪里来？","path":"/RSSBOX/rss/dad4292a.html","content":"这里探讨的是一个非常简单的问题，我们是怎么来到这里的？前言世界上的所有人都是由这些元素和化合物组成的，他们的平常甚至让我们高智能的人类感到尴尬。事实上，人体的 99% 都是由空气、水、碳以及白垩组成，另外还能找到少量的比较特别的元素，如铁、锌、磷和硫。实际上，组成人体的所有物质加起来最多花几十块钱就能买到。但是，也不知道怎么的，这些数以万计的普普通通的原子能够携手起来，组成一个能够思考呼吸的活生生的人。这些简单的积木是如何组合在一起。这无疑是最令人着迷的问题。你可能会认为仅仅通过科学是不能够回答这个神奇的问题的。但是你敢肯定吗？现在，我们有理由相信，科学已经在这个大胆解释这个问题方面超越了宗教和哲学的解释力。下面我们将通过一系列相互盘根错节的伟大发现来解释自然界的不为人知的一面。在这之中蕴含了世上最基本的法则，即不确定性的产生。这是关于看似廖无生气、毫无目的和动机的物质世界，是如何自发的产生这些极其精细的绚丽的自然。这是关于世间最基本的法则是如何使这个世界展现出混沌和不可预测性，如何通过简单的物质创造出人。这是关于一个奇异的发现，即有序和混沌之间，奇特而难以置信的联系。图灵的方程式自然界充满了生长、发展和混乱，其中到处都是离奇的形状和杂乱的斑点。自然界的图案从来都不会固定不变，从来都不会按原样重复。这一切看上去混乱的现象都受到数学方程式的影响。事实上，他们完全被数学规则所支配。这种数学规则与我们长久以来的直觉相悖。因此不难相信第一个能够担负此揭示自然界的数学根基重任的人会拥有超乎寻常的智慧。他即是一个伟大的科学家，同时也是一个大悲剧。他 1912 年生于伦敦，他的名字就是阿兰・图灵。阿兰・图灵是一个不凡之人，他是有史以来最伟大的数学家之一。他提出了许多具有基础性的理论和见解。他的思想为现代计算机的出现提供了理论基础。在二战期间，他在布莱切利园工作，也就是今天的米尔顿凯恩斯外。当时政府正在这里进行一个叫做 X 站的秘密项目。他的建立是为了破解德军的情报密码。X 站项目组的密码破解人员做出了卓越有成效的工作，其中图灵的贡献至关重要。他亲自参与了破解德国海军密码的工作，因为他的工作数以万计的盟军得以幸存，同时这也导致战略形势的有利扭转。但是图灵的天赋不仅仅表现在破解密码上，这仅仅是他那超乎常人的洞察力的一部分。对于图灵来说，自然界的密码才是终极密码。在他的一生中，他热切的寻找着破解这个密码的方法。图灵是一个很有独创精神的人，他意识到这样一种可能性，简单的数学方程式可以描述复杂的生物世界的一些现象。他的这种想法以前从来没有人尝试过，自然界中所有的迷中最吸引图灵的，就是如何能够使用数学方程式来描述人类的智能。图灵迷上这个想法是有原因的，这就是年轻的克里斯托弗・马尔孔的死。图灵是同性恋，克里斯托弗・马尔孔的死，在那时以及他的一生产生了重要的影响。克里斯托弗・马尔孔突然的死亡了导致阿兰图灵在情感方面产生了极大的触动。但是你能想象，他想把这个事实用科学来解释，他想解释的问题是，我们的心智怎么了，什么是心智。图灵相信生物界的复杂的系统是可以用数学方程式来描述的，并且人类的智能也是如此。他的这种信念导致了现代计算机的产生。之后一个更加激进的想法出现在图灵的脑中，这个想法就是通过简单的数学描述来解释胚胎中发生的复杂的过程。这个过程被叫做形态发生。它非常令人费解，起初胚胎中的所有细胞都是相同的，细胞们开始组合到一起，并且细胞之间渐渐产生了差异，这是如何发生的呢？物质不会自己思考，也没有一个中央系统在里面进行调度，一开始都是一样的细胞，为什么有的能够变成皮肤，而有的却变成了眼睛呢？形态发生是一类现象中的一个例子，这类现象叫做自组织现象。在图灵之前没有人懂得自组织现象的机制。直到 1952 年 图灵发表了他的这一篇论文，阐述了用数学方程式来解释形态发生现象。论文中的大胆的猜测是令人震惊的，其中图灵使用了一个在天文学和原子物理学中很常见的一种数学方程式，来描述生命过程。之前从来没有人做过这种尝试，然而图灵的方程式却第一次的做到了，描述了一个生物系统的自我组织的过程，这解释了即使简单的、毫无自然界事物特性的东西，也可以演变出栩栩如生的东西。图灵的成果中令人大吃一惊的地方是，我们可以通过设定非常简单，甚至简单到可以仅仅通过简单的方程式就能描述的初始状态，然后让它进行演变。然后突然间，复杂和混乱就会出现，产生的复杂图案就像是自然界的结果。从许多方面来看这些都是难以置信的。其实图灵的方程式描述的都是我们很熟知的东西，但是从来没有人将这些数学方程式应用到生物学领域。试想一阵风吹过沙丘，进而产生了一系列图形。小颗粒自我组织成波纹浪花和沙丘，即使那些小颗粒是彼此相同的。并且没有人告诉他们到底要怎么去组成属于他们的那一部分。图灵认为，以一种非常形似的方式，在胚胎中渗过的化学物质可能会引导细胞进行自我组织，进而产生各种不同的形状。这是图灵给出的非常粗略的解释，他阐述了一堆毫无生气的化学物质，如何演化为各种不同的形状。在他的论文中做了一些改进，来使他的方程式能够自发的产生生物图案，那种与动物表皮相似的图案。图灵到处向别人展示自己生成的图形，你看这难道不像母牛身上的图案吗，其他人的反应是，这个人的脑子有问题吧。但是图灵相信自己在做一件有意义的事情，他们的确像母牛身上的花纹，这就解释了母牛拥有这种斑点花纹的现象。这样一个数学从未触及的领域，生物界的图案，生成动物斑纹，突然间，这个领域向我们敞开了门。我们发现数学方程式在这个领域会有用武之地，即使图灵提出的方程式，并不是这一新理论的全部，但仍然是一次重要的创举，提出了这种新方法的可能性。我们现在知道形态发生，要比图灵所描述的数学方程式复杂多了。事实上，关于 DNA 分子确切的运转机制，仍然是现代科学上争论的话题。但是图灵提出的数学支配万物的观点确是革命性的，阿兰・图灵的论文是整个形态发生理论的奠基石，他为我们提供了一种解释，连达尔文都没能提出的，解释自然界生物花纹的产生机制。达尔文仅仅告诉我们，生物的花纹是来源于基因的，并且这种花纹的继承是决定于环境的，但是达尔文并没有揭示，这种生物花纹到底是如何生长的，而这是真正的迷。图灵的贡献在于提供了了解这种化学机制的途径。图灵提出了一种伟大而勇敢的见解。不幸的是我们仅仅能够推测这颗伟大的大脑，是如何想出这些的，在他发表他的开创性论文之后不久，一个可怕的并且完全可以避免的悲剧摧毁了他的生命。在他在布莱切利园破解密码工作之后，你一定可以想到图灵会得到很多赞誉，以感谢他为国家做出的贡献。这再明显不过了。战后发生在他身上的事情是一个悲剧，这是英国科学史上令人感到羞耻的事情。同年，图灵发表了关于形态发生的论文，他与阿诺德默里这个男人发生了短暂的情感，但是这段情感发展的很令人不愉快，默里对图灵进行了一次入室盗窃，但图灵将这件事情报告给警察的时候，警察连同图灵一起逮捕了。法庭上，原告声称，图灵以他的学历来诱导默里走上了歧途。图灵被判有严重的猥亵罪，法官给了图灵可怕的选择，他要么进监狱，要么接受雌性荷尔蒙注射，进而治疗他的同性恋倾向，他选择了后者。这导致了他连续不断的消沉。1954 年 6 月 8 日，图灵的尸体被他的清洁工发现。他是一天前死于咬了一口自己注入了氰化物的苹果，结束了自己的生命。阿兰图灵死的时候 41 岁，这对于科学来说是一种无法估量的损失。图灵不曾想到他的思想会启发后人，将一种全新的数学方法应用到生物学领域。科学家发现他发现的这种方程式，确实能够解释好多生物组织的形态。回头看看，我们知道图灵真正捕捉到了复杂与混乱源于简单规则，这样的法则。他意外的迈出了，通往新科学的第一步。贝洛索夫的试液这个过程的第二步更是始料未及，可悲的是伟大的第二步也是一个悲剧。在 20 世纪 50 年代初，在图灵发表他的对后人影响巨大的论文之际，一名杰出的俄国化学家 斯・贝洛索夫，开始了他自己的探索，关于自然界中的化学。正在高墙铁网之后的苏维埃卫生部，他正在研究我们的身体是如何从糖中提取能量的。像图灵一样贝洛索夫也是在一个个人项目中工作，他刚刚完成了一段从事科学工作的经历。贝洛索夫构想好了一种新的化合物配方，来模仿人体内葡萄糖的吸收，这种混合物就摆在实验室的座位前面，在被摇晃的时候清澈而透明。在他添加最后一种试剂的时候，整个混合物的颜色发生了变化。当然，这还不算什么特别的，就像我们将墨汁倒入水中水也会改变颜色。但是接下来发生的事情令人感到惊奇，混合物又变得清澈无色了。贝洛索夫感到很吃惊，化学物质混合以后会发生反应，但这个过程不能自发的发生逆反应，在不受外界干预的情况下发生可逆变化。你可以很容易的将一个无色的液体变成有色的，但这个过程的逆过程却不太可能。这太奇怪了，贝洛索夫的试液并没有简单的发生逆变化，试液被摇晃后，就不断的在无色和有色之间变化，就好像这个试液在受一种神秘的内部机制驱动一样。他非常谨慎小心地又重复多次这个实验，他的混合物能够不断的在无色和有色之间变化，他发现的东西好像魔术一样。一个好像是违反自然法则的物理现象。贝洛索夫觉得自己的发现意义重大，将自己的发现记录下来，并努力让更多的人了解他的发现，但是当他将他的发现递交到一个顶尖的苏联科学杂志的时候，他收到了一个完全出乎意料的诅咒式的回复，杂志的编辑告诉贝洛索夫，他的发现是不可能在实验室重现的，因为这与基本的物理法则相违背，对这个现象的唯一的解释就是，贝洛索夫在实验的时候出了错，他的发现是不会被出版的。这个拒绝深深的打击了贝洛索夫，他感到非常的羞辱，结果放弃了自己的实验，而后他又放弃了自己从事的事业。具有悲剧色彩的讽刺是，由于贝洛索夫所处的闭塞环境，贝洛索夫从来没有机会看到图灵的工作成果。如果他看到了图灵的发现，就能为自己的发现提供有力的证据，事实上，贝洛索夫的不稳定试液，不仅没有违背物理法则，而且是实在就是真实的，能够体现图灵的预言的例子，尽管这两者的发现初看起来并没有什么联系。其他科学家发现，如果将贝洛索夫的试液的一种变种，不加搅拌的放入培养皿中，而不是摇晃他们的话，他们就会发生自我组织，事实上 ，他们产生的条纹会比图灵预测的花纹要复杂。他们会产生令人惊诧的复杂的条纹，毫无预兆。贝洛索夫实验的惊奇之处在于，它能生成一种系统，这种系统产生了图灵方程式所预测的图案，在一个看上去无色的溶液中，产生了这种奇异的原型图案，这明显不是什么抽象科学，贝洛索夫图案的运动模式 ，与我们心脏在跳动的时候周围细胞的运动模式完全相同。动物的皮毛和心脏的跳动，自组织现象在自然界中随处可见。牛顿经典物理学为什么在科学界在图灵和贝洛索夫的年代 ，却对这种想法不感兴趣甚至持有敌意呢？原因就是人类的臭毛病，主流科学家不喜欢这种观点，这与主流科学家的科学直觉相违背，也与现有的科学成就相违背，若想改变这种观念，我们需要一种彻底的，改变传统的发现。实际上，在 20 世纪初期，科学家们把宇宙看作一个巨大而复杂的机械装置，有点像一个大号的太阳系仪，整个宇宙就好像是一个巨大的错综复杂的机器 ，严格的遵守着数学规则。如果你知道这个机器的运转机制和初始状态，那么随着你转动这个手柄，它将严格地按照预期的行为运转。在牛顿生活的时代里，当人们在探索驱使宇宙运转的法则时，他们把宇宙看作这种按照确定规则运转的机器。宇宙就好比是一个被设定好的机器，遵循确定的法则按部就班的按照这个法则运转。宇宙中复杂的现象是有复杂的内部规则驱使的，但是一旦一开始让它运转它只会做一件事。人们从这里看到的现象，都可以使用严格的数学公式来描述。这实际上是很简单的事情，一旦找到能够描述系统运行的数学方程式，那么你就能够预测系统的走向。这是一个伟大的想法。它开始于牛顿的万有引力。万有引力成功的解释了行星围绕太阳运转的现象。科学家们后来又不断的发现了新的方程，牛顿物理似乎已成为了预言宇宙的终极方法。它给我们暗示了这样一种可能，从原则上讲，未来是可以预测的。我们采用的测量手段越精确，我们就能越精确的预测未来的情形。但是牛顿物理产生了一个可怕的后果，如果有一个系统我们能够用数学方程式精确描述，就像这个太阳仪一样，那么一旦它表现出了一些我们不能预测的行为之后，科学家就只能认为，是有某种外界力量影响了这个系统，比如说是沙土跑进去了，或者是小零件磨损了，或者有人对它进行了认为的改动。一般情况下，我们会这样假设，如果我们遇到了非预期的现象，那么这种现象应该是来自系统外部的干扰，而不是来自系统内部的。不可预测的现象，不是来自系统的本身，而是来自与外部对它的影响。从这种观点的角度来看，自我组织这种现象是很荒谬的，而图灵和贝洛索夫所表达的，复杂图案可以从系统中自我生成，而不需要外界力量是非常受当时的主流科学所忌讳的。要想使人接受自我组织理论，那么，就必须推翻牛顿物理学，但这看上去很不可能，无论如何这种新思想在 60 年代末期，成为了新时代的奇景。蝴蝶效应然而同时，伴随着登月计划，一小群信奉牛顿力学的科学家，意外的发现了一些事情不对劲，完全不对劲。在 20 世纪后半叶，科学界的噩梦出现了，这个噩梦，动摇了牛顿的思想，并将我们推向了一片思想的混乱。具有讽刺意味的是，迫使科学界接受自我组织理论的事情，是一种我们叫做混沌的现象。混沌这个词被广泛使用，但是在科学领域中，它有它专指的意义。它指的是在一个能被数学方程式精确描述的系统中，可以自发生成不可预测的现象，并且不需要任何外界的干预。通过使用非常简单的法则或方程式，并且里面不包含任何的随机性，系统中的所有元素都是确定的，并且我们完全掌握系统的法则，即使是这样的系统也会产生完全不可预测的现象。混沌的发现并不讨主流科学的喜欢，有一个人迫使科学界接受混沌，他就是美国的气象学家爱德华・洛伦兹。在 20 世纪 60 年代早期，他试图寻找能够预测天气变化的数学模型。就像许多他的同事一样，他相信天气系统与我们太阳系仪是一样具有确定性的，一个可以被数学描述和预测的物理系统。但是他错了，当洛伦兹写下一个用于描述气流的及其简单的数学方程式时，这些方程式并没有达到他预想的目的，他们没有做出任何有价值的预测，这就好像说某一天中的一阵清风，将会决定一个月后的某天是冰雪连天，还是清空万里。对于一个像太阳系仪这样精准的系统来说，怎么会产生不可预测性呢？这源于他的内部构造，由于齿轮的链接方式。事实上，在某种情况下，即使在初始的时候有一点点误差，哪怕这个误差小到难以测量，这个误差会随着机械的运转而不断被放大，随着系统的运转，系统的状态会一点一点的偏离你所期望的状态。洛伦兹在一次演讲中表达了这一颠覆性的想法，演讲的题目是 —— 一只蝴蝶在巴西扇动翅膀会使美国的德克萨斯刮一场龙卷风吗？这是一次有力而吸引人的演讲，数月之内，我们的语言中就添加了一个新的词汇 —— 蝴蝶效应。蝴蝶效应就是混沌系统的标志，它开启了之后的一切。在 70 年代早期 一个叫罗伯特・梅的年轻澳大利亚人，正在研究一个数学方程式，用它来模拟生物种群随时间的变化。但是这个过程中同样牵扯到蝴蝶效应，哪怕生物的繁殖率发生了极小的变动，都会导致种群数量结果发生巨大的变化，这个数值可能会毫无征兆的上下起伏。传统的使用数学方程式，来描述系统行为的方法似乎走到了死胡同，某种意义上，这是信仰牛顿学说的人的美梦之终结。随着我们的计算能力的提高，我们就有能力处理更复杂的方程组，但刚才我们所看到的否认了这种观念。你可以从一个及其简单的方程式开始，这个方程式简单而不存在任何的随机性，但是如果它产生的行为能够表现出一种混沌性，那么你就不能再回溯到系统的初始状态了。数百年来所建立的科学观点在几年内就被瓦解了，可以精确描述宇宙的运行的这一想法变成了幻影。看上去具有逻辑确定的事情，却变得更像是一种信仰。更糟的是，这种现像到处都是。因为混沌到处都是。似乎不可预测性是固有的，存在于我们生活的宇宙中。全球气候可能会在几年的时间内，发生剧烈的变化；股市可能会毫无征兆的崩盘；我们可能会在一夜之间从地球上灭绝。如果这种事情发生的话 没有人能够阻止，不幸的是，我不得不说以上这些都是真的。但是盲目的恐惧混沌现象是毫无意义的。因为混沌是一条基本的物理法则，我们必须承认它是生活中的一部分。混沌理论的出现一直影响到之后 20 到 30 年人们的思想，它改变了人们对于科学工作的看法，它深刻的改变了科学家看问题的方式，以至于科学家们现在已经离不开混沌理论了。混沌理论想要说明的是，简单的数学方程式能够繁衍出复杂的行为，这种复杂性超出你我的想象，所以简单而机械的系统能够表现出复杂和丰富的行为。混沌理论的发现，是科学史上的一次重大的转折点，它摧毁了牛顿信仰者的梦想，科学家们现在越来越看得惯图灵和贝洛索夫在自发生成花纹上所做的工作。自反馈系统更重要的是，由于他们的工作，一个伟大的真相浮出水面，那是一种内在而隐蔽的关联，一个贯穿宇宙的关联，关联着自然的神秘力量和自我组织现象，以及蝴蝶效应产生的混沌结果。图灵、贝洛索夫、梅、洛伦兹这些人都分别发现了一种重大思想的不同侧面。他们发现自然界具有固有的不可预测性，这种不可预测性的内部驱动力，也可以使系统表现出特定的结构和花纹，有序与混沌，似乎要比我们想象的要联系的更加紧密，但这种联系是如何实现的呢？贝洛索夫的花纹与天气变化之间有什么联系呢？首先，虽然两个系统都有复杂的工作机制，但他们都是基于及其简单的是数学法则，其次，这些数学法则都具有一种独特的特性，都具有自我链接的特性或者说是自反馈。为了向你展示这一点，展示简单的自反馈系统的力量，将使用一种看上去简单甚至无聊的实验。身后的屏幕连接到一台摄像机，这台摄像机同时也在拍摄着我和屏幕，这样不断的循环就能产生无数个人的影像，都投影到屏幕上，这是一个典型的自反馈系统。图片中不断的嵌套着其他的图片。这乍看上去肯定有规则，但当我们放大镜头，奇怪的事情发生了，我首先发现的是实物图像和屏幕上显示的图像不像了，火柴的微小运动被迅速放大，当影像在摄像机和屏幕之间反复映射的时候，即使我能用精确的数学对外的每一步动作进行描述，但我却不能预测火焰微小的变化，会导致最终的图像如何变化，这就是一种实实在在的蝴蝶效应。下面的现象变得更离奇了，通过像系统中添加一点点的扰动，这些奇异而美丽的图案就出现了，这种简单的依赖反馈的系统，呈现出了混沌与有序，同样的数学方程式同时产生了混沌和有序的图案。这将改变你对世界的看法，在传统观念中，自然界都是有序的，混乱存在与系统之外，即有序与混乱是相互独立的这种想法是错的，其实混乱和有序，就像同一架钢琴上弹出的高音和低音，这个发现有史以来第一次如此接近自然界的数学本质。我们能从图灵的工作，以及化学和生物中得到的，最重要的启示是，所有复杂的图形都是来自，宇宙间简单的演变过程，像扩散， 像化学反应率，这些简单的过程最终导致了图案的产生，所以到处能看到图案，他们不断的产生。曼德勃罗集合从 70 年代开始越来越多的科学家，开始接受混沌理论，以及对自然界能够自发产生复杂花纹的认可，但是有一位科学家比别人走的更远，对这个令人惊奇颇感迷惑的问题带了个新思路，他是一个具有传奇色彩的不喜欢按套路出牌的人，他叫伯努瓦・曼德勃罗。伯努瓦・曼德勃罗 并不是一个寻常的孩子，他跳了两个年级。并且由于他是战时欧洲的犹太人，所以他受到的正规教育极其有限，他基本上是靠自学以及亲属对他的教育，他从来没有正式的学习过字母，甚至没有学过 5 以上的乘法。但是和图灵一样，曼德勃罗具有一种洞悉事物本质的本领，他能从混乱中看到我们所看不到的规律，他能够发现形式和结构，然而我们却仅仅能看到一片混乱，他能够感知一种新奇的数学，用来支配整个自然界的运转。曼德勃罗的一生都致力于找到一种能够揭示自然界复杂性的一种数学支持。曼德勃罗当时为 IBM 工作，而不是学校的学术圈内，他试图解决一大堆的问题，关于自然界以及金融界的不确定性，在各个方面的表现。我觉得他知道自己，所做的所有工作都是一个大问题的不同侧面。他是一个具有原创精神的人，他觉得求解这个大问题是他真正想做的事情。在曼德勃罗看来，几百年来传统的数学研究都仅限于，规则的图形是一种很不可取的行为，就像直线和圆，传统的数学是没有办法描述不规则的形状的，而真实世界确是有不规则形状组成的，就像这个鹅卵石，它是一个球体还是一个立方体呢，还是它们中间的某种形状？他到底是一种什么形状？曼德勃罗想，是否有一种法则，能够描述自然界的不规则性，那些蓬松的云朵、树和河流的分支，以及蜿蜒的海岸线之间有什么共同的数学基础。是的，有的。自然界中所有的形状的共同特点，就是自相似性。这指的是事物的局部，不断的在更微小的尺度上重复自己，不断的精细到每一个细微之处，树枝就是一个绝好的例子，他们不断的分叉，重复这这个简单的过程，在更微小的尺度上不断重复这这个过程。我们的肺的结构同样遵循这个原则构建，我们体内血管的分布同样遵循这样的规则，河流分成更小的溪流也是这样，自然界可以依照这种方法不断重复各种形状。看看这个罗马花椰菜，他的总体结构是由许多重复的小圆锥组成的，曼德勃罗意识到自我重复性，是一种全新的几何学的基础，并给这种几何图形起了一个名字 —— 分形。分形看上去非常简单直观，但是我们如何才能用数学对它进行描述，你能够利用分形的本质画一幅相似的图形吗？那么这张图形会像什么？你能仅使用一些简单的数学法则，来绘制一幅看上去像是自然创造的图案吗？曼德勃罗找到了答案。曼德勃罗爱 20 世纪 50 年代末就职与 IBM，因此有机会使用计算机，并且利用计算机这个工具来寻找自然界的数学本质。在新一代的超级计算机的帮助下，他开始研究一个看上去很奇怪，但却异常简单的数学方程式，使用这个数学方程式可以绘制一幅不同寻常的图形，我将向你展示的是一幅非常吸引人的图画。它完全由数学产生，惊人之举往往不同寻常。这就是曼德勃罗集合，他被称为上帝的指纹，当我们仔细看看这幅图后，你就知道我们为什么这么叫了。&gt;&gt; 鼠标选择区域放大查看曼德勃罗集合细节 &lt;&lt;就像树和花椰菜一样，你看得越仔细你发现的细节就越多，在这个集合中的每一个图形，都包含了无穷多个小图形，小的子图无穷无尽，然而所有这些复杂的分支都来自有一个简单的方程，这个方程有一个非常重要的特性，它是自反馈的。每一次输出都成为了下一次计算的输入，这种反馈系统展示了一个简单的，方程式是如何展现出无比绚丽的图案的。但是令人惊奇的是，曼德勃罗集合并不是一个奇怪的数学巧合，它的这种无穷分形的特性，反映了一种自然的有序的本性，图灵图案、 贝洛索夫的化学反应、曼德勃罗的分形都分别指向了一个自然本质。简单的法则 无穷的创造当我们看到自然界的复杂面貌时，我们倾向于问，他们来自哪里，我们总是抱有这样的观念。简单的事物不能导致复杂性的产生，复杂的现象必须源于复杂的设计。但是我们刚才看到的数学方程式告诉我们，极其简单的法则也会繁衍出复杂的现象。当你看到复杂现象的时候，你应该想到，驱使它产生的只不过是简单的法则，所以同一个方程式从不同的角度看，既简单又复杂，这就意味着我们需要重新思考，简单性于复杂性之间的关系。复杂的系统可以基于简单的法则。这是一个重大的启示，也是一种伟大的思想。它似乎适用于整个世界。看看这群飞鸟，每一只鸟都遵循简单的法则，但是整个鸟群确是一个极其复杂的东西，他会自动的避开障碍在没有领航情况下进行自我导航，甚至是做计划。虽然这个鸟群的行为很显著，但我们却不能预测他的行为，它不会重复原先的行为，即使是在相同的环境下。就像贝洛索夫的化学反应一样，每次你进行这个化学反应产生的图案都稍有不同，他们可能看上去相似但却不可能相同。对于自反馈的影响和 沙丘同样是这样，我们知道它会产生某种图像，但是我们却不能预测确切的形状。生物进化的原料我们关心的问题是，大自然能够以这种方式将简单的法则，演绎出复杂的现象吗？试着解释一下为什么会有生命的存在，它能够解释为什么充满砂石的世界，是怎么产生人类的吗？毫无生机的事物是如何变得充满智能的呢？进化正是基于这些简单的生物花纹，进化将这些当成原料，并把它们以不同的方式结合在一起，看看哪些结合方式有效，哪些无效，保存那些有效的结合，并在它们的基础上进一步演化。这是一个完全没有意识控制的变换过程，但这基本上就是现实中发生的事情，放眼望去，进化的过程到处都是使用自然界的自我组织的图案。我们的心脏使用类似贝洛索夫反应的方式来驱使它有节律跳动，我们的血管的组织形式就像分形，就连我们的脑细胞也是遵循极其简单的规则，进化过程丰富并筛选这我们的世界的复杂性。这是近代科学最有魅力的发现。计算机模拟的进化机制一方面你拥有一个具有自组织能力的复杂系统，它可以产生不可预测的行为，另一方面需要将进化机制作用于它，这样才能产生适应环境的东西，进化通过无拘无束的创造力，约束着系统的演确实难以置信。但当这个约束过程发生在宇宙级的时间尺度上，就容易理解了，从地球上第一次出现生物到我们人类的产生，整整花了 35 亿年的时间，但是我们现在手头上，有一种可以在更短的尺度上模拟这个过程的发明。你知道我指的是什么吗？很可能你天天就坐在这个发明面前，当然，这就是计算机。现代的计算机每秒可以进行上亿次计算，我们可以让它们做一些奇特的事情，它们可以模拟进化过程，确切的说，计算机可以利用进化规则，来约束自己产生真实世界中的现象，使用进化规则来约束和筛选生物组织。现在，计算机科学家发现这种能够自我演变的软件，可以取代人类最聪明的头脑来解决问题。我们在最初的实验研究中发现，进化这种系统就像一种算法一样创造着，能够适应环境的复杂系统。托斯顿和他的研究小组的研究目标是，使用计算机模拟的进化机制，来创造能够控制躯体运动的虚拟大脑。一开始他们随即设置了 100 个虚拟大脑，就像你看到的这样这些大脑很笨，然后进化的力量来了，计算机自动的选择表现稍好的大脑，然后让它们产生后代，然后再选择能够做的更好的大脑，并让它们继续产生后代，下一代中能够更好的控制躯体行动的大脑，会继续得到繁殖后代的机会。令人惊奇的是，通过 10 代的繁衍，虽然还是有一些不稳定，但这些小人确实能够行走了。更神奇的事情是，你最终得到了一种能够正确行走的东西，但是令人感到有点害怕得到是，你却不知该它为什么能走，以及是如何行走的。你眼睁睁看着这个大脑，却不知道它内部是如何工作的，因为这个结果是进化自动产生的结果，经过 20 代的繁衍后，我们看到了这个，变成了这个，这些虚拟生物随后演化出了比行走更复杂的行为，它们产生的行为，是很难通过传统的编程方式实现的，它们对于突发事件做出了像人类一样的反应。即使这些算法都是由我们人类编写的，但当它们一旦开始进化之后，我们就难以加以控制了，然后我们预想不到的事情就发生了。真有一种滑稽的感觉，你创造了它们，然后它们抛开你自己做主，一种不假思索的不断尝试的进化过程，创造了这些能够行动并作出反应的虚拟生物。我们这里看到的是一个绝妙的实验证据，来证明简单的法则具有无穷的创造力，看着计算机里面自动表演着难以用写程序的方式描述的行为，它是一个展现自我组织能力的绝好例子。这说明了进化本身，就像我们看到的其他系统一样，是一个基于简单法则和回馈的系统，在这个系统中复杂性自发的产生了。想想看，这个简单的法则就是，机体需要重复略有变化的行为，反馈来源于环境，这种环境选择了更适应它的行为得到生存，结果就是，在没有可以设计和规划的情况下，前所未有的复杂性就这样产生了。有意思的事情是，一个个体可以进化到更高的一个结构状态，一旦你获得了一种包含某种行为的系统，并且这些行为可以被选择，被某种过程选择或被环境的反馈选择。所以进化过程这个达尔文学术的中心议题，在某种意义上就是图灵的反馈系统运行在多个过程之上。尾声这就是整个事情的本质，未经过精心设计的极其简单的法则，能够无意识的创造出无比复杂的系统。这样看来，这些计算机虚拟的生物就是自组织系统，就像贝洛索夫在他的化学实验中发现的现象一样，就像沙丘和曼德勃罗集合所呈现出的现象一样，就像我们的肺、心脏以及我们这个星球上的天气系统。伟大的设计并不需要一个伟大的设计者，这就是宇宙所固有的本性。令人难以接受这种观点的是，所有形状、花纹和结构的产生，并不需要一个有意识的创造者，但这种设计本身可能需要一个更聪明的设计者，他做的事情就是将整个宇宙，作为一个巨大的仿真，在这里你设定了一些初始条件，然后一切的一切都自发的产生了。伴随着所有的惊奇，伴随着所有的美丽，图案形成的数学本质预示着同样的图案会在许许多多不同的场合出现。如化学生物系统，在这些系统的本质里，都存在同样的数学基础，在这些内部本质的外面，就是我们看到的今天的这个世界。我想，这是一个令人兴奋的想法，那么我们最终能从这些当中学到什么呢？这就是宇宙间所有的复杂性，所有的多样性，都源于一些简单而毫无目的的法则的不断繁衍的结果，但是请记住，尽管这个过程力量无比，但他却具有固有的不可预测性，即使我可以充满信心的告诉你未来精彩无限，但我仍要负责任的告诉你，未来将会发生什么确是不为人知的。 我们从哪里来？https://blog.mhuig.top/p/dad4292a/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"记一次仅在 IPv4 环境下访问 IPv6 网络的经历","path":"/RSSBOX/rss/ef7fd902.html","content":"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 您好, 这里需要密码. 记一次仅在 IPv4 环境下访问 IPv6 网络的经历https://blog.mhuig.top/p/5e9edb45/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"神秘数字 4.669","path":"/RSSBOX/rss/c8c741f1.html","content":"从无序迈向有序 神秘数字 4.669https://blog.mhuig.top/p/373468cd/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"大数据架构演变","path":"/RSSBOX/rss/7e3480e9.html","content":"在 Hadoop 系列框架还没出现之前，数据分析工作已经经历漫长的发展，其中以 BI 系统为主的数据分析，已经有非常成熟和稳定的技术解决方案和生态系统，BI 系统的架构图如下：BI 又叫商业智能，其包括与传统业务系统的区别在于：业务系统更注重于事务型的数据处理，用来支撑企业的各业务线；而 BI 是将企业中所有数据汇聚成数据仓库（DW）并对其进行分析型操作，其中 Cube 是 BI 的核心模块。Cube 是一额更高层的业务抽象模型，在 Cube 上可以进行上钻、下钻、切片等操作、BI 系统都是基于关系型数据库，关系型数据库使用 SQL 语句进行操作，但是 SQL 在多维操作相对较弱，所以 Cube 有自己独有的查询语言多维查询语言 —— MDX大多数的数据库服务厂商都提供 BI 服务，轻易便可搭建出一套 OLAP 分析系统OLTP 联机事务处理，表现为企业中的应用系统如 OA、CRM、ERP、财务软件等供各部门使用OLAP 联机分析处理，也叫决策支持系统 DSS，通常进行使用者是企业高管或部门管理者但是随着互联网发展，BI 系统也暴露除了一些缺点:BI 系统多以分析业务数据产生结构化数据为主，对于非结构化和半结构化数据处理乏力。例如图片、文本、音频的存储、分析。随着异构数据源增加，要解析数据内容进入数据仓库，则需要非常复杂的 ETL 程序，从而导致 ETL 变得过于庞大和容易出错，需要大量人力进行维护随着数据的增长，性能会成为瓶颈，在 TB/PB 级别的数据处理上表现的尤为乏力数据仓库的原始数据都是只读的用来分析，不存实务操作者导致传统的范式约束大大影响了性能由于 BI 的一系列问题，在以 Hadoop 生态圈的大数据分析平台逐渐表现出其优异性，围绕 Hadoop 体系的生态圈也不断变大，对于 Hadoop 系统来说，从根本上解决了传统数据仓库瓶颈的问题随着大数据平台的不断发展，现在主要对数据的处理时效进行区分为针对于 T+1 数据的离线处理架构，其主要应用框架由 Hadoop、Hive、Sqoop 等组成针对实时数据的流式处理架构，其主要由 Spark、Flink、Flume、Kafka 等组成Lambda 架构“我们正在从 IT 时代走向 DT 时代 (数据时代)。IT 和 DT 之间，不仅仅是技术的变革，更是思想意识的变革，IT 主要是为自我服务，用来更好地自我控制和管理，DT 则是激活生产力，让别人活得比你好” —— 阿里巴巴董事局主席马云。Hadoop 作为解决对大数据量低成本规模化的处理的解决方案被广泛应用但是 MapReduce 或者 Hive 很难做到低延迟，用 Storm 开发的实时流处理技术可以帮助解决延迟性的问题，但它并不完美Storm 不支持 exactly-once 语义，因此不能保证状态数据的正确性Storm 不支持基于事件时间的处理后来出现了一种混合分析的方法，它将上述两个方案结合起来，既保证低延迟，又保障正确性 ——LambdaLambda 架构是由 Storm 的作者 Nathan Marz 提出的一个实时大数据处理框架Marz 在 Twitter 工作期间开发了著名的实时大数据处理框架 Storm，Lambda 架构是其根据多年进行分布式大数据系统的经验总结提炼而成Lambda 的目标:高容错、低延时、可扩展Lambda 特性整合离线计算和实时计算读写分离和复杂性隔离可集成 Hadoop，Kafka，Storm，Spark，HBase 等Marz 认为大数据系统应具有以下的关键特性（Lambda 架构的关键特性）：Robust and fault-tolerant（容错性和鲁棒性）：让系统从错误中快速恢复Low latency reads and updates（低延时）：响应是低延时Scalable（横向扩容）：通过增加机器的个数来提高系统的性能General（通用性）：支持多领域的数据分析（金融、社交、电子商务等）Extensible（可扩展）：以最小的开发代价来增加新功能Allows ad hoc queries（方便查询）：即时查询，快速简便的进行查询Debuggable（易调试）：快速定位错误Lambda 架构通过分解的三层架构来解决问题Batch LayerSpeed LayerServing LayerBatch Layer理想状态下，任何数据查询都可以从表达式 Query= function (all data) 获得，但是若数据达到相当大的一个级别（例如 PB），且还需要支持实时查询时，就需要耗费非常庞大的资源可以将数据提前进行计算处理成为 Batch View，这样当需要执行查询时，可以从 Batch View 中读取结果。这样一个预先运算好的 View 是可以建立索引的，因而可以支持随机读取Batch Layer 总结为：Batch View = function(all data)Query = function(BatchView)Speed LayerBatch Layer 的离线处理可以很好的满足大多数应用场景，但有很多场景的数据是不断实时生成，并且需要实时查询处理。Speed Layer 正是用来处理增量的实时数据并生成 Realtime ViewSpeed Layer 处理的数据是最近的增量数据流，Batch Layer 处理的是全体数据集Speed Layer 为了效率，接收到新数据时不断更新 Realtime View，而 Batch Layer 根据全体离线数据集直接得到 Batch ViewSpeed Layer 是一种增量计算，所以延迟小Speed Layer 总结为：RealtimeView＝function(RealtimeView，new data)Batch Layer 和 Speed Layer 优点：容错性：Speed Layer 中处理的数据也不断写入 Batch Layer，当 Batch Layer 中重新计算的数据集包含 Speed Layer 处理的数据集后，当前的 Realtime View 就可以丢弃，这也就意味着 Speed Layer 处理中引入的错误，在 Batch Layer 重新计算时都可以得到修正。这点也可以看成是 CAP 理论中的最终一致性（Eventual Consistency）的体现复杂性隔离：Batch Layer 处理的是离线数据，可以很好的掌控。Speed Layer 采用增量算法处理实时数据，复杂性比 Batch Layer 要高很多。通过分开 Batch Layer 和 Speed Layer，把复杂性隔离到 Speed Layer，可以很好的提高整个系统的鲁棒性和可靠性Query = function( Batch View , Realtime View )Realtime View = function( Realtime View , new data )Batch View = function( all data )Serving Layer用于响应用户的查询请求，合并 Batch View 和 Realtime View 中的结果数据集到最终的数据集Kappa 架构Lambda 架构有时会出现批量数据和实时数据结果对不上的问题LinkedIn 的 Jay Kreps 提出了一个新的架构：KAPPA它的理念是：鉴于大家认为批量数据和实时数据对不上是个问题，它直接去掉了批量数据；而直接通过队列（Kafka），放入实时数据之中。例如：将所有的数据直接放到原来的 Kafka 中，然后通过 Kafka 的 Streaming，去直接面向查询该架构也存在着一些问题：不能及时查询和训练。例如：我们的分析师想通过一条 SQL 语句，来查询前五秒的状态数据。这对于 KAPPA 架构是很难去实现的面对各种需求，它同样也逃不过每次需要重新做一次 Data Streaming。也就是说，它无法实现 Ad—hoc 查询，我们必需针对某个需求事先准备好，才能进行数据分析新数据源的结构问题。例如：要新增一台智能硬件设备，我们就要重新开发一遍它对应的适配格式、负责采集的 SDK、以及 SDK 的接收端等，即整体都要重复开发一遍IOTA 架构IOTA 架构整体思路设定标准数据模型，通过边缘计算技术把所有的计算过程分散在数据产生、计算和查询过程当中，以统一的数据模型贯穿始终，从而提高整体的预算效率，同时满足即时计算的需要，可以使用各种 Ad-hoc Query 来查询底层数据Common Data Model（核心）：从数据收集到数据存储和处理使用统一的数据模型​ “主 - 谓 - 宾”、“对象 - 事件”、“产品 - 事件”、“地点 - 时间” 模型等等​ 例，“X 用户 – 事件 1 – A 页面（2018/4/11 20:00）SDKs：数据的采集端，不仅仅是过去的简单的 SDK，在复杂的计算情况下，会赋予 SDK 更复杂的计算，在设备端就转化为形成统一的数据模型来进行传送Real Time Data：实时数据缓存区，这部分是为了达到实时计算的目的，海量数据接收不可能海量实时入历史数据库，那样会出现建立索引延迟、历史数据碎片文件等问题。因此，有一个实时数据缓存区来存储最近几分钟或者几秒钟的数据。这块可以使用 Kudu 或者 Hbase 等组件来实现。这部分数据会通过 Dumper 来合并到历史数据当中。此处的数据模型和 SDK 端数据模型是保持一致的，都是 Common Data Model，例如 “主 - 谓 - 宾” 模型Historical Data：历史数据沉浸区，这部分是保存了大量的历史数据，为了实现 Ad-hoc 查询，将自动建立相关索引提高整体历史数据查询效率，从而实现秒级复杂查询百亿条数据的反馈。例如可以使用 HDFS 存储历史数据，此处的数据模型依然 SDK 端数据模型是保持一致的 Common Data ModelDumper：Dumper 的主要工作就是把最近几秒或者几分钟的实时数据，根据汇聚规则、建立索引，存储到历史存储结构当中，可以使用 map reduce、C、Scala 来撰写，把相关的数据从 Realtime Data 区写入 Historical Data 区Query Engine：查询引擎，提供统一的对外查询接口和协议（例如 SQL JDBC），把 Realtime Data 和 Historical Data 合并到一起查询，从而实现对于数据实时的 Ad-hoc 查询。例如常见的计算引擎可以使用 presto、impala、clickhouse 等Realtime model feedback：通过 Edge computing 技术，在边缘端有更多的交互可以做，可以通过在 Realtime Data 去设定规则来对 Edge SDK 端进行控制 大数据架构演变https://blog.mhuig.top/p/7e3480e9/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"离散世界与连续世界的联系","path":"/RSSBOX/rss/473bb82a.html","content":"探索当年，哥德巴赫异想天开就想离散的世界的阶乘能不能用连续世界的积分来表达。哥德巴赫有个好朋友叫伯努利，伯努利有个学生叫欧拉。欧拉有个学生叫拉格朗日，拉格朗日有个学生叫柯西哥德巴赫与伯努利交流，问伯努利：有没有离散世界和连续世界可以相等的呢？伯努利想不明白，就问他的学生欧拉，然后欧拉一晚上想出来了。故事结束伽玛函数换元，令 则推导建立递推式由于得到：当时，当时，如：又如：数学归纳可得：参考文献Γ 函数 wikipedia 离散世界与连续世界的联系https://blog.mhuig.top/p/2b995a0c/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"title":"用 GitHub 搭建一个简单的脚本库","path":"/RSSBOX/rss/40f230bb.html","content":"细心的朋友可能会发现，我提供的一些脚本都可以在不需要任何包管理工具的情况下通过一行命令安装，本文讲述如何搭建一个轻量级的脚本库，方便随时执行自己存放于仓库的脚本。 用 GitHub 搭建一个简单的脚本库https://xaoxuu.com/blog/20210102/","tags":["Blogs","XAOXUU"],"categories":["Blogs","XAOXUU"]},{"title":"一大波题目正在来袭","path":"/RSSBOX/rss/bc43343e.html","content":"一大波题目正在来袭https://blog.mhuig.top/p/bc43343e/","tags":["Blogs","MHuiG"],"categories":["Blogs","MHuiG"]},{"path":"/RSSBOX/index.html","content":"RSSBOX主页订阅项目博客 RSSBOX五郎订阅的各种奇奇怪怪的信息流，目前正在收集东方相关信息中。。 Todo 收集东方相关信息流订阅源（长期任务） 收集东方相关网站，创作者等信息（长期任务） 编写东方相关入门介绍文档文章，制作新人引导式文档。 开设运维东方相关社区（待定？） 创建Github组织专门管理东方相关开发资源 妥善安排东方专属域名托管 Logs 2023.5.16 丰富订阅源，增设自建RSSHub，实现网易云音乐在线播放，修改b站播放器适配问题 2023.5.15 更换RSSBOX订阅源为东方相关订阅源 东方网站THBwiki国内优秀的中文东方wikiTouhou Wiki国外英文的东方wikiWikipedia TouhouWikipedia上的东方记载Fandom TouhouWikifandom上的Touhou英文wikiTouhou.CC相当老牌的国内东方论坛网站喵玉殿官作集结国内东方论坛网站Bilibili-东方频道B站东方频道分区微博-东方话题微博东方话题分区知乎-东方话题知乎东方话题分区Youtube-touhouYoutube Touhou tag videosniconico-東方niconico touhou tag videosTwitter-touhou知乎东方话题分区Pixiv-touhouPixiv东方标签绘图 IMC.RE 讨论群公告讨论Discord 其他联系方式DodoQQ群DiscordBilibiliZhihuGithub"},{"title":"社团指南","path":"/RSSBOX/wiki/guides/clubs/index.html","content":"东方社团指南"},{"title":"指南首页","path":"/RSSBOX/wiki/guides/index.html","content":"东方入坑指南"},{"title":"开发指南","path":"/RSSBOX/wiki/guides/develop/index.html","content":"东方开发指南"},{"title":"游戏指南","path":"/RSSBOX/wiki/guides/games/index.html","content":"东方游戏指南"},{"title":"音乐指南","path":"/RSSBOX/wiki/guides/music/index.html","content":"东方音乐指南"},{"title":"图片指南","path":"/RSSBOX/wiki/guides/photos/index.html","content":"东方图片指南"},{"title":"视频指南","path":"/RSSBOX/wiki/guides/videos/index.html","content":"东方视频指南THBwiki国内优秀的中文东方wikiTouhou Wiki国外英文的东方wikiWikipedia TouhouWikipedia上的东方记载Fandom TouhouWikifandom上的Touhou英文wikiTouhou.CC相当老牌的国内东方论坛网站喵玉殿官作集结国内东方论坛网站Bilibili-东方频道B站东方频道分区微博-东方话题微博东方话题分区知乎-东方话题知乎东方话题分区Youtube-touhouYoutube Touhou tag videosniconico-東方niconico touhou tag videosTwitter-touhou知乎东方话题分区Pixiv-touhouPixiv东方标签绘图"},{"title":"网站指南","path":"/RSSBOX/wiki/guides/webs/index.html","content":"东方网站指南THBwiki国内优秀的中文东方wikiTouhou Wiki国外英文的东方wikiWikipedia TouhouWikipedia上的东方记载Fandom TouhouWikifandom上的Touhou英文wikiTouhou.CC相当老牌的国内东方论坛网站喵玉殿官作集结国内东方论坛网站Bilibili-东方频道B站东方频道分区微博-东方话题微博东方话题分区知乎-东方话题知乎东方话题分区Youtube-touhouYoutube Touhou tag videosniconico-東方niconico touhou tag videosTwitter-touhou知乎东方话题分区Pixiv-touhouPixiv东方标签绘图"}]